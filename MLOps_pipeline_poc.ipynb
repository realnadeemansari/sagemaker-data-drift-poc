{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "128d09a3-f6b2-42bf-aa1c-70a2611de5ae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "import sagemaker.session\n",
    "from sagemaker.workflow.pipeline_context import PipelineSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d542ff7b-4042-4696-9e68-3b552ba6ff19",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "region = boto3.Session().region_name\n",
    "sagemaker_session = sagemaker.session.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "pipeline_session = PipelineSession()\n",
    "default_bucket = sagemaker_session.default_bucket()\n",
    "model_package_group_name = f\"MyAbaloneModelPackageGroupName\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "b6c131b5-a166-48c7-9299-5dc1dcc428ab",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'arn:aws:iam::709891711940:role/service-role/SageMaker-MLOpsEngineer-poc'"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "0156f1a9-408a-4556-9e63-f6ffc3c8defb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!mkdir -p data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "916cc0d1-a7f7-46eb-a324-995ddf07117e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-us-east-1-709891711940/abalone/abalone-dataset.csv\n"
     ]
    }
   ],
   "source": [
    "local_path = \"data/abalone-dataset.csv\"\n",
    "s3 = boto3.resource(\"s3\")\n",
    "s3.Bucket(f\"sagemaker-servicecatalog-seedcode-{region}\").download_file(\"dataset/abalone-dataset.csv\", local_path)\n",
    "\n",
    "base_uri = f\"s3://{default_bucket}/abalone\"\n",
    "# input_data_uri = sagemaker.s3.S3Uploader.upload(\n",
    "#     local_path=local_path,\n",
    "#     desired_s3_uri=base_uri\n",
    "# )\n",
    "\n",
    "print(input_data_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "edfe38b0-810b-4c85-907a-12437e62550b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-us-east-1-709891711940/abalone/abalone-dataset-batch.csv\n"
     ]
    }
   ],
   "source": [
    "local_path = 'data/abalone-dataset-batch.csv'\n",
    "s3 = boto3.resource(\"s3\")\n",
    "s3.Bucket(f\"sagemaker-servicecatalog-seedcode-{region}\").download_file(\"dataset/abalone-dataset-batch\", local_path)\n",
    "\n",
    "base_uri = f\"s3://{default_bucket}/abalone\"\n",
    "batch_data_uri = sagemaker.s3.S3Uploader.upload(\n",
    "    local_path=local_path,\n",
    "    desired_s3_uri=base_uri\n",
    ")\n",
    "\n",
    "print(batch_data_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "cc8f0d33-e511-48b7-9c79-b9bb1d1b548a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.workflow.parameters import (\n",
    "    ParameterInteger,\n",
    "    ParameterString,\n",
    "    ParameterFloat\n",
    ")\n",
    "\n",
    "processing_instance_count = ParameterInteger(\n",
    "    name=\"ProcessingInstanceCount\",\n",
    "    default_value=1\n",
    ")\n",
    "\n",
    "model_approval_status = ParameterString(\n",
    "    name=\"ModelApprovalStatus\",\n",
    "    default_value=\"PendingManualApproval\"\n",
    ")\n",
    "\n",
    "input_data = ParameterString(\n",
    "    name=\"InputData\",\n",
    "    default_value=input_data_uri\n",
    ")\n",
    "\n",
    "batch_data = ParameterString(\n",
    "    name=\"BatchData\",\n",
    "    default_value=batch_data_uri\n",
    ")\n",
    "\n",
    "mse_threshold = ParameterFloat(name=\"MseThreshold\", default_value=6.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "aa03977d-bc8f-4ffc-8341-2f202c5ff509",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!mkdir -p abalone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "68d24721-5ee5-4e95-9ff6-86d5ec75b0dd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting abalone/preprocessing.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile abalone/preprocessing.py\n",
    "import argparse\n",
    "import os\n",
    "import requests\n",
    "import tempfile\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "\n",
    "# Because this is a headerless CSV file, specify the column name here.\n",
    "feature_columns_names = [\n",
    "    \"sex\",\n",
    "    \"length\",\n",
    "    \"diameter\",\n",
    "    \"height\",\n",
    "    \"whole_weight\",\n",
    "    \"shucked_weight\",\n",
    "    \"viscera_weight\",\n",
    "    \"shell_weight\",\n",
    "]\n",
    "\n",
    "label_column = \"rings\"\n",
    "\n",
    "feature_columns_dtype = {\n",
    "    \"sex\": str,\n",
    "    \"length\": np.float64,\n",
    "    \"diameter\": np.float64,\n",
    "    \"height\": np.float64,\n",
    "    \"whole_weight\": np.float64,\n",
    "    \"shucked_weight\": np.float64,\n",
    "    \"viscera_weight\": np.float64,\n",
    "    \"shell_weight\": np.float64\n",
    "}\n",
    "label_column_dtype = {\"rings\": np.float64}\n",
    "\n",
    "def merge_two_dicts(x, y):\n",
    "    z = x.copy()\n",
    "    z.update(y)\n",
    "    return z\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    base_dir = \"opt/ml/processing\"\n",
    "    df = pd.read_csv(\n",
    "        f\"{base_dir}/input/abalone-dataset.csv\",\n",
    "        header=None,\n",
    "        names=feature_columns_names + [label_column],\n",
    "        dtype=merge_two_dicts(feature_columns_dtype, label_column_dtype)\n",
    "    )\n",
    "    numeric_features = list(feature_columns_names)\n",
    "    numeric_features.remove(\"sex\")\n",
    "    numeric_transformer = Pipeline(\n",
    "        steps=[\n",
    "            (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "            (\"scaler\", StandardScaler())\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    categorical_features = [\"sex\"]\n",
    "    categorical_transformer = Pipeline(\n",
    "        steps=[\n",
    "            (\"imputer\", SimpleImputer(strategy=\"constant\", fill_value=\"missing\")),\n",
    "            (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    preprocess = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"num\", numeric_transformer, numeric_features),\n",
    "            (\"cat\", categorical_transformer, categorical_features)\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    y = df.pop(\"rings\")\n",
    "    X_pre = preprocess.fit_transform(df)\n",
    "    y_pre = y.to_numpy().reshape(len(y), 1)\n",
    "    \n",
    "    X = np.concatenate((y_pre, X_pre), axis=1)\n",
    "    np.random.shuffle(X)\n",
    "    train, validation, test = np.split(X, [int(.7*len(X)), int(.85*len(X))])\n",
    "    \n",
    "    pd.DataFrame(train).to_csv(f\"{base_dir}/train/train.csv\", header=False, index=False)\n",
    "    pd.DataFrame(validation).to_csv(f\"{base_dir}/validation/validation.csv\", header=False, index=False)\n",
    "    pd.DataFrame(test).to_csv(f\"{base_dir}/test/test.csv\", header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "1c9e4a58-8ac8-4679-8ca1-62454b2e40ab",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'us-east-1'"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "06380a09-fc48-44fb-814c-33716dadc238",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:Defaulting to only available Python version: py3\n"
     ]
    }
   ],
   "source": [
    " from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "\n",
    "framework_version = \"0.23-1\"\n",
    "\n",
    "sklearn_processor = SKLearnProcessor(\n",
    "    framework_version=framework_version,\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    instance_count=processing_instance_count,\n",
    "    base_job_name=\"sklearn-abalone-process\",\n",
    "    sagemaker_session=pipeline_session,\n",
    "    role=role\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "3fae21ec-ad65-48fe-939e-083a6575e49a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sagemaker/workflow/pipeline_context.py:273: UserWarning: Running within a PipelineSession, there will be No Wait, No Logs, and No Job being started.\n",
      "  UserWarning,\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "from sagemaker.workflow.steps import ProcessingStep\n",
    "\n",
    "processor_args = sklearn_processor.run(\n",
    "    inputs=[\n",
    "        ProcessingInput(source=input_data, destination=\"/opt/ml/processing/input\"),\n",
    "    ],\n",
    "    outputs=[\n",
    "        ProcessingOutput(output_name=\"train\", source=\"/opt/ml/processing/train\"),\n",
    "        ProcessingOutput(output_name=\"validation\", source=\"/opt/ml/processing/validation\"),\n",
    "        ProcessingOutput(output_name=\"test\", source=\"/opt/ml/processing/test\")\n",
    "    ],\n",
    "    code=\"abalone/preprocessing.py\",\n",
    ")\n",
    "\n",
    "step_process = ProcessingStep(name=\"MyAbaloneProcess\", step_args=processor_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "64e376b7-a006-44e2-a5f5-faac25f1b4d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "# from sagemaker.workflow.steps import ProcessingStep\n",
    "\n",
    "# step_process = ProcessingStep(\n",
    "#     name=\"MyAbaloneProcess\",\n",
    "#     processor=sklearn_processor,\n",
    "#     inputs=[\n",
    "#         ProcessingInput(source=input_data, destination=\"/opt/ml/processing/input\"),\n",
    "#     ],\n",
    "#     outputs=[\n",
    "#         ProcessingOutput(output_name=\"train\", source=\"/opt/ml/processing/train\"),\n",
    "#         ProcessingOutput(output_name=\"validation\", source=\"/opt/ml/processing/validation\"),\n",
    "#         ProcessingOutput(output_name=\"test\", source=\"/opt/ml/processing/test\")\n",
    "#     ],\n",
    "#     code=\"abalone/preprocessing.py\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "62dec5c7-19be-48db-a0b3-cebec0128744",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParameterString(name='InputData', parameter_type=<ParameterTypeEnum.STRING: 'String'>, default_value='s3://sagemaker-us-east-1-709891711940/abalone/abalone-dataset.csv')"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "f293c065-db87-417a-aa2a-2cb54c4f72f4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_path = f\"s3://{default_bucket}/MyAbaloneTrain\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "87dd5dfa-3e6f-4d5e-ad48-f13d14a41fec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.estimator import Estimator\n",
    "\n",
    "image_uri = sagemaker.image_uris.retrieve(\n",
    "    framework=\"xgboost\",\n",
    "    region=region,\n",
    "    version=\"1.0-1\",\n",
    "    py_version=\"py3\",\n",
    "    instance_type=\"ml.m5.xlarge\"\n",
    ")\n",
    "\n",
    "xgb_train = Estimator(\n",
    "    image_uri=image_uri,\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    instance_count=1,\n",
    "    output_path=model_path,\n",
    "    sagemaker_session=pipeline_session,\n",
    "    role=role\n",
    ")\n",
    "\n",
    "xgb_train.set_hyperparameters(\n",
    "    objective=\"reg:linear\",\n",
    "    num_round=50,\n",
    "    max_depth=5,\n",
    "    eta=0.2,\n",
    "    gamma=4,\n",
    "    min_child_weight=6,\n",
    "    subsample=0.7,\n",
    "    silent=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "caa56850-77f3-4157-a43b-158f9ac28977",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.inputs import TrainingInput\n",
    "train_args = xgb_train.fit(\n",
    "    inputs={\n",
    "        \"train\": TrainingInput(\n",
    "            s3_data=step_process.properties.ProcessingOutputConfig.Outputs[\"train\"].S3Output.S3Uri,\n",
    "            content_type=\"text/csv\"\n",
    "        ),\n",
    "        \"validation\": TrainingInput(\n",
    "            s3_data=step_process.properties.ProcessingOutputConfig.Outputs[\"validation\"].S3Output.S3Uri,\n",
    "            content_type=\"text/csv\"\n",
    "        ),\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "bd471c52-bffd-4465-9ded-6814725b7b44",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.inputs import TrainingInput\n",
    "from sagemaker.workflow.steps import TrainingStep\n",
    "\n",
    "step_train = TrainingStep(\n",
    "    name=\"MyAbaloneTrain\",\n",
    "    step_args=train_args\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "379b8712-6266-4945-8b1e-e76cab1ebcb3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting abalone/evaluation.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile abalone/evaluation.py\n",
    "import json\n",
    "import pathlib\n",
    "import pickle\n",
    "import tarfile\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    model_path = f\"/opt/ml/processing/model/model.tar.gz\"\n",
    "    with tarfile.open(model_path) as tar:\n",
    "        tar.extractall(path=\".\")\n",
    "        \n",
    "    model = pickle.load(open(\"xgboost-model\", \"rb\"))\n",
    "    \n",
    "    test_path = \"/opt/ml/processing/test/test.csv\"\n",
    "    df = pd.read_csv(test_path, header=None)\n",
    "    \n",
    "    y_test = df.iloc[:, 0].to_numpy()\n",
    "    df.drop(df.columns[0], axis=1, inplace=True)\n",
    "    \n",
    "    X_test = xgboost.DMatrix(df.values)\n",
    "    predictions = model.predict(X_test)\n",
    "    \n",
    "    mse = mean_squared_error(y_test, predictions)\n",
    "    std = np.std(y_test - predictions)\n",
    "    \n",
    "    report_dict = {\n",
    "        \"regression_metrics\": {\n",
    "            \"mse\": {\n",
    "                \"value\": mse,\n",
    "                \"standard_deviation\": std\n",
    "            },\n",
    "        },\n",
    "    }\n",
    "    \n",
    "    output_dir = \"/opt/ml/processing/evaluation\"\n",
    "    pathlib.Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    evaluation_path = f\"{output_dir}/evaluation.json\"\n",
    "    with open(evaluation_path, \"w\") as f:\n",
    "        f.write(json.dumps(report_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "1fbc2684-e2ef-49cf-85c0-2cf39b11c776",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.processing import ScriptProcessor\n",
    "\n",
    "script_eval = ScriptProcessor(\n",
    "    image_uri=image_uri,\n",
    "    command=[\"python3\"],\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    instance_count=1,\n",
    "    base_job_name=\"my-script-abalone-eval\",\n",
    "    sagemaker_session=pipeline_session,\n",
    "    role=role\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "f4034df3-c3d1-4f5b-a969-80d14548e761",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "eval_args = script_eval.run(\n",
    "    inputs=[\n",
    "        ProcessingInput(\n",
    "            source=step_train.properties.ModelArtifacts.S3ModelArtifacts,\n",
    "            destination=\"/opt/ml/processing/model\",\n",
    "        ),\n",
    "        ProcessingInput(\n",
    "            source=step_process.properties.ProcessingOutputConfig.Outputs[\"test\"].S3Output.S3Uri,\n",
    "            destination=\"/opt/ml/processing/test\"\n",
    "        ),\n",
    "    ],\n",
    "    outputs=[\n",
    "        ProcessingOutput(output_name=\"evaluation\", source=\"/opt/ml/processing/evaluation\"),\n",
    "    ],\n",
    "    code=\"abalone/evaluation.py\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "1223e728-40c0-4b2a-9a16-06d0461faadf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.workflow.properties import PropertyFile\n",
    "\n",
    "evaluation_report = PropertyFile(\n",
    "    name=\"MyEvaluationReport\",\n",
    "    output_name=\"evaluation\",\n",
    "    path=\"evaluation.json\"\n",
    ")\n",
    "\n",
    "step_eval = ProcessingStep(\n",
    "    name=\"MyAbaloneEval\",\n",
    "    step_args=eval_args,\n",
    "    property_files=[evaluation_report],\n",
    ")\n",
    "\n",
    "# step_eval = ProcessingStep(\n",
    "#     name=\"MyAbaloneEval\",\n",
    "#     processor=script_eval,\n",
    "#     inputs=[\n",
    "#         ProcessingInput(\n",
    "#             source=step_train.properties.ModelArtifacts.S3ModelArtifacts,\n",
    "#             destination=\"/opt/ml/processing/model\"\n",
    "#         ),\n",
    "#         ProcessingInput(\n",
    "#             source=step_process.properties.ProcessingOutputConfig.Outputs[\n",
    "#                 \"test\",\n",
    "#             ].S3Output.S3Uri,\n",
    "#             destination=\"/opt/ml/processing/test\"\n",
    "#         )\n",
    "#     ],\n",
    "#     outputs=[\n",
    "#         ProcessingOutput(output_name=\"evaluation\", source=\"/opt/ml/processing/evaluation\"),\n",
    "#     ],\n",
    "#     code=\"abalone/evaluation.py\",\n",
    "#     property_files=[evaluation_report],\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "e966e772-1096-4896-b1b1-63a5bb82ece7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.model import Model\n",
    "\n",
    "model = Model(\n",
    "    image_uri=image_uri,\n",
    "    model_data=step_train.properties.ModelArtifacts.S3ModelArtifacts,\n",
    "    sagemaker_session=pipeline_session,\n",
    "    role=role\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "ab917a3e-6a48-43a7-8289-ee850308c0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.inputs import CreateModelInput\n",
    "\n",
    "model_args = model.create(\n",
    "    instance_type=\"ml.m5.large\",\n",
    "    accelerator_type=\"ml.eia1.medium\"\n",
    ")\n",
    "# inputs = CreateModelInput(\n",
    "#     instance_type=\"ml.m5.xlarge\",\n",
    "#     accelerator_type=\"ml.eia1.medium\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "38a55f19-0e2e-4688-89ee-2aa3dae6de44",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.inputs import CreateModelInput\n",
    "from sagemaker.workflow.model_step import ModelStep\n",
    "\n",
    "step_create_model = ModelStep(\n",
    "    name=\"MyAbaloneCreateModel\",\n",
    "    step_args=model_args\n",
    ")\n",
    "\n",
    "# from sagemaker.workflow.steps import CreateModelStep\n",
    "\n",
    "# step_create_model = CreateModelStep(\n",
    "#     name=\"MyAbaloneCreateModel\",\n",
    "#     model=model,\n",
    "#     inputs=inputs\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "25840d94-fb28-4612-bd58-26061f5d09f2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.transformer import Transformer\n",
    "\n",
    "transformer = Transformer(\n",
    "    model_name=step_create_model.properties.ModelName,\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    instance_count=1,\n",
    "    output_path=f\"s3://{default_bucket}/MyAbaloneTransform\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "2b72b2de-4c59-40e9-b516-53daba460c5f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.inputs import TransformInput\n",
    "from sagemaker.workflow.steps import TransformStep\n",
    "\n",
    "step_transform = TransformStep(\n",
    "    name=\"MyAbaloneTransform\",\n",
    "    transformer=transformer,\n",
    "    inputs=TransformInput(data=batch_data)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "508bb10d-cd63-44d1-8ee3-3387af56e1be",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://sagemaker-us-east-1-709891711940/my-script-abalone-eval-2023-05-18-13-09-00-765/output/evaluation'"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "step_eval.arguments[\"ProcessingOutputConfig\"][\"Outputs\"][0][\"S3Output\"][\"S3Uri\"]\n",
    "# a + [\"evaluation.json\"]\n",
    "\n",
    "# step_eval.arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa4969c-82d5-4d4a-96ce-f227201dba34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "da2a47aa-033e-473e-b1d1-f6729df5bfbc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sagemaker/workflow/pipeline_context.py:273: UserWarning: Running within a PipelineSession, there will be No Wait, No Logs, and No Job being started.\n",
      "  UserWarning,\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.model_metrics import MetricsSource, ModelMetrics\n",
    "from sagemaker.workflow.step_collections import RegisterModel\n",
    "from sagemaker.workflow.model_step import ModelStep\n",
    "\n",
    "model_metrics = ModelMetrics(\n",
    "    model_statistics=MetricsSource(\n",
    "        s3_uri=\"{}/evaluation.json\".format(\n",
    "            step_eval.arguments[\"ProcessingOutputConfig\"][\"Outputs\"][0][\"S3Output\"][\"S3Uri\"]\n",
    "        ),\n",
    "        content_type=\"application/json\",\n",
    "    )\n",
    ")\n",
    "\n",
    "register_args = model.register(\n",
    "    content_types=[\"text/csv\"],\n",
    "    response_types=[\"text/csv\"],\n",
    "    inference_instances=[\"ml.t2.medium\", \"ml.m5.xlarge\"],\n",
    "    transform_instances=[\"ml.m5.xlarge\"],\n",
    "    model_package_group_name=model_package_group_name,\n",
    "    approval_status=model_approval_status,\n",
    "    model_metrics=model_metrics\n",
    ")\n",
    "\n",
    "step_register = ModelStep(\n",
    "    name=\"MyAbaloneRegisterModel\",\n",
    "    step_args=register_args\n",
    ")\n",
    "# step_register = RegisterModel(\n",
    "#     name=\"MyAbaloneRegisterModel\",\n",
    "#     estimator=xgb_train,\n",
    "#     model_data=step_train.properties.ModelArtifacts.S3ModelArtifacts,\n",
    "#     content_types=[\"text/csv\"],\n",
    "#     response_types=[\"text/csv\"],\n",
    "#     inference_instances=[\"ml.t2.medium\", \"ml.m5.xlarge\"],\n",
    "#     transform_instances=[\"ml.m5.xlarge\"],\n",
    "#     model_package_group_name=model_package_group_name,\n",
    "#     model_metrics=model_metrics\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "ab36459b-1675-433e-81bf-c93a622f60b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.workflow.fail_step import FailStep\n",
    "from sagemaker.workflow.functions import Join\n",
    "\n",
    "step_fail = FailStep(\n",
    "    name=\"MyAbaloneMSEFail\",\n",
    "    error_message=Join(on=\" \", values=[\"Execution failed due to MSE >\", mse_threshold]),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "c725129e-2190-477d-8dac-d6916acd9ee9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'MyAbaloneEval'"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "step_eval.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "a54813e0-8151-4667-824a-18edfd1bcf39",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.workflow.conditions import ConditionLessThanOrEqualTo\n",
    "from sagemaker.workflow.condition_step import ConditionStep\n",
    "from sagemaker.workflow.functions import JsonGet\n",
    "\n",
    "cond_lte = ConditionLessThanOrEqualTo(\n",
    "    left=JsonGet(\n",
    "        step_name=step_eval.name,\n",
    "        property_file=evaluation_report,\n",
    "        json_path=\"regression_metrics.mse.value\",\n",
    "    ),\n",
    "    right=mse_threshold,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "fb4ee041-b8d2-4a55-a838-4915024583e7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('MyAbaloneRegisterModel',\n",
       " 'MyAbaloneCreateModel',\n",
       " 'MyAbaloneTransform',\n",
       " 'MyAbaloneMSEFail')"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "step_register.name, step_create_model.name, step_transform.name, step_fail.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "92fe5db6-fd5f-4986-86fe-d9d8eec0e070",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "step_cond = ConditionStep(\n",
    "    name=\"MyAbaloneMSECond\",\n",
    "    conditions=[cond_lte],\n",
    "    if_steps=[step_register, step_create_model, step_transform],\n",
    "    else_steps=[step_fail],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "5449911f-6ed8-4ee1-acc2-8fba82e71bae",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'MyAbaloneMSECond'"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "step_cond.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "366a34f2-c4d2-44c4-9273-9c870a003f78",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "\n",
    "pipeline_name = f\"MyAbalonePipeline\"\n",
    "pipeline = Pipeline(\n",
    "    name=pipeline_name,\n",
    "    parameters=[\n",
    "        processing_instance_count,\n",
    "        model_approval_status,\n",
    "        input_data,\n",
    "        batch_data,\n",
    "        mse_threshold,\n",
    "    ],\n",
    "    steps=[step_process\n",
    "           , step_train,\n",
    "    step_eval, \n",
    "    step_cond]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "2fdee971-a7d3-4b0a-8401-3f31b52bc9e6",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sagemaker.workflow._utils:Popping out 'CertifyForMarketplace' from the pipeline definition since it will be overridden in pipeline execution time.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Version': '2020-12-01',\n",
       " 'Metadata': {},\n",
       " 'Parameters': [{'Name': 'ProcessingInstanceCount',\n",
       "   'Type': 'Integer',\n",
       "   'DefaultValue': 1},\n",
       "  {'Name': 'ModelApprovalStatus',\n",
       "   'Type': 'String',\n",
       "   'DefaultValue': 'PendingManualApproval'},\n",
       "  {'Name': 'InputData',\n",
       "   'Type': 'String',\n",
       "   'DefaultValue': 's3://sagemaker-us-east-1-709891711940/abalone/abalone-dataset.csv'},\n",
       "  {'Name': 'BatchData',\n",
       "   'Type': 'String',\n",
       "   'DefaultValue': 's3://sagemaker-us-east-1-709891711940/abalone/abalone-dataset-batch.csv'},\n",
       "  {'Name': 'MseThreshold', 'Type': 'Float', 'DefaultValue': 6.0}],\n",
       " 'PipelineExperimentConfig': {'ExperimentName': {'Get': 'Execution.PipelineName'},\n",
       "  'TrialName': {'Get': 'Execution.PipelineExecutionId'}},\n",
       " 'Steps': [{'Name': 'MyAbaloneProcess',\n",
       "   'Type': 'Processing',\n",
       "   'Arguments': {'ProcessingResources': {'ClusterConfig': {'InstanceType': 'ml.m5.xlarge',\n",
       "      'InstanceCount': {'Get': 'Parameters.ProcessingInstanceCount'},\n",
       "      'VolumeSizeInGB': 30}},\n",
       "    'AppSpecification': {'ImageUri': '683313688378.dkr.ecr.us-east-1.amazonaws.com/sagemaker-scikit-learn:0.23-1-cpu-py3',\n",
       "     'ContainerEntrypoint': ['python3',\n",
       "      '/opt/ml/processing/input/code/preprocessing.py']},\n",
       "    'RoleArn': 'arn:aws:iam::709891711940:role/service-role/SageMaker-MLOpsEngineer-poc',\n",
       "    'ProcessingInputs': [{'InputName': 'input-1',\n",
       "      'AppManaged': False,\n",
       "      'S3Input': {'S3Uri': {'Get': 'Parameters.InputData'},\n",
       "       'LocalPath': '/opt/ml/processing/input',\n",
       "       'S3DataType': 'S3Prefix',\n",
       "       'S3InputMode': 'File',\n",
       "       'S3DataDistributionType': 'FullyReplicated',\n",
       "       'S3CompressionType': 'None'}},\n",
       "     {'InputName': 'code',\n",
       "      'AppManaged': False,\n",
       "      'S3Input': {'S3Uri': 's3://sagemaker-us-east-1-709891711940/MyAbalonePipeline/code/4b47e1408dc04a2c96e013d1d6c840c8/preprocessing.py',\n",
       "       'LocalPath': '/opt/ml/processing/input/code',\n",
       "       'S3DataType': 'S3Prefix',\n",
       "       'S3InputMode': 'File',\n",
       "       'S3DataDistributionType': 'FullyReplicated',\n",
       "       'S3CompressionType': 'None'}}],\n",
       "    'ProcessingOutputConfig': {'Outputs': [{'OutputName': 'train',\n",
       "       'AppManaged': False,\n",
       "       'S3Output': {'S3Uri': {'Std:Join': {'On': '/',\n",
       "          'Values': ['s3:/',\n",
       "           'sagemaker-us-east-1-709891711940',\n",
       "           'MyAbalonePipeline',\n",
       "           {'Get': 'Execution.PipelineExecutionId'},\n",
       "           'MyAbaloneProcess',\n",
       "           'output',\n",
       "           'train']}},\n",
       "        'LocalPath': '/opt/ml/processing/train',\n",
       "        'S3UploadMode': 'EndOfJob'}},\n",
       "      {'OutputName': 'validation',\n",
       "       'AppManaged': False,\n",
       "       'S3Output': {'S3Uri': {'Std:Join': {'On': '/',\n",
       "          'Values': ['s3:/',\n",
       "           'sagemaker-us-east-1-709891711940',\n",
       "           'MyAbalonePipeline',\n",
       "           {'Get': 'Execution.PipelineExecutionId'},\n",
       "           'MyAbaloneProcess',\n",
       "           'output',\n",
       "           'validation']}},\n",
       "        'LocalPath': '/opt/ml/processing/validation',\n",
       "        'S3UploadMode': 'EndOfJob'}},\n",
       "      {'OutputName': 'test',\n",
       "       'AppManaged': False,\n",
       "       'S3Output': {'S3Uri': {'Std:Join': {'On': '/',\n",
       "          'Values': ['s3:/',\n",
       "           'sagemaker-us-east-1-709891711940',\n",
       "           'MyAbalonePipeline',\n",
       "           {'Get': 'Execution.PipelineExecutionId'},\n",
       "           'MyAbaloneProcess',\n",
       "           'output',\n",
       "           'test']}},\n",
       "        'LocalPath': '/opt/ml/processing/test',\n",
       "        'S3UploadMode': 'EndOfJob'}}]}}},\n",
       "  {'Name': 'MyAbaloneTrain',\n",
       "   'Type': 'Training',\n",
       "   'Arguments': {'AlgorithmSpecification': {'TrainingInputMode': 'File',\n",
       "     'TrainingImage': '683313688378.dkr.ecr.us-east-1.amazonaws.com/sagemaker-xgboost:1.0-1-cpu-py3'},\n",
       "    'OutputDataConfig': {'S3OutputPath': 's3://sagemaker-us-east-1-709891711940/MyAbaloneTrain'},\n",
       "    'StoppingCondition': {'MaxRuntimeInSeconds': 86400},\n",
       "    'ResourceConfig': {'VolumeSizeInGB': 30,\n",
       "     'InstanceCount': 1,\n",
       "     'InstanceType': 'ml.m5.xlarge'},\n",
       "    'RoleArn': 'arn:aws:iam::709891711940:role/service-role/SageMaker-MLOpsEngineer-poc',\n",
       "    'InputDataConfig': [{'DataSource': {'S3DataSource': {'S3DataType': 'S3Prefix',\n",
       "        'S3Uri': {'Get': \"Steps.MyAbaloneProcess.ProcessingOutputConfig.Outputs['train'].S3Output.S3Uri\"},\n",
       "        'S3DataDistributionType': 'FullyReplicated'}},\n",
       "      'ContentType': 'text/csv',\n",
       "      'ChannelName': 'train'},\n",
       "     {'DataSource': {'S3DataSource': {'S3DataType': 'S3Prefix',\n",
       "        'S3Uri': {'Get': \"Steps.MyAbaloneProcess.ProcessingOutputConfig.Outputs['validation'].S3Output.S3Uri\"},\n",
       "        'S3DataDistributionType': 'FullyReplicated'}},\n",
       "      'ContentType': 'text/csv',\n",
       "      'ChannelName': 'validation'}],\n",
       "    'HyperParameters': {'objective': 'reg:linear',\n",
       "     'num_round': '50',\n",
       "     'max_depth': '5',\n",
       "     'eta': '0.2',\n",
       "     'gamma': '4',\n",
       "     'min_child_weight': '6',\n",
       "     'subsample': '0.7',\n",
       "     'silent': '0'},\n",
       "    'DebugHookConfig': {'S3OutputPath': 's3://sagemaker-us-east-1-709891711940/MyAbaloneTrain',\n",
       "     'CollectionConfigurations': []},\n",
       "    'ProfilerConfig': {'S3OutputPath': 's3://sagemaker-us-east-1-709891711940/MyAbaloneTrain',\n",
       "     'DisableProfiler': False}}},\n",
       "  {'Name': 'MyAbaloneEval',\n",
       "   'Type': 'Processing',\n",
       "   'Arguments': {'ProcessingResources': {'ClusterConfig': {'InstanceType': 'ml.m5.xlarge',\n",
       "      'InstanceCount': 1,\n",
       "      'VolumeSizeInGB': 30}},\n",
       "    'AppSpecification': {'ImageUri': '683313688378.dkr.ecr.us-east-1.amazonaws.com/sagemaker-xgboost:1.0-1-cpu-py3',\n",
       "     'ContainerEntrypoint': ['python3',\n",
       "      '/opt/ml/processing/input/code/evaluation.py']},\n",
       "    'RoleArn': 'arn:aws:iam::709891711940:role/service-role/SageMaker-MLOpsEngineer-poc',\n",
       "    'ProcessingInputs': [{'InputName': 'input-1',\n",
       "      'AppManaged': False,\n",
       "      'S3Input': {'S3Uri': {'Get': 'Steps.MyAbaloneTrain.ModelArtifacts.S3ModelArtifacts'},\n",
       "       'LocalPath': '/opt/ml/processing/model',\n",
       "       'S3DataType': 'S3Prefix',\n",
       "       'S3InputMode': 'File',\n",
       "       'S3DataDistributionType': 'FullyReplicated',\n",
       "       'S3CompressionType': 'None'}},\n",
       "     {'InputName': 'input-2',\n",
       "      'AppManaged': False,\n",
       "      'S3Input': {'S3Uri': {'Get': \"Steps.MyAbaloneProcess.ProcessingOutputConfig.Outputs['test'].S3Output.S3Uri\"},\n",
       "       'LocalPath': '/opt/ml/processing/test',\n",
       "       'S3DataType': 'S3Prefix',\n",
       "       'S3InputMode': 'File',\n",
       "       'S3DataDistributionType': 'FullyReplicated',\n",
       "       'S3CompressionType': 'None'}},\n",
       "     {'InputName': 'code',\n",
       "      'AppManaged': False,\n",
       "      'S3Input': {'S3Uri': 's3://sagemaker-us-east-1-709891711940/MyAbalonePipeline/code/b44e7c73a7798d79031d5f4466f660eb/evaluation.py',\n",
       "       'LocalPath': '/opt/ml/processing/input/code',\n",
       "       'S3DataType': 'S3Prefix',\n",
       "       'S3InputMode': 'File',\n",
       "       'S3DataDistributionType': 'FullyReplicated',\n",
       "       'S3CompressionType': 'None'}}],\n",
       "    'ProcessingOutputConfig': {'Outputs': [{'OutputName': 'evaluation',\n",
       "       'AppManaged': False,\n",
       "       'S3Output': {'S3Uri': 's3://sagemaker-us-east-1-709891711940/my-script-abalone-eval-2023-05-18-13-09-00-765/output/evaluation',\n",
       "        'LocalPath': '/opt/ml/processing/evaluation',\n",
       "        'S3UploadMode': 'EndOfJob'}}]}},\n",
       "   'PropertyFiles': [{'PropertyFileName': 'MyEvaluationReport',\n",
       "     'OutputName': 'evaluation',\n",
       "     'FilePath': 'evaluation.json'}]},\n",
       "  {'Name': 'MyAbaloneMSECond',\n",
       "   'Type': 'Condition',\n",
       "   'Arguments': {'Conditions': [{'Type': 'LessThanOrEqualTo',\n",
       "      'LeftValue': {'Std:JsonGet': {'PropertyFile': {'Get': 'Steps.MyAbaloneEval.PropertyFiles.MyEvaluationReport'},\n",
       "        'Path': 'regression_metrics.mse.value'}},\n",
       "      'RightValue': {'Get': 'Parameters.MseThreshold'}}],\n",
       "    'IfSteps': [{'Name': 'MyAbaloneRegisterModel-RegisterModel',\n",
       "      'Type': 'RegisterModel',\n",
       "      'Arguments': {'ModelPackageGroupName': 'MyAbaloneModelPackageGroupName',\n",
       "       'ModelMetrics': {'ModelQuality': {'Statistics': {'ContentType': 'application/json',\n",
       "          'S3Uri': 's3://sagemaker-us-east-1-709891711940/my-script-abalone-eval-2023-05-18-13-09-00-765/output/evaluation/evaluation.json'}},\n",
       "        'Bias': {},\n",
       "        'Explainability': {}},\n",
       "       'InferenceSpecification': {'Containers': [{'Image': '683313688378.dkr.ecr.us-east-1.amazonaws.com/sagemaker-xgboost:1.0-1-cpu-py3',\n",
       "          'Environment': {},\n",
       "          'ModelDataUrl': {'Get': 'Steps.MyAbaloneTrain.ModelArtifacts.S3ModelArtifacts'}}],\n",
       "        'SupportedContentTypes': ['text/csv'],\n",
       "        'SupportedResponseMIMETypes': ['text/csv'],\n",
       "        'SupportedRealtimeInferenceInstanceTypes': ['ml.t2.medium',\n",
       "         'ml.m5.xlarge'],\n",
       "        'SupportedTransformInstanceTypes': ['ml.m5.xlarge']},\n",
       "       'ModelApprovalStatus': {'Get': 'Parameters.ModelApprovalStatus'}}},\n",
       "     {'Name': 'MyAbaloneCreateModel-CreateModel',\n",
       "      'Type': 'Model',\n",
       "      'Arguments': {'ExecutionRoleArn': 'arn:aws:iam::709891711940:role/service-role/SageMaker-MLOpsEngineer-poc',\n",
       "       'PrimaryContainer': {'Image': '683313688378.dkr.ecr.us-east-1.amazonaws.com/sagemaker-xgboost:1.0-1-cpu-py3',\n",
       "        'Environment': {},\n",
       "        'ModelDataUrl': {'Get': 'Steps.MyAbaloneTrain.ModelArtifacts.S3ModelArtifacts'}}}},\n",
       "     {'Name': 'MyAbaloneTransform',\n",
       "      'Type': 'Transform',\n",
       "      'Arguments': {'ModelName': {'Get': 'Steps.MyAbaloneCreateModel-CreateModel.ModelName'},\n",
       "       'TransformInput': {'DataSource': {'S3DataSource': {'S3DataType': 'S3Prefix',\n",
       "          'S3Uri': {'Get': 'Parameters.BatchData'}}}},\n",
       "       'TransformOutput': {'S3OutputPath': 's3://sagemaker-us-east-1-709891711940/MyAbaloneTransform'},\n",
       "       'TransformResources': {'InstanceCount': 1,\n",
       "        'InstanceType': 'ml.m5.xlarge'}}}],\n",
       "    'ElseSteps': [{'Name': 'MyAbaloneMSEFail',\n",
       "      'Type': 'Fail',\n",
       "      'Arguments': {'ErrorMessage': {'Std:Join': {'On': ' ',\n",
       "         'Values': ['Execution failed due to MSE >',\n",
       "          {'Get': 'Parameters.MseThreshold'}]}}}}]}}]}"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "json.loads(pipeline.definition())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "6766d747-b83f-4a90-82a2-3c0cc8b1631d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'PipelineArn': 'arn:aws:sagemaker:us-east-1:709891711940:pipeline/myabalonepipeline',\n",
       " 'ResponseMetadata': {'RequestId': '1b588229-2bb3-4d50-a57e-79227148ee73',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '1b588229-2bb3-4d50-a57e-79227148ee73',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '85',\n",
       "   'date': 'Thu, 18 May 2023 13:16:03 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.upsert(role_arn=role)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "c77148e7-c85f-4f70-b755-c79364728443",
   "metadata": {},
   "outputs": [],
   "source": [
    "execution = pipeline.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "d4a0dcd9-5109-4ece-8eaa-b8ede11dbbc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'PipelineArn': 'arn:aws:sagemaker:us-east-1:709891711940:pipeline/myabalonepipeline',\n",
       " 'PipelineExecutionArn': 'arn:aws:sagemaker:us-east-1:709891711940:pipeline/myabalonepipeline/execution/oxnwnmq2numt',\n",
       " 'PipelineExecutionDisplayName': 'execution-1684415767214',\n",
       " 'PipelineExecutionStatus': 'Executing',\n",
       " 'PipelineExperimentConfig': {'ExperimentName': 'myabalonepipeline',\n",
       "  'TrialName': 'oxnwnmq2numt'},\n",
       " 'CreationTime': datetime.datetime(2023, 5, 18, 13, 16, 7, 98000, tzinfo=tzlocal()),\n",
       " 'LastModifiedTime': datetime.datetime(2023, 5, 18, 13, 16, 7, 98000, tzinfo=tzlocal()),\n",
       " 'CreatedBy': {'UserProfileArn': 'arn:aws:sagemaker:us-east-1:709891711940:user-profile/d-odvpujw44yfo/default-1681294514475',\n",
       "  'UserProfileName': 'default-1681294514475',\n",
       "  'DomainId': 'd-odvpujw44yfo'},\n",
       " 'LastModifiedBy': {'UserProfileArn': 'arn:aws:sagemaker:us-east-1:709891711940:user-profile/d-odvpujw44yfo/default-1681294514475',\n",
       "  'UserProfileName': 'default-1681294514475',\n",
       "  'DomainId': 'd-odvpujw44yfo'},\n",
       " 'ResponseMetadata': {'RequestId': 'a0c3278a-98eb-4f61-997f-6a22a74da44b',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': 'a0c3278a-98eb-4f61-997f-6a22a74da44b',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '850',\n",
       "   'date': 'Thu, 18 May 2023 13:16:08 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "execution.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "a0602916-c84b-4c42-896b-d18d59b5a4aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "execution.wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db3149be-58fb-4a51-a3fe-bb66d6f6cb10",
   "metadata": {},
   "outputs": [],
   "source": [
    "execution.list_steps()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f2a3ec6c-d01a-44e2-a1d3-8cb1310400f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_client = boto3.client(\"sagemaker\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e1346c03-313d-4fb9-bff2-071b785bfe7b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_package_arn = sm_client.list_model_packages(ModelPackageGroupName=model_package_group_name)[\"ModelPackageSummaryList\"][0][\"ModelPackageArn\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "09550ba5-4b52-4af2-b5b2-f3aeaf8b9666",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'arn:aws:sagemaker:us-east-1:709891711940:model-package/myabalonemodelpackagegroupname/1'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_package_arn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a79faa90-eeaa-4471-b9a0-1d9d469d8244",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_package_update_input_dict = {\n",
    "    \"ModelPackageArn\": model_package_arn,\n",
    "    \"ModelApprovalStatus\": \"Approved\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "06fcc2b7-267e-47d9-bf8b-14d9923a5dc5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_package_update_response = sm_client.update_model_package(**model_package_update_input_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dabbcf91-49f2-4c0a-b6a0-3f3760563358",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ModelPackageArn': 'arn:aws:sagemaker:us-east-1:709891711940:model-package/myabalonemodelpackagegroupname/1',\n",
       " 'ResponseMetadata': {'RequestId': '7e5d1d91-9d8e-48c3-9e35-bf74c952a537',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '7e5d1d91-9d8e-48c3-9e35-bf74c952a537',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '109',\n",
       "   'date': 'Fri, 26 May 2023 10:20:42 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_package_update_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "251b7d3b-c3da-4a9e-aa9b-3c9f929c7299",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----!"
     ]
    }
   ],
   "source": [
    "from sagemaker import ModelPackage\n",
    "from time import gmtime, strftime\n",
    "\n",
    "model = ModelPackage(\n",
    "    role=role,\n",
    "    model_package_arn=model_package_arn,\n",
    "    sagemaker_session=sagemaker_session\n",
    ")\n",
    "\n",
    "model.deploy(initial_instance_count=1, instance_type='ml.m5.xlarge')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef8705cc-5227-4156-a2ba-23756df267f8",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "# Amazon SageMaker Drift Detection\n",
    "\n",
    "This sample demonstrates how to setup an Amazon SageMaker MLOps deployment pipeline for Drift detection\n",
    "\n",
    "![Solution Architecture](docs/drift-solution-architecture.png)\n",
    "\n",
    "The following are the high-level steps to deploy this solution:\n",
    "\n",
    "1. Publish the SageMaker [MLOps Project template](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-projects-templates.html) in the [AWS Service Catalog](https://aws.amazon.com/servicecatalog/)\n",
    "2. Create a new Project in [Amazon SageMaker Studio](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-projects-create.html)\n",
    "\n",
    "Once complete, you can Train and Deploy machine learning models, and send traffic to the Endpoint to cause the Model Monitor to raise a drift alert.\n",
    "\n",
    "## Get Started\n",
    "\n",
    "Use this following AWS CloudFormation quick start to create a custom [SageMaker MLOps project](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-projects-templates-custom.html) template in the [AWS Service Catalog](https://aws.amazon.com/servicecatalog/) and configure the portfolio and products so you can launch the project from within your Studio domain.\n",
    "\n",
    "[![Launch Stack](https://s3.amazonaws.com/cloudformation-examples/cloudformation-launch-stack.png)](https://console.aws.amazon.com/cloudformation/home?region=us-east-1#/stacks/quickcreate?templateUrl=https%3A%2F%2Faws-ml-blog.s3.amazonaws.com%2Fartifacts%2Famazon-sagemaker-drift-detection%2Fdrift-service-catalog.yml&stackName=drift-pipeline&param_ExecutionRoleArn=&param_PortfolioName=SageMaker%20Organization%20Templates&param_PortfolioOwner=administrator&param_ProductVersion=1.0)\n",
    "\n",
    "Follow are the list of the parameters. \n",
    "\n",
    "| Parameters         | Description                                    |\n",
    "| ------------------ | ---------------------------------------------- |\n",
    "| ExecutionRoleArn   | The SageMaker Studio execution role (required) |\n",
    "| PortfolioName      | The name of the portfolio                      |\n",
    "| PortfolioOwner     | The owner of the portfolio                     |\n",
    "| ProductVersion     | The product version to deploy                  |\n",
    "\n",
    "You can copy the the required `ExecutionRoleArn` role from your **User Details** in the SageMaker Studio dashboard.\n",
    "\n",
    "![Execution Role](docs/studio-execution-role.png)\n",
    "\n",
    "Alternatively see [BUILD.md](BUILD.md) for instructions on how to build the MLOps template from source.\n",
    "\n",
    "## Creating a new Project in Amazon SageMaker Studio\n",
    "\n",
    "Once your MLOps project template is registered in **AWS Service Catalog** you can create a project using your new template.\n",
    "\n",
    "1. Switch back to the Launcher\n",
    "2. Click **New Project** from the **ML tasks and components** section.\n",
    "\n",
    "On the Create project page, SageMaker templates is chosen by default. This option lists the built-in templates. However, you want to use the template you published for Amazon SageMaker drift detection.\n",
    "\n",
    "3. Choose **Organization templates**.\n",
    "4. Choose **Amazon SageMaker drift detection template for real-time deployment**.\n",
    "5. Choose **Select project template**.\n",
    "\n",
    "![Select Template](docs/drift-select-template.png)\n",
    "\n",
    "`NOTE`: If you have recently updated your AWS Service Catalog Project, you may need to refresh SageMaker Studio to ensure it picks up the latest version of your template.\n",
    "\n",
    "6. In the **Project details** section, for **Name**, enter **drift-pipeline**.\n",
    "  - The project name must have 32 characters or fewer.\n",
    "7. In the Project template parameter, for **RetrainSchedule**, input a validate [Cron Schedule](https://docs.aws.amazon.com/sagemaker/latest/dg/model-monitor-schedule-expression.html)\n",
    "  - This defaults to `cron(0 12 1 * ? *)` which is the first day of every month.\n",
    "8. Choose **Create project**.\n",
    "\n",
    "![Create Project](docs/drift-create-project.png)\n",
    "\n",
    "`NOTE`: If the **Create project** button is not enabled, touch the value in the **RetrainSchedule** to allow continuing.\n",
    "\n",
    "### Project Resources\n",
    "\n",
    "The MLOps Drift Detection template will create the following AWS services and resources:\n",
    "\n",
    "1. An [Amazon Simple Storage Service](https://aws.amazon.com/s3/) (Amazon S3) bucket is created for output model artifacts generated from the pipeline.\n",
    "\n",
    "2. Two repositories are added to [AWS CodeCommit](https://aws.amazon.com/codecommit/):\n",
    "  -  The first repository provides code to create a multi-step model building pipeline using [AWS CloudFormation](https://aws.amazon.com/cloudformation/).  The pipeline includes the following steps: data processing, model baseline, model training, model evaluation, and conditional model registration based on accuracy. The pipeline trains a linear regression model using the XGBoost algorithm on trip data from the [NYC Taxi Dataset](https://registry.opendata.aws/nyc-tlc-trip-records-pds/). This repository also includes the [build-pipeline.ipynb](build_pipeline/build-pipeline.ipynb) notebook to [Run the Pipeline](#run-the-pipeline) (see below)\n",
    "  - The second repository contains code and configuration files for model deployment and monitoring. This repo also uses [AWS CodePipeline](https://aws.amazon.com/codepipeline/) and [CodeBuild](https://aws.amazon.com/codebuild/), which run an [AWS CloudFormation](https://aws.amazon.com/cloudformation/) template to create model endpoints for staging and production.  This repository includes the [prod-config.json](deployment_pipeline/prod-config.json) configure to set metrics and threshold for drift detection.\n",
    "\n",
    "3. Two AWS CodePipeline pipelines:\n",
    "  - The [model build pipeline](build_pipeline) creates or updates the pipeline definition and then starts a new execution with a custom [AWS Lambda](https://aws.amazon.com/lambda/) function whenever a new commit is made to the ModelBuild CodeCommit repository. The first time the CodePipeline is started, it will fail to complete expects input data to be uploaded to the Amazon S3 artifact bucket.\n",
    "  - The [deployment pipeline](deployment_pipeline/README.md) automatically triggers whenever a new model version is added to the model registry and the status is marked as Approved. Models that are registered with Pending or Rejected statuses arent deployed.\n",
    "\n",
    "4. [SageMaker Pipelines](https://aws.amazon.com/sagemaker/pipelines) uses the following resources:\n",
    "  - This workflow contains the directed acyclic graph (DAG) that creates a baseline and training job in parallel following up with a step to evaluate the model.  Each step in the pipeline keeps track of the lineage and steps are cached for quickly re-running the pipeline.  \n",
    "  - Within SageMaker Pipelines, the [SageMaker Model Registry](https://docs.aws.amazon.com/sagemaker/latest/dg/model-registry.html) tracks the model versions and respective artifacts, including the lineage and metadata for how they were created. Different model versions are grouped together under a model group, and new models registered to the registry are automatically versioned. The model registry also provides an approval workflow for model versions and supports deployment of models in different accounts. You can also use the model registry through the boto3 package.\n",
    "\n",
    "5. Two SageMaker Endpoints:\n",
    "  - After a model is approved in the registry, the artifact is automatically deployed to a staging endpoint followed by a manual approval step.\n",
    "  - If approved, its deployed to a production endpoint in the same AWS account along with a Model Monitoring schedule configured to detect drift compared against the baseline.\n",
    "\n",
    "6. Two [Amazon Event Bridge](https://aws.amazon.com/eventbridge/) Rules and [CloudWatch](https://aws.amazon.com/cloudwatch/) Alarm:\n",
    "  - One scheduled rule configured to re-train the model on a regular schedule. \n",
    "  - One CloudWatch alarm that will trigger when drift is detected in the Model Monitor and trigger a rule to re-train the model.\n",
    "\n",
    "You will see a summary of these resources in the project page including the Repositories and Pipelines.  The Model groups and Endpoints will become visible after we have completed running the pipeline.\n",
    "\n",
    "## Run the Pipeline\n",
    "\n",
    "Once your project is created, following the instructions to [Clone the Code Repository](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-projects-walkthrough.html#sagemaker-proejcts-walkthrough-clone)\n",
    "\n",
    "![Solution Architecture](docs/drift-clone-repository.png)\n",
    "\n",
    "1. Choose **Repositories**, and in the **Local path** column for the repository that ends with *build*, choose **clone repo....**\n",
    "2. In the dialog box that appears, accept the defaults and choose **Clone repository**\n",
    "3. When clone of the repository is complete, the local path appears in the **Local path** column. Click on the path to open the local folder that contains the repository code in SageMaker Studio.\n",
    "4. Click on the [build-pipeline.ipynb](build_pipeline/build-pipeline.ipynb) file to open the notebook.\n",
    "\n",
    "In the notebook, provide the **Project Name** in the first cell to get started:\n",
    "\n",
    "```\n",
    "project_name = \"<<project_name>>\"  # << Update this drift detection project\n",
    "```\n",
    "\n",
    "Then follow the series of steps in the notebook to run through the sample:\n",
    "\n",
    "1. Fetch the [NYC Taxi Dataset](https://registry.opendata.aws/nyc-tlc-trip-records-pds/) and upload to S3\n",
    "2. Start the model build pipeline\n",
    "3. Review the training job performance\n",
    "4. Update the [Model Registry](https://docs.aws.amazon.com/sagemaker/latest/dg/model-registry-approve.html) status to `Approved`\n",
    "5. Deploy the model to Staging\n",
    "6. Make predictions against the Staging Endpoint\n",
    "7. Manually Approve the Staging endpoint in the [deployment pipeline](deployment_pipeline/README.md)\n",
    "8. Deploy the model to Production\n",
    "9. Make predictions against the Production Endpoint to cause the the Model Monitor to alarm on drift detection.\n",
    "\n",
    "### Model Monitor\n",
    "\n",
    "To [visualize the results](https://docs.aws.amazon.com/sagemaker/latest/dg/model-monitor-interpreting-visualize-results.html) of Model Monitoring in Amazon SageMaker Studio select the Production Endpoint from the **Components and registries** left navigation pane or from the **Endpoints** tab in the project summary.\n",
    "\n",
    "![Solution Architecture](docs/drift-model-monitor.png)\n",
    "\n",
    "Once the Model Monitor **Data Quality** schedule has completed its execution (usually about 10 minutes past the hour) you will be able to navigate to the **Monitoring job history** tab to see that *issue found* will be identified in the **Monitoring status** column.\n",
    "\n",
    "## Running Costs\n",
    "\n",
    "This section outlines cost considerations for running the Drift Detection Pipeline. Completing the pipeline will deploy an endpoint with 2 production variants which will cost less than $8 per day. Further cost breakdowns are below.\n",
    "\n",
    "- **CodeBuild**  Charges per minute used. First 100 minutes each month come at no charge. For information on pricing beyond the first 100 minutes, see [AWS CodeBuild Pricing](https://aws.amazon.com/codebuild/pricing/).\n",
    "- **CodeCommit**  $1/month if you didn't opt to use your own GitHub repository.\n",
    "- **CodePipeline**  CodePipeline costs $1 per active pipeline* per month. Pipelines are free for the first 30 days after creation. More can be found at [AWS CodePipeline Pricing](https://aws.amazon.com/codepipeline/pricing/).\n",
    "- **SageMaker**  Prices vary based on EC2 instance usage for the Notebook Instances, Model Hosting, Model Training and Model Monitoring; each charged per hour of use. For more information, see [Amazon SageMaker Pricing](https://aws.amazon.com/sagemaker/pricing/).\n",
    "  - The three `mml.m5.xlarge` *baseline, training and evaluation jobs* run for approx 20 minutes at $0.23 an hour, and cost less than $1.\n",
    "  - The one `ml.t2.medium` instance for staging *hosting* endpoint costs $0.056 per hour, or $1.34 per day.\n",
    "  - The two `ml.m5.large` instances for production *hosting* endpoint at 2 x $0.115 per hour, or $5.52 per day.\n",
    "  - The one `ml.m5.xlarge` instance for *model monitor* schedule at $0.23 an hour, and cost less than $1 per day.\n",
    "- **S3**  Low cost, prices will vary depending on the size of the models/artifacts stored. The first 50 TB each month will cost only $0.023 per GB stored. For more information, see [Amazon S3 Pricing](https://aws.amazon.com/s3/pricing/).\n",
    "- **Lambda** - Low cost, $0.20 per 1 million request see [AWS Lambda Pricing](https://aws.amazon.com/lambda/pricing/).\n",
    "\n",
    "## Cleaning Up\n",
    "\n",
    "The [build-pipeline.ipynb](build_pipeline/build-pipeline.ipynb) notebook includes cells that you can run to cleanup the resources.\n",
    "\n",
    "1. SageMaker prod endpoint\n",
    "2. SageMaker staging endpoint\n",
    "3. SageMaker Pipeline Workflow and Model Package Group\n",
    "\n",
    "You can also clean up resources using the [AWS Command Line Interface](http://aws.amazon.com/cli) (AWS CLI):\n",
    "\n",
    "1. Delete the CloudFormation stack created to provision the Production endpoint:\n",
    "\n",
    "```\n",
    "aws cloudformation delete-stack --stack-name sagemaker-<<project_name>>-deploy-prod\n",
    "```\n",
    "\n",
    "2. Delete the CloudFormation stack created to provision the Staging endpoint:\n",
    "\n",
    "```\n",
    "aws cloudformation delete-stack --stack-name sagemaker-<<project_name>>-deploy-staging\n",
    "```\n",
    "\n",
    "3. Delete the CloudFormation stack created to provision the SageMaker Pipeline and Model Package Group:\n",
    "\n",
    "```\n",
    "aws cloudformation delete-stack --stack-name sagemaker-<<project_name>>-deploy-pipeline\n",
    "```\n",
    "\n",
    "4. Empty the S3 bucket containing the artifacts output from the drift deployment pipeline:\n",
    "\n",
    "```\n",
    "aws s3 rm --recursive s3://sagemaker-project-<<project_id>>-<<region_name>>\n",
    "```\n",
    "\n",
    "5. Delete the project, which removes the CloudFormation stack that created the deployment pipeline:\n",
    "\n",
    "```\n",
    "aws sagemaker delete-project --project-name <<project_name>>\n",
    "```\n",
    "\n",
    "6. Delete the AWS Service Catalog project template:\n",
    "\n",
    "```\n",
    "aws cloudformation delete-stack --stack-name <<drift-pipeline>>\n",
    "```\n",
    "\n",
    "## Security\n",
    "\n",
    "See [CONTRIBUTING](CONTRIBUTING.md#security-issue-notifications) for more information.\n",
    "\n",
    "## License\n",
    "\n",
    "This library is licensed under the MIT-0 License. See the [LICENSE](LICENSE) file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fba05855-d000-436e-96c7-3c17d075912a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'arn:aws:iam::709891711940:role/service-role/SageMaker-MLOpsEngineer-poc'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5d36128d-6f8c-4487-bd91-fdf5e57d89b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.model_monitor import DataCaptureConfig\n",
    "\n",
    "# Set to True to enable data capture\n",
    "enable_capture = True\n",
    "\n",
    "# Optional - Sampling percentage. Choose an integer value between 0 to 100 \n",
    "# sampling_percentage = int\n",
    "# Sampling percentage = 30 # Example 30%\n",
    "\n",
    "# Optional - The S3 URI of stored captured-data location\n",
    "s3_capture_upload_path = f\"s3://{default_bucket}/my-abalone-capture_data\"\n",
    "\n",
    "# Specify either Input, Output or both.\n",
    "capture_modes = [\"REQUEST\", \"RESPONSE\"] # In this example, we specify both\n",
    "# capture mode = [\"REQUEST\"] # Example - If you want to only capture input.\n",
    "\n",
    "# Configuration object passed in when deploying Models to SM endpoints\n",
    "data_capture_config = DataCaptureConfig(\n",
    "    enable_capture=enable_capture,\n",
    "    # sampling_percentage=sampling_percentage, # Optional\n",
    "    destination_s3_uri=s3_capture_upload_path, # Optional\n",
    "    capture_options=capture_modes\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b1dfe43e-dbc7-47c8-8b45-bc2b52c5ffbc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://sagemaker-us-east-1-709891711940/my-abalone-capture_data'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s3_capture_upload_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7ac25094-88a2-441a-9407-d632ddef5b05",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-2023-05-26-10-26-28-921\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "endpoint_name = '1-2023-05-26-10-26-28-921'\n",
    "# endpoint_name = f'my-abalone-endpoint-{datetime.utcnow():%Y-%m-%d-%H%M}'\n",
    "print(endpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79714b2c-f7d7-44c8-b393-6d51e7db22d3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3eae126d-490d-472b-83ed-856405b323ba",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----!"
     ]
    }
   ],
   "source": [
    "initial_instance_count = 1\n",
    "instance_type = \"ml.m5.xlarge\"\n",
    "\n",
    "model.deploy(\n",
    "    initial_instance_count=initial_instance_count,\n",
    "    instance_type=instance_type,\n",
    "    endpoint_name=endpoint_name,\n",
    "    data_capture_config=data_capture_config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5a3580b1-dd30-42a6-8f65-1b043172a5dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'us-east-1'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ef4fa6af-e141-43f8-a4c7-b23ee7b57b6f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.predictor import Predictor\n",
    "from sagemaker.serializers import CSVSerializer\n",
    "from sagemaker.deserializers import CSVDeserializer\n",
    "\n",
    "predictor = Predictor(endpoint_name=endpoint_name,\n",
    "                      serializer=CSVSerializer,\n",
    "                      deserializer=CSVDeserializer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "5d6ffce0-465e-4e8a-99b1-e10b1a111ada",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The endpoint attribute has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'1-2023-05-26-10-26-28-921'"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor.endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6364890c-27fc-4a1a-b312-dc185f2fd891",
   "metadata": {},
   "source": [
    "# Data Capture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "9d33c496-d465-4277-a0c8-cf2fb1655a5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "........................................................................................................................................................................................................\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "    from time import sleep\n",
    "\n",
    "    validate_dataset = \"validation_with_predictions.csv\"\n",
    "    sm_client = boto3.client(\"sagemaker-runtime\")\n",
    "    # Cut off threshold of 80%\n",
    "    cutoff = 0.8\n",
    "\n",
    "    limit = 200 # Need at least 200 samples to compute standard deviations\n",
    "    i = 0\n",
    "    with open(f\"test_data/{validate_dataset}\", \"w\") as validation_file:\n",
    "        validation_file.write(\"prediction,label\\n\") # CSV header\n",
    "        with open(\"test_data/validation.csv\", \"r\") as f:\n",
    "            for row in f:\n",
    "                (label, input_cols) = row.split(\",\", 1)\n",
    "                res = sm_client.invoke_endpoint(EndpointName=endpoint_name,\n",
    "                              ContentType='text/csv',\n",
    "                              Body=input_cols)\n",
    "                prediction = res[\"Body\"].read().decode()\n",
    "                # prediction = \"1\" if probability > cutoff else \"0\"\n",
    "                validation_file.write(f\"{prediction},{label}\\n\")\n",
    "                i += 1\n",
    "                if i > limit:\n",
    "                    break\n",
    "                print(\".\", end=\"\", flush=True)\n",
    "                sleep(0.5)\n",
    "    print()\n",
    "    print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "7f39b868-f7da-4b7b-8662-94acbd598c0e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "local_path_validation_with_prediction = \"test_data/validation_with_predictions.csv\"\n",
    "base_uri_validation_with_prediction = f\"s3://{default_bucket}/MyAbalonePipeline/7p6mevksvmgc/MyAbaloneProcess/output/validation\"\n",
    "\n",
    "input_data_uri = sagemaker.s3.S3Uploader.upload(\n",
    "    local_path=local_path_validation_with_prediction,\n",
    "    desired_s3_uri=base_uri_validation_with_prediction\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "597ce5d0-8ebb-4e14-a8b4-e79d4927f6e4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://sagemaker-us-east-1-709891711940/MyAbalonePipeline/7p6mevksvmgc/MyAbaloneProcess/output/validation/validation_with_predictions.csv'"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a39623f0-9bac-43e1-9bb7-499d38dd3428",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'arn:aws:iam::709891711940:role/service-role/SageMaker-MLOpsEngineer-poc'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "role"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e52a59-36cd-4ee7-adfa-b782a400cba0",
   "metadata": {},
   "source": [
    "# Create a baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "021f1e01-46af-411b-9d1f-30aa20422450",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:Defaulting to the only supported framework/algorithm version: .\n",
      "INFO:sagemaker.image_uris:Ignoring unnecessary instance type: None.\n",
      "INFO:sagemaker:Creating processing-job with name baseline-suggestion-job-2023-06-13-14-59-49-342\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..........................\u001b[34m2023-06-13 15:04:11,325 - matplotlib.font_manager - INFO - Generating new fontManager, this may take some time...\u001b[0m\n",
      "\u001b[34m2023-06-13 15:04:11.863276: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\u001b[0m\n",
      "\u001b[34m2023-06-13 15:04:11.863306: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\u001b[0m\n",
      "\u001b[34m2023-06-13 15:04:13.397810: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\u001b[0m\n",
      "\u001b[34m2023-06-13 15:04:13.397840: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\u001b[0m\n",
      "\u001b[34m2023-06-13 15:04:13.397860: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (ip-10-0-160-23.ec2.internal): /proc/driver/nvidia/version does not exist\u001b[0m\n",
      "\u001b[34m2023-06-13 15:04:13.398115: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\u001b[0m\n",
      "\u001b[34mTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\u001b[0m\n",
      "\u001b[34m2023-06-13 15:04:14,944 - __main__ - INFO - All params:{'ProcessingJobArn': 'arn:aws:sagemaker:us-east-1:709891711940:processing-job/baseline-suggestion-job-2023-06-13-14-59-49-342', 'ProcessingJobName': 'baseline-suggestion-job-2023-06-13-14-59-49-342', 'Environment': {'dataset_format': '{\"csv\": {\"header\": false, \"output_columns_position\": \"START\"}}', 'dataset_source': '/opt/ml/processing/input/baseline_dataset_input', 'output_path': '/opt/ml/processing/output', 'publish_cloudwatch_metrics': 'Disabled'}, 'AppSpecification': {'ImageUri': '156813124566.dkr.ecr.us-east-1.amazonaws.com/sagemaker-model-monitor-analyzer', 'ContainerEntrypoint': None, 'ContainerArguments': None}, 'ProcessingInputs': [{'InputName': 'baseline_dataset_input', 'AppManaged': False, 'S3Input': {'LocalPath': '/opt/ml/processing/input/baseline_dataset_input', 'S3Uri': 's3://sagemaker-us-east-1-709891711940/MyAbalonePipeline/7p6mevksvmgc/MyAbaloneProcess/output/train/train.csv', 'S3DataDistributionType': 'FullyReplicated', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3CompressionType': 'None', 'S3DownloadMode': 'StartOfJob'}, 'DatasetDefinition': None}], 'ProcessingOutputConfig': {'Outputs': [{'OutputName': 'monitoring_output', 'AppManaged': False, 'S3Output': {'LocalPath': '/opt/ml/processing/output', 'S3Uri': 's3://sagemaker-us-east-1-709891711940/my-abalone-baseline-results', 'S3UploadMode': 'EndOfJob'}, 'FeatureStoreOutput': None}], 'KmsKeyId': None}, 'ProcessingResources': {'ClusterConfig': {'InstanceCount': 1, 'InstanceType': 'ml.m5.xlarge', 'VolumeSizeInGB': 20, 'VolumeKmsKeyId': None}}, 'RoleArn': 'arn:aws:iam::709891711940:role/service-role/SageMaker-MLOpsEngineer-poc', 'StoppingCondition': {'MaxRuntimeInSeconds': 3600}}\u001b[0m\n",
      "\u001b[34m2023-06-13 15:04:14,944 - __main__ - INFO - Current Environment:{'dataset_format': '{\"csv\": {\"header\": false, \"output_columns_position\": \"START\"}}', 'dataset_source': '/opt/ml/processing/input/baseline_dataset_input', 'output_path': '/opt/ml/processing/output', 'publish_cloudwatch_metrics': 'Disabled'}\u001b[0m\n",
      "\u001b[34m2023-06-13 15:04:14,944 - __main__ - INFO - categorical_drift_method:None\u001b[0m\n",
      "\u001b[34m2023-06-13 15:04:14,944 - DefaultDataAnalyzer - INFO - Performing analysis with input: {\"dataset_source\": \"/opt/ml/processing/input/baseline_dataset_input\", \"dataset_format\": {\"csv\": {\"header\": false, \"output_columns_position\": \"START\"}}, \"output_path\": \"/opt/ml/processing/output\", \"monitoring_input_type\": null, \"analysis_type\": null, \"problem_type\": null, \"inference_attribute\": null, \"probability_attribute\": null, \"ground_truth_attribute\": null, \"probability_threshold_attribute\": null, \"positive_label\": null, \"record_preprocessor_script\": null, \"post_analytics_processor_script\": null, \"baseline_constraints\": null, \"baseline_statistics\": null, \"data_quality_monitoring_config\": {\"evaluate_constraints\": \"Enabled\", \"emit_metrics\": \"Enabled\", \"datatype_check_threshold\": 1.0, \"domain_content_threshold\": 1.0, \"distribution_constraints\": {\"perform_comparison\": \"Enabled\", \"comparison_threshold\": 0.1, \"comparison_method\": \"Robust\", \"categorical_comparison_threshold\": 0.1, \"categorical_drift_method\": \"LInfinity\"}}, \"start_time\": null, \"end_time\": null, \"metric_time\": null, \"cloudwatch_metrics_directory\": \"/opt/ml/output/metrics/cloudwatch\", \"publish_cloudwatch_metrics\": \"Disabled\", \"sagemaker_endpoint_name\": null, \"sagemaker_monitoring_schedule_name\": null, \"output_message_file\": \"/opt/ml/output/message\", \"detect_outliers\": null, \"detect_drift\": null, \"image_data\": null, \"report_enabled\": false, \"auto_ml_job_detail\": null}\u001b[0m\n",
      "\u001b[34m2023-06-13 15:04:14,944 - DefaultDataAnalyzer - INFO - Bootstrapping yarn\u001b[0m\n",
      "\u001b[34m2023-06-13 15:04:14,945 - bootstrap - INFO - Copy aws jars\u001b[0m\n",
      "\u001b[34m2023-06-13 15:04:15,002 - bootstrap - INFO - Copy cluster config\u001b[0m\n",
      "\u001b[34m2023-06-13 15:04:15,003 - bootstrap - INFO - Write runtime cluster config\u001b[0m\n",
      "\u001b[34m2023-06-13 15:04:15,003 - bootstrap - INFO - Resource Config is: {'current_host': 'algo-1', 'hosts': ['algo-1']}\u001b[0m\n",
      "\u001b[34m2023-06-13 15:04:15,010 - bootstrap - INFO - Finished Yarn configuration files setup.\u001b[0m\n",
      "\u001b[34m2023-06-13 15:04:15,010 - bootstrap - INFO - Starting spark process for master node algo-1\u001b[0m\n",
      "\u001b[34m2023-06-13 15:04:15,010 - bootstrap - INFO - Running command: /usr/hadoop-3.0.0/bin/hdfs namenode -format -force\u001b[0m\n",
      "\u001b[34mWARNING: /usr/hadoop-3.0.0/logs does not exist. Creating.\u001b[0m\n",
      "\u001b[34m2023-06-13 15:04:15,460 INFO namenode.NameNode: STARTUP_MSG: \u001b[0m\n",
      "\u001b[34m/************************************************************\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG: Starting NameNode\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   host = algo-1/10.0.160.23\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   args = [-format, -force]\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   version = 3.0.0\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   classpath = /usr/hadoop-3.0.0/etc/hadoop:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-server-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/curator-recipes-2.12.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jul-to-slf4j-1.7.25.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/zookeeper-3.4.9.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/junit-4.11.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-databind-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/json-smart-2.3.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jaxb-api-2.2.11.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-util-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/token-provider-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-servlet-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-simplekdc-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-server-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/metrics-core-3.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/hadoop-auth-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/re2j-1.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-util-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jersey-servlet-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-http-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/htrace-core4-4.1.0-incubating.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-net-3.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jcip-annotations-1.0-1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jersey-json-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jettison-1.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/curator-client-2.12.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-common-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerby-config-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-client-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-xml-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jersey-server-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-annotations-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/avro-1.7.7.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/woodstox-core-5.0.3.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/gson-2.2.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/hadoop-annotations-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-beanutils-1.9.3.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/javax.servlet-api-3.1.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerby-util-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-admin-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/accessors-smart-1.2.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-collections-3.2.2.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-lang3-3.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jsr305-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jersey-core-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-identity-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-configuration2-2.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-webapp-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/paranamer-2.3.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-crypto-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerby-asn1-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/httpcore-4.4.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-core-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jsr311-api-1.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/snappy-java-1.0.5.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/stax2-api-3.1.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerby-pkix-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/netty-3.10.5.Final.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/httpclient-4.5.2.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerby-xdr-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/slf4j-api-1.7.25.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/xz-1.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-io-2.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-io-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jsch-0.1.54.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/guava-11.0.2.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/curator-framework-2.12.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/nimbus-jose-jwt-4.41.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/asm-5.0.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-core-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-security-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/aws-java-sdk-bundle-1.11.199.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/hadoop-aws-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/hadoop-kms-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/hadoop-common-3.0.0-tests.jar:/usr/hadoop-3.0.0/share/hadoop/common/hadoop-common-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/hadoop-nfs-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-server-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/curator-recipes-2.12.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/zookeeper-3.4.9.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-databind-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/json-smart-2.3.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jaxb-api-2.2.11.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-util-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/token-provider-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-compress-1.4.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-servlet-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-simplekdc-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/json-simple-1.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-server-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/hadoop-auth-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/re2j-1.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-util-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-util-ajax-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jersey-servlet-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-http-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/htrace-core4-4.1.0-incubating.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-net-3.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jcip-annotations-1.0-1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jersey-json-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jettison-1.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/curator-client-2.12.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-common-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerby-config-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-client-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-xml-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jersey-server-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-annotations-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-xc-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/avro-1.7.7.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/woodstox-core-5.0.3.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/gson-2.2.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/hadoop-annotations-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/okio-1.4.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-beanutils-1.9.3.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/javax.servlet-api-3.1.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerby-util-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-admin-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/accessors-smart-1.2.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-collections-3.2.2.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-lang3-3.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jaxb-impl-2.2.3-1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jersey-core-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-identity-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-configuration2-2.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-webapp-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/paranamer-2.3.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-crypto-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-math3-3.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerby-asn1-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/httpcore-4.4.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-core-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jsr311-api-1.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/snappy-java-1.0.5.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-jaxrs-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/stax2-api-3.1.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerby-pkix-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/netty-3.10.5.Final.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/httpclient-4.5.2.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerby-xdr-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/xz-1.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/okhttp-2.4.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-io-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jsch-0.1.54.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/curator-framework-2.12.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/nimbus-jose-jwt-4.41.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/asm-5.0.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-core-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-security-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-native-client-3.0.0-tests.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-client-3.0.0-tests.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-native-client-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-client-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-httpfs-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-nfs-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-3.0.0-tests.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-core-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-common-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.0.0-tests.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-nativetask-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-app-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-annotations-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/json-io-2.5.1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jsp-api-2.1-6.1.14.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/joni-2.1.2.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/guice-servlet-4.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/metrics-core-3.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-hadoop2-compat-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jamon-runtime-2.4.1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/commons-math-2.2.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jasper-compiler-5.5.23.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-common-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-server-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-client-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jasper-runtime-5.5.23.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/commons-csv-1.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/disruptor-3.3.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-procedure-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jackson-jaxrs-base-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/htrace-core-3.1.0-incubating.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jsp-2.1-6.1.14.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jersey-client-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jackson-module-jaxb-annotations-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/java-util-1.9.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/commons-el-1.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/ehcache-3.3.1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jcodings-1.0.8.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/findbugs-annotations-1.3.9-1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/metrics-core-2.2.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/guice-4.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-hadoop-compat-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-prefix-tree-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-protocol-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/fst-2.50.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jackson-jaxrs-json-provider-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jersey-guice-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/servlet-api-2.5-6.1.14.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/HikariCP-java7-2.4.12.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/commons-httpclient-3.1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-nodemanager-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-timelineservice-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-timelineservice-hbase-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-registry-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-timelineservice-hbase-tests-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-applicationhistor\u001b[0m\n",
      "\u001b[34myservice-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-api-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-router-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-client-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-tests-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-web-proxy-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-common-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-common-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-3.0.0.jar\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r c25427ceca461ee979d30edd7a4b0f50718e6533; compiled by 'andrew' on 2017-12-08T19:16Z\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   java = 1.8.0_362\u001b[0m\n",
      "\u001b[34m************************************************************/\u001b[0m\n",
      "\u001b[34m2023-06-13 15:04:15,469 INFO namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]\u001b[0m\n",
      "\u001b[34m2023-06-13 15:04:15,472 INFO namenode.NameNode: createNameNode [-format, -force]\u001b[0m\n",
      "\u001b[34mFormatting using clusterid: CID-3f136b21-1aca-4548-98dd-9ce061981770\u001b[0m\n",
      "\u001b[34m2023-06-13 15:04:15,979 INFO namenode.FSEditLog: Edit logging is async:true\u001b[0m\n",
      "\u001b[34m2023-06-13 15:04:15,994 INFO namenode.FSNamesystem: KeyProvider: null\u001b[0m\n",
      "\u001b[34m2023-06-13 15:04:15,995 INFO namenode.FSNamesystem: fsLock is fair: true\u001b[0m\n",
      "\u001b[34m2023-06-13 15:04:15,997 INFO namenode.FSNamesystem: Detailed lock hold time metrics enabled: false\u001b[0m\n",
      "\u001b[34m2023-06-13 15:04:16,001 INFO namenode.FSNamesystem: fsOwner             = root (auth:SIMPLE)\u001b[0m\n",
      "\u001b[34m2023-06-13 15:04:16,001 INFO namenode.FSNamesystem: supergroup          = supergroup\u001b[0m\n",
      "\u001b[34m2023-06-13 15:04:16,001 INFO namenode.FSNamesystem: isPermissionEnabled = true\u001b[0m\n",
      "\u001b[34m2023-06-13 15:04:16,001 INFO namenode.FSNamesystem: HA Enabled: false\u001b[0m\n",
      "\u001b[34m2023-06-13 15:04:16,031 INFO common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling\u001b[0m\n",
      "\u001b[34m2023-06-13 15:04:16,043 INFO blockmanagement.DatanodeManager: dfs.block.invalidate.limit: configured=1000, counted=60, effected=1000\u001b[0m\n",
      "\u001b[34m2023-06-13 15:04:16,043 INFO blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true\u001b[0m\n",
      "\u001b[34m2023-06-13 15:04:16,046 INFO blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000\u001b[0m\n",
      "\u001b[34m2023-06-13 15:04:16,050 INFO blockmanagement.BlockManager: The block deletion will start around 2023 Jun 13 15:04:16\u001b[0m\n",
      "\u001b[34m2023-06-13 15:04:16,051 INFO util.GSet: Computing capacity for map BlocksMap\u001b[0m\n",
      "\u001b[34m2023-06-13 15:04:16,051 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m2023-06-13 15:04:16,052 INFO util.GSet: 2.0% max memory 3.1 GB = 63.8 MB\u001b[0m\n",
      "\u001b[34m2023-06-13 15:04:16,052 INFO util.GSet: capacity      = 2^23 = 8388608 entries\u001b[0m\n",
      "\u001b[34m2023-06-13 15:04:16,087 INFO blockmanagement.BlockManager: dfs.block.access.token.enable = false\u001b[0m\n",
      "\u001b[34m2023-06-13 15:04:16,090 INFO Configuration.deprecation: No unit for dfs.namenode.safemode.extension(30000) assuming MILLISECONDS\u001b[0m\n",
      "\u001b[34m2023-06-13 15:04:16,090 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.threshold-pct = 0.9990000128746033\u001b[0m\n",
      "\u001b[34m2023-06-13 15:04:16,091 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.min.datanodes = 0\u001b[0m\n",
      "\u001b[34m2023-06-13 15:04:16,091 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.extension = 30000\u001b[0m\n",
      "\u001b[34m2023-06-13 15:04:16,091 INFO blockmanagement.BlockManager: defaultReplication         = 3\u001b[0m\n",
      "\u001b[34m2023-06-13 15:04:16,091 INFO blockmanagement.BlockManager: maxReplication             = 512\u001b[0m\n",
      "\u001b[34m2023-06-13 15:04:16,091 INFO blockmanagement.BlockManager: minReplication             = 1\u001b[0m\n",
      "\u001b[34m2023-06-13 15:04:16,091 INFO blockmanagement.BlockManager: maxReplicationStreams      = 2\u001b[0m\n",
      "\u001b[34m2023-06-13 15:04:16,091 INFO blockmanagement.BlockManager: redundancyRecheckInterval  = 3000ms\u001b[0m\n",
      "\u001b[34m2023-06-13 15:04:16,091 INFO blockmanagement.BlockManager: encryptDataTransfer        = false\u001b[0m\n",
      "\u001b[34m2023-06-13 15:04:16,091 INFO blockmanagement.BlockManager: maxNumBlocksToLog          = 1000\u001b[0m\n",
      "\u001b[34m2023-06-13 15:04:16,116 INFO util.GSet: Computing capacity for map INodeMap\u001b[0m\n",
      "\u001b[34m2023-06-13 15:04:16,116 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m2023-06-13 15:04:16,116 INFO util.GSet: 1.0% max memory 3.1 GB = 31.9 MB\u001b[0m\n",
      "\u001b[34m2023-06-13 15:04:16,116 INFO util.GSet: capacity      = 2^22 = 4194304 entries\u001b[0m\n",
      "\u001b[34m2023-06-13 15:04:16,118 INFO namenode.FSDirectory: ACLs enabled? false\u001b[0m\n",
      "\u001b[34m2023-06-13 15:04:16,118 INFO namenode.FSDirectory: POSIX ACL inheritance enabled? true\u001b[0m\n",
      "\u001b[34m2023-06-13 15:04:16,118 INFO namenode.FSDirectory: XAttrs enabled? true\u001b[0m\n",
      "\u001b[34m2023-06-13 15:04:16,118 INFO namenode.NameNode: Caching file names occurring more than 10 times\u001b[0m\n",
      "\u001b[34m2023-06-13 15:04:16,122 INFO snapshot.SnapshotManager: Loaded config captureOpenFiles: false, skipCaptureAccessTimeOnlyChange: false, snapshotDiffAllowSnapRootDescendant: true\u001b[0m\n",
      "\u001b[34m2023-06-13 15:04:16,126 INFO util.GSet: Computing capacity for map cachedBlocks\u001b[0m\n",
      "\u001b[34m2023-06-13 15:04:16,126 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m2023-06-13 15:04:16,126 INFO util.GSet: 0.25% max memory 3.1 GB = 8.0 MB\u001b[0m\n",
      "\u001b[34m2023-06-13 15:04:16,126 INFO util.GSet: capacity      = 2^20 = 1048576 entries\u001b[0m\n",
      "\u001b[34m2023-06-13 15:04:16,163 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10\u001b[0m\n",
      "\u001b[34m2023-06-13 15:04:16,163 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10\u001b[0m\n",
      "\u001b[34m2023-06-13 15:04:16,163 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25\u001b[0m\n",
      "\u001b[34m2023-06-13 15:04:16,166 INFO namenode.FSNamesystem: Retry cache on namenode is enabled\u001b[0m\n",
      "\u001b[34m2023-06-13 15:04:16,166 INFO namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis\u001b[0m\n",
      "\u001b[34m2023-06-13 15:04:16,168 INFO util.GSet: Computing capacity for map NameNodeRetryCache\u001b[0m\n",
      "\u001b[34m2023-06-13 15:04:16,168 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m2023-06-13 15:04:16,168 INFO util.GSet: 0.029999999329447746% max memory 3.1 GB = 980.4 KB\u001b[0m\n",
      "\u001b[34m2023-06-13 15:04:16,168 INFO util.GSet: capacity      = 2^17 = 131072 entries\u001b[0m\n",
      "\u001b[34m2023-06-13 15:04:16,188 INFO namenode.FSImage: Allocated new BlockPoolId: BP-49095236-10.0.160.23-1686668656182\u001b[0m\n",
      "\u001b[34m2023-06-13 15:04:16,200 INFO common.Storage: Storage directory /opt/amazon/hadoop/hdfs/namenode has been successfully formatted.\u001b[0m\n",
      "\u001b[34m2023-06-13 15:04:16,207 INFO namenode.FSImageFormatProtobuf: Saving image file /opt/amazon/hadoop/hdfs/namenode/current/fsimage.ckpt_0000000000000000000 using no compression\u001b[0m\n",
      "\u001b[34m2023-06-13 15:04:16,281 INFO namenode.FSImageFormatProtobuf: Image file /opt/amazon/hadoop/hdfs/namenode/current/fsimage.ckpt_0000000000000000000 of size 389 bytes saved in 0 seconds.\u001b[0m\n",
      "\u001b[34m2023-06-13 15:04:16,293 INFO namenode.NNStorageRetentionManager: Going to retain 1 images with txid >= 0\u001b[0m\n",
      "\u001b[34m2023-06-13 15:04:16,296 INFO namenode.NameNode: SHUTDOWN_MSG: \u001b[0m\n",
      "\u001b[34m/************************************************************\u001b[0m\n",
      "\u001b[34mSHUTDOWN_MSG: Shutting down NameNode at algo-1/10.0.160.23\u001b[0m\n",
      "\u001b[34m************************************************************/\u001b[0m\n",
      "\u001b[34m2023-06-13 15:04:16,306 - bootstrap - INFO - Running command: /usr/hadoop-3.0.0/bin/hdfs --daemon start namenode\u001b[0m\n",
      "\u001b[34m2023-06-13 15:04:18,369 - bootstrap - INFO - Failed to run /usr/hadoop-3.0.0/bin/hdfs --daemon start namenode, return code 1\u001b[0m\n",
      "\u001b[34m2023-06-13 15:04:18,369 - bootstrap - INFO - Running command: /usr/hadoop-3.0.0/bin/hdfs --daemon start datanode\u001b[0m\n",
      "\u001b[34m2023-06-13 15:04:20,427 - bootstrap - INFO - Failed to run /usr/hadoop-3.0.0/bin/hdfs --daemon start datanode, return code 1\u001b[0m\n",
      "\u001b[34m2023-06-13 15:04:20,428 - bootstrap - INFO - Running command: /usr/hadoop-3.0.0/bin/yarn --daemon start resourcemanager\u001b[0m\n",
      "\u001b[34mWARNING: YARN_LOG_DIR has been replaced by HADOOP_LOG_DIR. Using value of YARN_LOG_DIR.\u001b[0m\n",
      "\u001b[34mWARNING: /var/log/yarn/ does not exist. Creating.\u001b[0m\n",
      "\u001b[34m2023-06-13 15:04:22,536 - bootstrap - INFO - Failed to run /usr/hadoop-3.0.0/bin/yarn --daemon start resourcemanager, return code 1\u001b[0m\n",
      "\u001b[34m2023-06-13 15:04:22,536 - bootstrap - INFO - Running command: /usr/hadoop-3.0.0/bin/yarn --daemon start nodemanager\u001b[0m\n",
      "\u001b[34mWARNING: YARN_LOG_DIR has been replaced by HADOOP_LOG_DIR. Using value of YARN_LOG_DIR.\u001b[0m\n",
      "\u001b[34m2023-06-13 15:04:24,659 - bootstrap - INFO - Failed to run /usr/hadoop-3.0.0/bin/yarn --daemon start nodemanager, return code 1\u001b[0m\n",
      "\u001b[34m2023-06-13 15:04:24,659 - bootstrap - INFO - Running command: /usr/hadoop-3.0.0/bin/yarn --daemon start proxyserver\u001b[0m\n",
      "\u001b[34mWARNING: YARN_LOG_DIR has been replaced by HADOOP_LOG_DIR. Using value of YARN_LOG_DIR.\u001b[0m\n",
      "\u001b[34m2023-06-13 15:04:26,819 - bootstrap - INFO - Failed to run /usr/hadoop-3.0.0/bin/yarn --daemon start proxyserver, return code 1\u001b[0m\n",
      "\u001b[34m2023-06-13 15:04:26,819 - DefaultDataAnalyzer - INFO - Total number of hosts in the cluster: 1\u001b[0m\n",
      "\u001b[34m2023-06-13 15:04:36,830 - DefaultDataAnalyzer - INFO - Running command: bin/spark-submit --master yarn --deploy-mode client --conf spark.hadoop.fs.s3a.aws.credentials.provider=org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider --conf spark.serializer=org.apache.spark.serializer.KryoSerializer /opt/amazon/sagemaker-data-analyzer-1.0-jar-with-dependencies.jar --analytics_input /tmp/spark_job_config.json\u001b[0m\n",
      "\u001b[34m2023-06-13 15:04:38,458 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\u001b[0m\n",
      "\u001b[34m2023-06-13 15:04:38,846 INFO Main: Start analyzing with args: --analytics_input /tmp/spark_job_config.json\u001b[0m\n",
      "\u001b[34m2023-06-13 15:04:38,881 INFO Main: Analytics input path: DataAnalyzerParams(/tmp/spark_job_config.json,yarn)\u001b[0m\n",
      "\u001b[34m2023-06-13 15:04:38,895 INFO FileUtil: Read file from path /tmp/spark_job_config.json.\u001b[0m\n",
      "\u001b[34m2023-06-13 15:04:39,393 INFO spark.SparkContext: Running Spark version 3.3.0\u001b[0m\n",
      "\u001b[34m2023-06-13 15:04:39,417 INFO resource.ResourceUtils: ==============================================================\u001b[0m\n",
      "\u001b[34m2023-06-13 15:04:39,417 INFO resource.ResourceUtils: No custom resources configured for spark.driver.\u001b[0m\n",
      "\u001b[34m2023-06-13 15:04:39,417 INFO resource.ResourceUtils: ==============================================================\u001b[0m\n",
      "\u001b[34m2023-06-13 15:04:39,418 INFO spark.SparkContext: Submitted application: SageMakerDataAnalyzer\u001b[0m\n",
      "\u001b[34m2023-06-13 15:04:39,447 INFO resource.ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 3, script: , vendor: , memory -> name: memory, amount: 11536, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\u001b[0m\n",
      "\u001b[34m2023-06-13 15:04:39,462 INFO resource.ResourceProfile: Limiting resource is cpus at 3 tasks per executor\u001b[0m\n",
      "\u001b[34m2023-06-13 15:04:39,464 INFO resource.ResourceProfileManager: Added ResourceProfile id: 0\u001b[0m\n",
      "\u001b[34m2023-06-13 15:04:39,513 INFO spark.SecurityManager: Changing view acls to: root\u001b[0m\n",
      "\u001b[34m2023-06-13 15:04:39,513 INFO spark.SecurityManager: Changing modify acls to: root\u001b[0m\n",
      "\u001b[34m2023-06-13 15:04:39,513 INFO spark.SecurityManager: Changing view acls groups to: \u001b[0m\n",
      "\u001b[34m2023-06-13 15:04:39,514 INFO spark.SecurityManager: Changing modify acls groups to: \u001b[0m\n",
      "\u001b[34m2023-06-13 15:04:39,514 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\u001b[0m\n",
      "\u001b[34m2023-06-13 15:04:39,856 INFO util.Utils: Successfully started service 'sparkDriver' on port 42161.\u001b[0m\n",
      "\u001b[34m2023-06-13 15:04:39,885 INFO spark.SparkEnv: Registering MapOutputTracker\u001b[0m\n",
      "\u001b[34m2023-06-13 15:04:39,921 INFO spark.SparkEnv: Registering BlockManagerMaster\u001b[0m\n",
      "\u001b[34m2023-06-13 15:04:39,943 INFO storage.BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\u001b[0m\n",
      "\u001b[34m2023-06-13 15:04:39,943 INFO storage.BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\u001b[0m\n",
      "\u001b[34m2023-06-13 15:04:39,976 INFO spark.SparkEnv: Registering BlockManagerMasterHeartbeat\u001b[0m\n",
      "\u001b[34m2023-06-13 15:04:39,999 INFO storage.DiskBlockManager: Created local directory at /tmp/blockmgr-584cca06-3a97-4dbf-ae9e-31b42d19b56d\u001b[0m\n",
      "\u001b[34m2023-06-13 15:04:40,017 INFO memory.MemoryStore: MemoryStore started with capacity 1458.6 MiB\u001b[0m\n",
      "\u001b[34m2023-06-13 15:04:40,055 INFO spark.SparkEnv: Registering OutputCommitCoordinator\u001b[0m\n",
      "\u001b[34m2023-06-13 15:04:40,088 INFO spark.SparkContext: Added JAR file:/opt/amazon/sagemaker-data-analyzer-1.0-jar-with-dependencies.jar at spark://10.0.160.23:42161/jars/sagemaker-data-analyzer-1.0-jar-with-dependencies.jar with timestamp 1686668679388\u001b[0m\n",
      "\u001b[34m2023-06-13 15:04:40,626 INFO client.RMProxy: Connecting to ResourceManager at /10.0.160.23:8032\u001b[0m\n",
      "\u001b[34m2023-06-13 15:04:41,300 INFO conf.Configuration: resource-types.xml not found\u001b[0m\n",
      "\u001b[34m2023-06-13 15:04:41,301 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\u001b[0m\n",
      "\u001b[34m2023-06-13 15:04:41,309 INFO yarn.Client: Verifying our application has not requested more than the maximum memory capability of the cluster (15731 MB per container)\u001b[0m\n",
      "\u001b[34m2023-06-13 15:04:41,309 INFO yarn.Client: Will allocate AM container, with 896 MB memory including 384 MB overhead\u001b[0m\n",
      "\u001b[34m2023-06-13 15:04:41,310 INFO yarn.Client: Setting up container launch context for our AM\u001b[0m\n",
      "\u001b[34m2023-06-13 15:04:41,310 INFO yarn.Client: Setting up the launch environment for our AM container\u001b[0m\n",
      "\u001b[34m2023-06-13 15:04:41,318 INFO yarn.Client: Preparing resources for our AM container\u001b[0m\n",
      "\u001b[34m2023-06-13 15:04:41,407 WARN yarn.Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\u001b[0m\n",
      "\u001b[34m2023-06-13 15:04:43,227 INFO yarn.Client: Uploading resource file:/tmp/spark-36e0c8e4-1b32-422a-b989-479ad9565ec4/__spark_libs__3398713469153606137.zip -> hdfs://10.0.160.23/user/root/.sparkStaging/application_1686668662029_0001/__spark_libs__3398713469153606137.zip\u001b[0m\n",
      "\u001b[34m2023-06-13 15:04:44,579 INFO yarn.Client: Uploading resource file:/tmp/spark-36e0c8e4-1b32-422a-b989-479ad9565ec4/__spark_conf__1446846501798339126.zip -> hdfs://10.0.160.23/user/root/.sparkStaging/application_1686668662029_0001/__spark_conf__.zip\u001b[0m\n",
      "\u001b[34m2023-06-13 15:04:45,033 INFO spark.SecurityManager: Changing view acls to: root\u001b[0m\n",
      "\u001b[34m2023-06-13 15:04:45,033 INFO spark.SecurityManager: Changing modify acls to: root\u001b[0m\n",
      "\u001b[34m2023-06-13 15:04:45,033 INFO spark.SecurityManager: Changing view acls groups to: \u001b[0m\n",
      "\u001b[34m2023-06-13 15:04:45,034 INFO spark.SecurityManager: Changing modify acls groups to: \u001b[0m\n",
      "\u001b[34m2023-06-13 15:04:45,034 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\u001b[0m\n",
      "\u001b[34m2023-06-13 15:04:45,069 INFO yarn.Client: Submitting application application_1686668662029_0001 to ResourceManager\u001b[0m\n",
      "\u001b[34m2023-06-13 15:04:45,257 INFO impl.YarnClientImpl: Submitted application application_1686668662029_0001\u001b[0m\n",
      "\u001b[34m2023-06-13 15:04:46,291 INFO yarn.Client: Application report for application_1686668662029_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2023-06-13 15:04:46,295 INFO yarn.Client: \u001b[0m\n",
      "\u001b[34m#011 client token: N/A\u001b[0m\n",
      "\u001b[34m#011 diagnostics: [Tue Jun 13 15:04:46 +0000 2023] Scheduler has assigned a container for AM, waiting for AM container to be launched\u001b[0m\n",
      "\u001b[34m#011 ApplicationMaster host: N/A\u001b[0m\n",
      "\u001b[34m#011 ApplicationMaster RPC port: -1\u001b[0m\n",
      "\u001b[34m#011 queue: default\u001b[0m\n",
      "\u001b[34m#011 start time: 1686668685166\u001b[0m\n",
      "\u001b[34m#011 final status: UNDEFINED\u001b[0m\n",
      "\u001b[34m#011 tracking URL: http://algo-1:8088/proxy/application_1686668662029_0001/\u001b[0m\n",
      "\u001b[34m#011 user: root\u001b[0m\n",
      "\u001b[34m2023-06-13 15:04:47,299 INFO yarn.Client: Application report for application_1686668662029_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2023-06-13 15:04:48,301 INFO yarn.Client: Application report for application_1686668662029_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2023-06-13 15:04:49,303 INFO yarn.Client: Application report for application_1686668662029_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2023-06-13 15:04:50,307 INFO yarn.Client: Application report for application_1686668662029_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2023-06-13 15:04:51,090 INFO cluster.YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> algo-1, PROXY_URI_BASES -> http://algo-1:8088/proxy/application_1686668662029_0001), /proxy/application_1686668662029_0001\u001b[0m\n",
      "\u001b[34m2023-06-13 15:04:51,311 INFO yarn.Client: Application report for application_1686668662029_0001 (state: RUNNING)\u001b[0m\n",
      "\u001b[34m2023-06-13 15:04:51,312 INFO yarn.Client: \u001b[0m\n",
      "\u001b[34m#011 client token: N/A\u001b[0m\n",
      "\u001b[34m#011 diagnostics: N/A\u001b[0m\n",
      "\u001b[34m#011 ApplicationMaster host: 10.0.160.23\u001b[0m\n",
      "\u001b[34m#011 ApplicationMaster RPC port: -1\u001b[0m\n",
      "\u001b[34m#011 queue: default\u001b[0m\n",
      "\u001b[34m#011 start time: 1686668685166\u001b[0m\n",
      "\u001b[34m#011 final status: UNDEFINED\u001b[0m\n",
      "\u001b[34m#011 tracking URL: http://algo-1:8088/proxy/application_1686668662029_0001/\u001b[0m\n",
      "\u001b[34m#011 user: root\u001b[0m\n",
      "\u001b[34m2023-06-13 15:04:51,314 INFO cluster.YarnClientSchedulerBackend: Application application_1686668662029_0001 has started running.\u001b[0m\n",
      "\u001b[34m2023-06-13 15:04:51,325 INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 42449.\u001b[0m\n",
      "\u001b[34m2023-06-13 15:04:51,325 INFO netty.NettyBlockTransferService: Server created on 10.0.160.23:42449\u001b[0m\n",
      "\u001b[34m2023-06-13 15:04:51,327 INFO storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\u001b[0m\n",
      "\u001b[34m2023-06-13 15:04:51,337 INFO storage.BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 10.0.160.23, 42449, None)\u001b[0m\n",
      "\u001b[34m2023-06-13 15:04:51,341 INFO storage.BlockManagerMasterEndpoint: Registering block manager 10.0.160.23:42449 with 1458.6 MiB RAM, BlockManagerId(driver, 10.0.160.23, 42449, None)\u001b[0m\n",
      "\u001b[34m2023-06-13 15:04:51,344 INFO storage.BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 10.0.160.23, 42449, None)\u001b[0m\n",
      "\u001b[34m2023-06-13 15:04:51,346 INFO storage.BlockManager: Initialized BlockManager: BlockManagerId(driver, 10.0.160.23, 42449, None)\u001b[0m\n",
      "\u001b[34m2023-06-13 15:04:51,524 INFO util.log: Logging initialized @14450ms to org.sparkproject.jetty.util.log.Slf4jLog\u001b[0m\n",
      "\u001b[34m2023-06-13 15:04:52,712 INFO cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM)\u001b[0m\n",
      "\u001b[34m2023-06-13 15:04:56,286 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.0.160.23:50750) with ID 1,  ResourceProfileId 0\u001b[0m\n",
      "\u001b[34m2023-06-13 15:04:56,461 INFO storage.BlockManagerMasterEndpoint: Registering block manager algo-1:41915 with 5.8 GiB RAM, BlockManagerId(1, algo-1, 41915, None)\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:10,515 INFO cluster.YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after waiting maxRegisteredResourcesWaitingTime: 30000000000(ns)\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:10,717 WARN spark.SparkContext: Spark is not running in local mode, therefore the checkpoint directory must not be on the local filesystem. Directory '/tmp' appears to be on the local filesystem.\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:10,772 INFO internal.SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:10,778 INFO internal.SharedState: Warehouse path is 'file:/usr/spark-3.3.0/spark-warehouse'.\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:11,958 INFO datasources.InMemoryFileIndex: It took 45 ms to list leaf files for 1 paths.\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:12,145 INFO memory.MemoryStore: Block broadcast_0 stored as values in memory (estimated size 416.9 KiB, free 1458.2 MiB)\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:12,430 INFO memory.MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 39.2 KiB, free 1458.2 MiB)\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:12,433 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.0.160.23:42449 (size: 39.2 KiB, free: 1458.6 MiB)\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:12,437 INFO spark.SparkContext: Created broadcast 0 from csv at DatasetReader.scala:99\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:12,790 INFO input.FileInputFormat: Total input files to process : 1\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:12,793 INFO input.FileInputFormat: Total input files to process : 1\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:12,796 INFO input.CombineFileInputFormat: DEBUG: Terminated node allocation with : CompletedNodes: 1, size left: 449137\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:12,855 INFO spark.SparkContext: Starting job: csv at DatasetReader.scala:99\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:12,873 INFO scheduler.DAGScheduler: Got job 0 (csv at DatasetReader.scala:99) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:12,874 INFO scheduler.DAGScheduler: Final stage: ResultStage 0 (csv at DatasetReader.scala:99)\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:12,874 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:12,876 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:12,883 INFO scheduler.DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at csv at DatasetReader.scala:99), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:12,940 INFO memory.MemoryStore: Block broadcast_1 stored as values in memory (estimated size 7.3 KiB, free 1458.1 MiB)\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:12,943 INFO memory.MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 4.2 KiB, free 1458.1 MiB)\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:12,944 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on 10.0.160.23:42449 (size: 4.2 KiB, free: 1458.6 MiB)\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:12,945 INFO spark.SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:12,961 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at csv at DatasetReader.scala:99) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:12,962 INFO cluster.YarnScheduler: Adding task set 0.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:13,004 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4618 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:13,240 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on algo-1:41915 (size: 4.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:15,161 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on algo-1:41915 (size: 39.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:17,481 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 4490 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:17,483 INFO cluster.YarnScheduler: Removed TaskSet 0.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:17,489 INFO scheduler.DAGScheduler: ResultStage 0 (csv at DatasetReader.scala:99) finished in 4.575 s\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:17,499 INFO scheduler.DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:17,499 INFO cluster.YarnScheduler: Killing all running tasks in stage 0: Stage finished\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:17,501 INFO scheduler.DAGScheduler: Job 0 finished: csv at DatasetReader.scala:99, took 4.645691 s\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:17,684 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on 10.0.160.23:42449 in memory (size: 4.2 KiB, free: 1458.6 MiB)\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:17,715 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on algo-1:41915 in memory (size: 4.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:19,805 INFO datasources.FileSourceStrategy: Pushed Filters: \u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:19,807 INFO datasources.FileSourceStrategy: Post-Scan Filters: \u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:19,809 INFO datasources.FileSourceStrategy: Output Data Schema: struct<_c0: string, _c1: string, _c2: string, _c3: string, _c4: string ... 9 more fields>\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:19,991 INFO memory.MemoryStore: Block broadcast_2 stored as values in memory (estimated size 416.5 KiB, free 1457.7 MiB)\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:20,011 INFO memory.MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 39.1 KiB, free 1457.7 MiB)\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:20,012 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on 10.0.160.23:42449 (size: 39.1 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:20,014 INFO spark.SparkContext: Created broadcast 2 from head at DataAnalyzer.scala:100\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:20,029 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:20,067 INFO spark.SparkContext: Starting job: head at DataAnalyzer.scala:100\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:20,068 INFO scheduler.DAGScheduler: Got job 1 (head at DataAnalyzer.scala:100) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:20,068 INFO scheduler.DAGScheduler: Final stage: ResultStage 1 (head at DataAnalyzer.scala:100)\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:20,068 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:20,071 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:20,074 INFO scheduler.DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[11] at head at DataAnalyzer.scala:100), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:20,128 INFO memory.MemoryStore: Block broadcast_3 stored as values in memory (estimated size 16.7 KiB, free 1457.7 MiB)\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:20,131 INFO memory.MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 7.9 KiB, free 1457.7 MiB)\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:20,132 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on 10.0.160.23:42449 (size: 7.9 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:20,133 INFO spark.SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:20,133 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[11] at head at DataAnalyzer.scala:100) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:20,134 INFO cluster.YarnScheduler: Adding task set 1.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:20,137 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4946 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:20,181 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on algo-1:41915 (size: 7.9 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:21,029 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on algo-1:41915 (size: 39.1 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:21,268 INFO storage.BlockManagerInfo: Added rdd_7_0 in memory on algo-1:41915 (size: 179.8 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:21,400 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 1265 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:21,400 INFO cluster.YarnScheduler: Removed TaskSet 1.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:21,401 INFO scheduler.DAGScheduler: ResultStage 1 (head at DataAnalyzer.scala:100) finished in 1.324 s\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:21,405 INFO scheduler.DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:21,406 INFO cluster.YarnScheduler: Killing all running tasks in stage 1: Stage finished\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:21,406 INFO scheduler.DAGScheduler: Job 1 finished: head at DataAnalyzer.scala:100, took 1.339394 s\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:21,481 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on algo-1:41915 in memory (size: 7.9 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:21,482 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on 10.0.160.23:42449 in memory (size: 7.9 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:21,737 INFO codegen.CodeGenerator: Code generated in 193.187338 ms\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:22,223 WARN util.package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:22,362 INFO scheduler.DAGScheduler: Registering RDD 16 (collect at AnalysisRunner.scala:326) as input to shuffle 0\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:22,365 INFO scheduler.DAGScheduler: Got map stage job 2 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:22,365 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 2 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:22,366 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:22,368 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:22,370 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 2 (MapPartitionsRDD[16] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:22,388 INFO memory.MemoryStore: Block broadcast_4 stored as values in memory (estimated size 113.9 KiB, free 1457.6 MiB)\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:22,391 INFO memory.MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 34.6 KiB, free 1457.6 MiB)\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:22,392 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on 10.0.160.23:42449 (size: 34.6 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:22,392 INFO spark.SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:22,394 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[16] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:22,394 INFO cluster.YarnScheduler: Adding task set 2.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:22,401 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4935 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:22,438 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on algo-1:41915 (size: 34.6 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:23,653 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 1253 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:23,653 INFO cluster.YarnScheduler: Removed TaskSet 2.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:23,655 INFO scheduler.DAGScheduler: ShuffleMapStage 2 (collect at AnalysisRunner.scala:326) finished in 1.283 s\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:23,656 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:23,656 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:23,657 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:23,657 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:23,752 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:23,754 INFO scheduler.DAGScheduler: Got job 3 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:23,754 INFO scheduler.DAGScheduler: Final stage: ResultStage 4 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:23,754 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 3)\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:23,754 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:23,756 INFO scheduler.DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[19] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:23,773 INFO memory.MemoryStore: Block broadcast_5 stored as values in memory (estimated size 166.4 KiB, free 1457.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:23,775 INFO memory.MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 45.7 KiB, free 1457.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:23,776 INFO storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on 10.0.160.23:42449 (size: 45.7 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:23,777 INFO spark.SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:23,777 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[19] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:23,777 INFO cluster.YarnScheduler: Adding task set 4.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:23,780 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 4.0 (TID 3) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:23,811 INFO storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on algo-1:41915 (size: 45.7 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:23,893 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 10.0.160.23:50750\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:24,299 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 4.0 (TID 3) in 520 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:24,299 INFO cluster.YarnScheduler: Removed TaskSet 4.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:24,301 INFO scheduler.DAGScheduler: ResultStage 4 (collect at AnalysisRunner.scala:326) finished in 0.539 s\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:24,302 INFO scheduler.DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:24,302 INFO cluster.YarnScheduler: Killing all running tasks in stage 4: Stage finished\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:24,302 INFO scheduler.DAGScheduler: Job 3 finished: collect at AnalysisRunner.scala:326, took 0.550434 s\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:24,373 INFO codegen.CodeGenerator: Code generated in 53.283726 ms\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:24,655 INFO codegen.CodeGenerator: Code generated in 24.706638 ms\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:24,721 INFO spark.SparkContext: Starting job: treeReduce at KLLRunner.scala:107\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:24,722 INFO scheduler.DAGScheduler: Got job 4 (treeReduce at KLLRunner.scala:107) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:24,722 INFO scheduler.DAGScheduler: Final stage: ResultStage 5 (treeReduce at KLLRunner.scala:107)\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:24,723 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:24,723 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:24,725 INFO scheduler.DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[29] at treeReduce at KLLRunner.scala:107), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:24,755 INFO memory.MemoryStore: Block broadcast_6 stored as values in memory (estimated size 37.6 KiB, free 1457.3 MiB)\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:24,757 INFO memory.MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 16.0 KiB, free 1457.3 MiB)\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:24,758 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on 10.0.160.23:42449 (size: 16.0 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:24,759 INFO spark.SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:24,760 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[29] at treeReduce at KLLRunner.scala:107) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:24,760 INFO cluster.YarnScheduler: Adding task set 5.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:24,762 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 5.0 (TID 4) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4946 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:24,784 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on algo-1:41915 (size: 16.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:25,235 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 5.0 (TID 4) in 474 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:25,236 INFO scheduler.DAGScheduler: ResultStage 5 (treeReduce at KLLRunner.scala:107) finished in 0.510 s\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:25,236 INFO cluster.YarnScheduler: Removed TaskSet 5.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:25,237 INFO scheduler.DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:25,237 INFO cluster.YarnScheduler: Killing all running tasks in stage 5: Stage finished\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:25,237 INFO scheduler.DAGScheduler: Job 4 finished: treeReduce at KLLRunner.scala:107, took 0.516406 s\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:25,733 INFO codegen.CodeGenerator: Code generated in 105.141353 ms\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:25,740 INFO scheduler.DAGScheduler: Registering RDD 34 (collect at AnalysisRunner.scala:326) as input to shuffle 1\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:25,741 INFO scheduler.DAGScheduler: Got map stage job 5 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:25,741 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 6 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:25,741 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:25,742 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:25,743 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 6 (MapPartitionsRDD[34] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:25,749 INFO memory.MemoryStore: Block broadcast_7 stored as values in memory (estimated size 73.0 KiB, free 1457.2 MiB)\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:25,751 INFO memory.MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 23.2 KiB, free 1457.2 MiB)\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:25,751 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on 10.0.160.23:42449 (size: 23.2 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:25,752 INFO spark.SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:25,753 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 6 (MapPartitionsRDD[34] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:25,753 INFO cluster.YarnScheduler: Adding task set 6.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:25,755 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 6.0 (TID 5) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4935 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:25,775 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on algo-1:41915 (size: 23.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:25,892 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 6.0 (TID 5) in 138 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:25,892 INFO cluster.YarnScheduler: Removed TaskSet 6.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:25,893 INFO scheduler.DAGScheduler: ShuffleMapStage 6 (collect at AnalysisRunner.scala:326) finished in 0.150 s\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:25,893 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:25,893 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:25,893 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:25,893 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:26,049 INFO storage.BlockManagerInfo: Removed broadcast_7_piece0 on algo-1:41915 in memory (size: 23.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:26,062 INFO storage.BlockManagerInfo: Removed broadcast_7_piece0 on 10.0.160.23:42449 in memory (size: 23.2 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:26,077 INFO storage.BlockManagerInfo: Removed broadcast_6_piece0 on 10.0.160.23:42449 in memory (size: 16.0 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:26,084 INFO storage.BlockManagerInfo: Removed broadcast_6_piece0 on algo-1:41915 in memory (size: 16.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:26,103 INFO storage.BlockManagerInfo: Removed broadcast_4_piece0 on 10.0.160.23:42449 in memory (size: 34.6 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:26,104 INFO storage.BlockManagerInfo: Removed broadcast_4_piece0 on algo-1:41915 in memory (size: 34.6 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:26,131 INFO codegen.CodeGenerator: Code generated in 142.401773 ms\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:26,138 INFO storage.BlockManagerInfo: Removed broadcast_5_piece0 on 10.0.160.23:42449 in memory (size: 45.7 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:26,140 INFO storage.BlockManagerInfo: Removed broadcast_5_piece0 on algo-1:41915 in memory (size: 45.7 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:26,149 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:26,150 INFO scheduler.DAGScheduler: Got job 6 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:26,150 INFO scheduler.DAGScheduler: Final stage: ResultStage 8 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:26,150 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 7)\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:26,150 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:26,151 INFO scheduler.DAGScheduler: Submitting ResultStage 8 (MapPartitionsRDD[37] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:26,154 INFO memory.MemoryStore: Block broadcast_8 stored as values in memory (estimated size 66.2 KiB, free 1457.6 MiB)\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:26,157 INFO memory.MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 19.1 KiB, free 1457.6 MiB)\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:26,158 INFO storage.BlockManagerInfo: Added broadcast_8_piece0 in memory on 10.0.160.23:42449 (size: 19.1 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:26,159 INFO spark.SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:26,160 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 8 (MapPartitionsRDD[37] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:26,160 INFO cluster.YarnScheduler: Adding task set 8.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:26,161 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 8.0 (TID 6) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:26,172 INFO storage.BlockManagerInfo: Added broadcast_8_piece0 in memory on algo-1:41915 (size: 19.1 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:26,177 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to 10.0.160.23:50750\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:26,299 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 8.0 (TID 6) in 138 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:26,299 INFO cluster.YarnScheduler: Removed TaskSet 8.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:26,302 INFO scheduler.DAGScheduler: ResultStage 8 (collect at AnalysisRunner.scala:326) finished in 0.150 s\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:26,302 INFO scheduler.DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:26,302 INFO cluster.YarnScheduler: Killing all running tasks in stage 8: Stage finished\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:26,303 INFO scheduler.DAGScheduler: Job 6 finished: collect at AnalysisRunner.scala:326, took 0.153640 s\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:26,394 INFO codegen.CodeGenerator: Code generated in 70.328424 ms\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:26,502 INFO spark.SparkContext: Starting job: countByKey at ColumnProfiler.scala:592\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:26,505 INFO scheduler.DAGScheduler: Registering RDD 45 (countByKey at ColumnProfiler.scala:592) as input to shuffle 2\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:26,505 INFO scheduler.DAGScheduler: Got job 7 (countByKey at ColumnProfiler.scala:592) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:26,505 INFO scheduler.DAGScheduler: Final stage: ResultStage 10 (countByKey at ColumnProfiler.scala:592)\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:26,505 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 9)\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:26,505 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 9)\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:26,508 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 9 (MapPartitionsRDD[45] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:26,518 INFO memory.MemoryStore: Block broadcast_9 stored as values in memory (estimated size 29.9 KiB, free 1457.6 MiB)\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:26,520 INFO memory.MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 13.8 KiB, free 1457.6 MiB)\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:26,521 INFO storage.BlockManagerInfo: Added broadcast_9_piece0 in memory on 10.0.160.23:42449 (size: 13.8 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:26,521 INFO spark.SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:26,522 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 9 (MapPartitionsRDD[45] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:26,522 INFO cluster.YarnScheduler: Adding task set 9.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:26,523 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 9.0 (TID 7) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4935 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:26,539 INFO storage.BlockManagerInfo: Added broadcast_9_piece0 in memory on algo-1:41915 (size: 13.8 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:27,813 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 9.0 (TID 7) in 1290 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:27,813 INFO cluster.YarnScheduler: Removed TaskSet 9.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:27,816 INFO scheduler.DAGScheduler: ShuffleMapStage 9 (countByKey at ColumnProfiler.scala:592) finished in 1.307 s\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:27,816 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:27,816 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:27,816 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 10)\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:27,816 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:27,816 INFO scheduler.DAGScheduler: Submitting ResultStage 10 (ShuffledRDD[46] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:27,827 INFO memory.MemoryStore: Block broadcast_10 stored as values in memory (estimated size 5.1 KiB, free 1457.6 MiB)\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:27,830 INFO memory.MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 3.0 KiB, free 1457.6 MiB)\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:27,831 INFO storage.BlockManagerInfo: Added broadcast_10_piece0 in memory on 10.0.160.23:42449 (size: 3.0 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:27,832 INFO spark.SparkContext: Created broadcast 10 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:27,833 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 10 (ShuffledRDD[46] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:27,833 INFO cluster.YarnScheduler: Adding task set 10.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:27,835 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 10.0 (TID 8) (algo-1, executor 1, partition 0, NODE_LOCAL, 4282 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:27,853 INFO storage.BlockManagerInfo: Added broadcast_10_piece0 in memory on algo-1:41915 (size: 3.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:27,862 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 2 to 10.0.160.23:50750\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:27,924 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 10.0 (TID 8) in 89 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:27,924 INFO cluster.YarnScheduler: Removed TaskSet 10.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:27,930 INFO scheduler.DAGScheduler: ResultStage 10 (countByKey at ColumnProfiler.scala:592) finished in 0.113 s\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:27,930 INFO scheduler.DAGScheduler: Job 7 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:27,931 INFO cluster.YarnScheduler: Killing all running tasks in stage 10: Stage finished\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:27,931 INFO scheduler.DAGScheduler: Job 7 finished: countByKey at ColumnProfiler.scala:592, took 1.429439 s\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:28,187 INFO scheduler.DAGScheduler: Registering RDD 51 (collect at AnalysisRunner.scala:326) as input to shuffle 3\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:28,187 INFO scheduler.DAGScheduler: Got map stage job 8 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:28,187 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 11 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:28,187 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:28,188 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:28,192 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 11 (MapPartitionsRDD[51] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:28,206 INFO memory.MemoryStore: Block broadcast_11 stored as values in memory (estimated size 82.9 KiB, free 1457.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:28,208 INFO memory.MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 27.1 KiB, free 1457.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:28,209 INFO storage.BlockManagerInfo: Added broadcast_11_piece0 in memory on 10.0.160.23:42449 (size: 27.1 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:28,209 INFO spark.SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:28,210 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 11 (MapPartitionsRDD[51] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:28,210 INFO cluster.YarnScheduler: Adding task set 11.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:28,212 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 11.0 (TID 9) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4935 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:28,225 INFO storage.BlockManagerInfo: Added broadcast_11_piece0 in memory on algo-1:41915 (size: 27.1 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:28,442 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 11.0 (TID 9) in 229 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:28,442 INFO cluster.YarnScheduler: Removed TaskSet 11.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:28,443 INFO scheduler.DAGScheduler: ShuffleMapStage 11 (collect at AnalysisRunner.scala:326) finished in 0.249 s\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:28,443 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:28,443 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:28,443 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:28,443 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:28,536 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:28,538 INFO scheduler.DAGScheduler: Got job 9 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:28,539 INFO scheduler.DAGScheduler: Final stage: ResultStage 13 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:28,539 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 12)\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:28,539 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:28,540 INFO scheduler.DAGScheduler: Submitting ResultStage 13 (MapPartitionsRDD[54] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:28,554 INFO memory.MemoryStore: Block broadcast_12 stored as values in memory (estimated size 167.5 KiB, free 1457.3 MiB)\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:28,556 INFO memory.MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 45.9 KiB, free 1457.3 MiB)\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:28,556 INFO storage.BlockManagerInfo: Added broadcast_12_piece0 in memory on 10.0.160.23:42449 (size: 45.9 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:28,557 INFO spark.SparkContext: Created broadcast 12 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:28,557 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 13 (MapPartitionsRDD[54] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:28,557 INFO cluster.YarnScheduler: Adding task set 13.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:28,559 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 13.0 (TID 10) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:28,568 INFO storage.BlockManagerInfo: Added broadcast_12_piece0 in memory on algo-1:41915 (size: 45.9 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:28,578 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 3 to 10.0.160.23:50750\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:28,708 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 13.0 (TID 10) in 150 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:28,708 INFO cluster.YarnScheduler: Removed TaskSet 13.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:28,709 INFO scheduler.DAGScheduler: ResultStage 13 (collect at AnalysisRunner.scala:326) finished in 0.166 s\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:28,716 INFO scheduler.DAGScheduler: Job 9 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:28,716 INFO cluster.YarnScheduler: Killing all running tasks in stage 13: Stage finished\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:28,717 INFO scheduler.DAGScheduler: Job 9 finished: collect at AnalysisRunner.scala:326, took 0.179464 s\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:28,896 INFO spark.SparkContext: Starting job: treeReduce at KLLRunner.scala:107\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:28,897 INFO scheduler.DAGScheduler: Got job 10 (treeReduce at KLLRunner.scala:107) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:28,898 INFO scheduler.DAGScheduler: Final stage: ResultStage 14 (treeReduce at KLLRunner.scala:107)\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:28,898 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:28,898 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:28,899 INFO scheduler.DAGScheduler: Submitting ResultStage 14 (MapPartitionsRDD[64] at treeReduce at KLLRunner.scala:107), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:28,905 INFO memory.MemoryStore: Block broadcast_13 stored as values in memory (estimated size 37.5 KiB, free 1457.2 MiB)\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:28,907 INFO memory.MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 16.0 KiB, free 1457.2 MiB)\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:28,908 INFO storage.BlockManagerInfo: Added broadcast_13_piece0 in memory on 10.0.160.23:42449 (size: 16.0 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:28,908 INFO spark.SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:28,909 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 14 (MapPartitionsRDD[64] at treeReduce at KLLRunner.scala:107) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:28,909 INFO cluster.YarnScheduler: Adding task set 14.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:28,910 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 14.0 (TID 11) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4946 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:28,920 INFO storage.BlockManagerInfo: Added broadcast_13_piece0 in memory on algo-1:41915 (size: 16.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:28,980 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 14.0 (TID 11) in 70 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:28,981 INFO cluster.YarnScheduler: Removed TaskSet 14.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:28,981 INFO scheduler.DAGScheduler: ResultStage 14 (treeReduce at KLLRunner.scala:107) finished in 0.081 s\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:28,982 INFO scheduler.DAGScheduler: Job 10 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:28,982 INFO cluster.YarnScheduler: Killing all running tasks in stage 14: Stage finished\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:28,982 INFO scheduler.DAGScheduler: Job 10 finished: treeReduce at KLLRunner.scala:107, took 0.085687 s\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:29,155 INFO scheduler.DAGScheduler: Registering RDD 69 (collect at AnalysisRunner.scala:326) as input to shuffle 4\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:29,155 INFO scheduler.DAGScheduler: Got map stage job 11 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:29,156 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 15 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:29,156 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:29,157 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:29,157 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 15 (MapPartitionsRDD[69] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:29,161 INFO memory.MemoryStore: Block broadcast_14 stored as values in memory (estimated size 73.0 KiB, free 1457.1 MiB)\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:29,163 INFO memory.MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 23.2 KiB, free 1457.1 MiB)\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:29,163 INFO storage.BlockManagerInfo: Added broadcast_14_piece0 in memory on 10.0.160.23:42449 (size: 23.2 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:29,164 INFO spark.SparkContext: Created broadcast 14 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:29,165 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 15 (MapPartitionsRDD[69] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:29,165 INFO cluster.YarnScheduler: Adding task set 15.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:29,166 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 15.0 (TID 12) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4935 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:29,181 INFO storage.BlockManagerInfo: Added broadcast_14_piece0 in memory on algo-1:41915 (size: 23.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:29,216 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 15.0 (TID 12) in 50 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:29,216 INFO cluster.YarnScheduler: Removed TaskSet 15.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:29,218 INFO scheduler.DAGScheduler: ShuffleMapStage 15 (collect at AnalysisRunner.scala:326) finished in 0.060 s\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:29,218 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:29,218 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:29,218 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:29,218 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:29,282 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:29,283 INFO scheduler.DAGScheduler: Got job 12 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:29,283 INFO scheduler.DAGScheduler: Final stage: ResultStage 17 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:29,283 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 16)\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:29,283 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:29,284 INFO scheduler.DAGScheduler: Submitting ResultStage 17 (MapPartitionsRDD[72] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:29,286 INFO memory.MemoryStore: Block broadcast_15 stored as values in memory (estimated size 66.2 KiB, free 1457.0 MiB)\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:29,287 INFO memory.MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 19.2 KiB, free 1457.0 MiB)\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:29,288 INFO storage.BlockManagerInfo: Added broadcast_15_piece0 in memory on 10.0.160.23:42449 (size: 19.2 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:29,288 INFO spark.SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:29,288 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 17 (MapPartitionsRDD[72] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:29,288 INFO cluster.YarnScheduler: Adding task set 17.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:29,290 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 17.0 (TID 13) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:29,307 INFO storage.BlockManagerInfo: Added broadcast_15_piece0 in memory on algo-1:41915 (size: 19.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:29,312 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 4 to 10.0.160.23:50750\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:29,319 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 17.0 (TID 13) in 30 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:29,319 INFO cluster.YarnScheduler: Removed TaskSet 17.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:29,319 INFO scheduler.DAGScheduler: ResultStage 17 (collect at AnalysisRunner.scala:326) finished in 0.035 s\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:29,320 INFO scheduler.DAGScheduler: Job 12 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:29,320 INFO cluster.YarnScheduler: Killing all running tasks in stage 17: Stage finished\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:29,320 INFO scheduler.DAGScheduler: Job 12 finished: collect at AnalysisRunner.scala:326, took 0.038025 s\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:29,370 INFO spark.SparkContext: Starting job: countByKey at ColumnProfiler.scala:592\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:29,371 INFO scheduler.DAGScheduler: Registering RDD 80 (countByKey at ColumnProfiler.scala:592) as input to shuffle 5\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:29,372 INFO scheduler.DAGScheduler: Got job 13 (countByKey at ColumnProfiler.scala:592) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:29,372 INFO scheduler.DAGScheduler: Final stage: ResultStage 19 (countByKey at ColumnProfiler.scala:592)\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:29,372 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 18)\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:29,372 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 18)\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:29,373 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 18 (MapPartitionsRDD[80] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:29,377 INFO memory.MemoryStore: Block broadcast_16 stored as values in memory (estimated size 29.9 KiB, free 1457.0 MiB)\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:29,378 INFO memory.MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 13.8 KiB, free 1457.0 MiB)\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:29,379 INFO storage.BlockManagerInfo: Added broadcast_16_piece0 in memory on 10.0.160.23:42449 (size: 13.8 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:29,379 INFO spark.SparkContext: Created broadcast 16 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:29,379 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 18 (MapPartitionsRDD[80] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:29,380 INFO cluster.YarnScheduler: Adding task set 18.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:29,382 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 18.0 (TID 14) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4935 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:29,395 INFO storage.BlockManagerInfo: Added broadcast_16_piece0 in memory on algo-1:41915 (size: 13.8 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:29,436 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 18.0 (TID 14) in 55 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:29,436 INFO cluster.YarnScheduler: Removed TaskSet 18.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:29,438 INFO scheduler.DAGScheduler: ShuffleMapStage 18 (countByKey at ColumnProfiler.scala:592) finished in 0.064 s\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:29,438 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:29,438 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:29,439 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 19)\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:29,439 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:29,439 INFO scheduler.DAGScheduler: Submitting ResultStage 19 (ShuffledRDD[81] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:29,440 INFO memory.MemoryStore: Block broadcast_17 stored as values in memory (estimated size 5.1 KiB, free 1457.0 MiB)\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:29,441 INFO memory.MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 3.0 KiB, free 1457.0 MiB)\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:29,442 INFO storage.BlockManagerInfo: Added broadcast_17_piece0 in memory on 10.0.160.23:42449 (size: 3.0 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:29,442 INFO spark.SparkContext: Created broadcast 17 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:29,443 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 19 (ShuffledRDD[81] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:29,443 INFO cluster.YarnScheduler: Adding task set 19.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:29,444 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 19.0 (TID 15) (algo-1, executor 1, partition 0, NODE_LOCAL, 4282 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:29,458 INFO storage.BlockManagerInfo: Added broadcast_17_piece0 in memory on algo-1:41915 (size: 3.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:29,461 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 5 to 10.0.160.23:50750\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:29,471 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 19.0 (TID 15) in 27 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:29,471 INFO cluster.YarnScheduler: Removed TaskSet 19.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:29,472 INFO scheduler.DAGScheduler: ResultStage 19 (countByKey at ColumnProfiler.scala:592) finished in 0.031 s\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:29,472 INFO scheduler.DAGScheduler: Job 13 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:29,472 INFO cluster.YarnScheduler: Killing all running tasks in stage 19: Stage finished\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:29,472 INFO scheduler.DAGScheduler: Job 13 finished: countByKey at ColumnProfiler.scala:592, took 0.101789 s\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:29,523 INFO scheduler.DAGScheduler: Registering RDD 86 (collect at AnalysisRunner.scala:326) as input to shuffle 6\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:29,523 INFO scheduler.DAGScheduler: Got map stage job 14 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:29,523 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 20 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:29,523 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:29,524 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:29,524 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 20 (MapPartitionsRDD[86] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:29,527 INFO memory.MemoryStore: Block broadcast_18 stored as values in memory (estimated size 41.7 KiB, free 1456.9 MiB)\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:29,529 INFO memory.MemoryStore: Block broadcast_18_piece0 stored as bytes in memory (estimated size 17.6 KiB, free 1456.9 MiB)\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:29,530 INFO storage.BlockManagerInfo: Added broadcast_18_piece0 in memory on 10.0.160.23:42449 (size: 17.6 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:29,530 INFO spark.SparkContext: Created broadcast 18 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:29,531 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 20 (MapPartitionsRDD[86] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:29,531 INFO cluster.YarnScheduler: Adding task set 20.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:29,532 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 20.0 (TID 16) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4935 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:29,543 INFO storage.BlockManagerInfo: Added broadcast_18_piece0 in memory on algo-1:41915 (size: 17.6 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:29,701 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 20.0 (TID 16) in 169 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:29,701 INFO cluster.YarnScheduler: Removed TaskSet 20.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:29,702 INFO scheduler.DAGScheduler: ShuffleMapStage 20 (collect at AnalysisRunner.scala:326) finished in 0.177 s\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:29,703 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:29,703 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:29,703 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:29,703 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:29,726 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:29,729 INFO scheduler.DAGScheduler: Got job 15 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:29,729 INFO scheduler.DAGScheduler: Final stage: ResultStage 22 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:29,729 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 21)\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:29,730 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:29,730 INFO scheduler.DAGScheduler: Submitting ResultStage 22 (MapPartitionsRDD[89] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:29,733 INFO memory.MemoryStore: Block broadcast_19 stored as values in memory (estimated size 62.0 KiB, free 1456.9 MiB)\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:29,735 INFO memory.MemoryStore: Block broadcast_19_piece0 stored as bytes in memory (estimated size 22.9 KiB, free 1456.8 MiB)\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:29,736 INFO storage.BlockManagerInfo: Added broadcast_19_piece0 in memory on 10.0.160.23:42449 (size: 22.9 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:29,736 INFO spark.SparkContext: Created broadcast 19 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:29,737 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 22 (MapPartitionsRDD[89] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:29,737 INFO cluster.YarnScheduler: Adding task set 22.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:29,738 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 22.0 (TID 17) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:29,747 INFO storage.BlockManagerInfo: Added broadcast_19_piece0 in memory on algo-1:41915 (size: 22.9 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:29,753 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 6 to 10.0.160.23:50750\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:29,805 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 22.0 (TID 17) in 67 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:29,805 INFO cluster.YarnScheduler: Removed TaskSet 22.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:29,806 INFO scheduler.DAGScheduler: ResultStage 22 (collect at AnalysisRunner.scala:326) finished in 0.075 s\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:29,806 INFO scheduler.DAGScheduler: Job 15 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:29,806 INFO cluster.YarnScheduler: Killing all running tasks in stage 22: Stage finished\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:29,807 INFO scheduler.DAGScheduler: Job 15 finished: collect at AnalysisRunner.scala:326, took 0.079729 s\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:29,818 INFO codegen.CodeGenerator: Code generated in 8.506835 ms\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:29,857 INFO codegen.CodeGenerator: Code generated in 16.763556 ms\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:29,887 INFO spark.SparkContext: Starting job: treeReduce at KLLRunner.scala:107\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:29,888 INFO scheduler.DAGScheduler: Got job 16 (treeReduce at KLLRunner.scala:107) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:29,888 INFO scheduler.DAGScheduler: Final stage: ResultStage 23 (treeReduce at KLLRunner.scala:107)\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:29,888 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:29,889 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:29,890 INFO scheduler.DAGScheduler: Submitting ResultStage 23 (MapPartitionsRDD[99] at treeReduce at KLLRunner.scala:107), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:29,896 INFO memory.MemoryStore: Block broadcast_20 stored as values in memory (estimated size 33.1 KiB, free 1456.8 MiB)\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:29,901 INFO memory.MemoryStore: Block broadcast_20_piece0 stored as bytes in memory (estimated size 15.0 KiB, free 1456.8 MiB)\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:29,902 INFO storage.BlockManagerInfo: Added broadcast_20_piece0 in memory on 10.0.160.23:42449 (size: 15.0 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:29,903 INFO spark.SparkContext: Created broadcast 20 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:29,903 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 23 (MapPartitionsRDD[99] at treeReduce at KLLRunner.scala:107) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:29,903 INFO cluster.YarnScheduler: Adding task set 23.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:29,904 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 23.0 (TID 18) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4946 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:29,915 INFO storage.BlockManagerInfo: Added broadcast_20_piece0 in memory on algo-1:41915 (size: 15.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:29,953 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 23.0 (TID 18) in 49 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:29,953 INFO cluster.YarnScheduler: Removed TaskSet 23.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:29,953 INFO scheduler.DAGScheduler: ResultStage 23 (treeReduce at KLLRunner.scala:107) finished in 0.063 s\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:29,954 INFO scheduler.DAGScheduler: Job 16 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:29,954 INFO cluster.YarnScheduler: Killing all running tasks in stage 23: Stage finished\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:29,954 INFO scheduler.DAGScheduler: Job 16 finished: treeReduce at KLLRunner.scala:107, took 0.066497 s\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:30,081 INFO storage.BlockManagerInfo: Removed broadcast_12_piece0 on 10.0.160.23:42449 in memory (size: 45.9 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:30,094 INFO codegen.CodeGenerator: Code generated in 51.223812 ms\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:30,095 INFO storage.BlockManagerInfo: Removed broadcast_12_piece0 on algo-1:41915 in memory (size: 45.9 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:30,100 INFO scheduler.DAGScheduler: Registering RDD 104 (collect at AnalysisRunner.scala:326) as input to shuffle 7\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:30,101 INFO scheduler.DAGScheduler: Got map stage job 17 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:30,101 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 24 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:30,101 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:30,102 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:30,102 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 24 (MapPartitionsRDD[104] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:30,111 INFO memory.MemoryStore: Block broadcast_21 stored as values in memory (estimated size 31.9 KiB, free 1457.0 MiB)\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:30,113 INFO memory.MemoryStore: Block broadcast_21_piece0 stored as bytes in memory (estimated size 13.0 KiB, free 1457.0 MiB)\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:30,113 INFO storage.BlockManagerInfo: Added broadcast_21_piece0 in memory on 10.0.160.23:42449 (size: 13.0 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:30,114 INFO spark.SparkContext: Created broadcast 21 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:30,118 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 24 (MapPartitionsRDD[104] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:30,118 INFO cluster.YarnScheduler: Adding task set 24.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:30,120 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 24.0 (TID 19) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4935 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:30,128 INFO storage.BlockManagerInfo: Added broadcast_21_piece0 in memory on algo-1:41915 (size: 13.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:30,181 INFO storage.BlockManagerInfo: Removed broadcast_15_piece0 on algo-1:41915 in memory (size: 19.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:30,184 INFO storage.BlockManagerInfo: Removed broadcast_15_piece0 on 10.0.160.23:42449 in memory (size: 19.2 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:30,225 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 24.0 (TID 19) in 106 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:30,225 INFO cluster.YarnScheduler: Removed TaskSet 24.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:30,226 INFO scheduler.DAGScheduler: ShuffleMapStage 24 (collect at AnalysisRunner.scala:326) finished in 0.123 s\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:30,226 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:30,227 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:30,227 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:30,227 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:30,349 INFO storage.BlockManagerInfo: Removed broadcast_17_piece0 on 10.0.160.23:42449 in memory (size: 3.0 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:30,371 INFO storage.BlockManagerInfo: Removed broadcast_17_piece0 on algo-1:41915 in memory (size: 3.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:30,374 INFO codegen.CodeGenerator: Code generated in 103.354382 ms\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:30,385 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:30,395 INFO scheduler.DAGScheduler: Got job 18 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:30,395 INFO scheduler.DAGScheduler: Final stage: ResultStage 26 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:30,395 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 25)\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:30,395 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:30,395 INFO scheduler.DAGScheduler: Submitting ResultStage 26 (MapPartitionsRDD[107] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:30,397 INFO memory.MemoryStore: Block broadcast_22 stored as values in memory (estimated size 21.1 KiB, free 1457.0 MiB)\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:30,399 INFO memory.MemoryStore: Block broadcast_22_piece0 stored as bytes in memory (estimated size 8.3 KiB, free 1457.0 MiB)\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:30,399 INFO storage.BlockManagerInfo: Added broadcast_22_piece0 in memory on 10.0.160.23:42449 (size: 8.3 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:30,400 INFO spark.SparkContext: Created broadcast 22 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:30,400 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 26 (MapPartitionsRDD[107] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:30,400 INFO cluster.YarnScheduler: Adding task set 26.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:30,401 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 26.0 (TID 20) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:30,422 INFO storage.BlockManagerInfo: Added broadcast_22_piece0 in memory on algo-1:41915 (size: 8.3 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:30,427 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 7 to 10.0.160.23:50750\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:30,439 INFO storage.BlockManagerInfo: Removed broadcast_20_piece0 on algo-1:41915 in memory (size: 15.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:30,439 INFO storage.BlockManagerInfo: Removed broadcast_20_piece0 on 10.0.160.23:42449 in memory (size: 15.0 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:30,508 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 26.0 (TID 20) in 107 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:30,508 INFO cluster.YarnScheduler: Removed TaskSet 26.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:30,509 INFO scheduler.DAGScheduler: ResultStage 26 (collect at AnalysisRunner.scala:326) finished in 0.113 s\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:30,509 INFO scheduler.DAGScheduler: Job 18 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:30,509 INFO cluster.YarnScheduler: Killing all running tasks in stage 26: Stage finished\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:30,509 INFO scheduler.DAGScheduler: Job 18 finished: collect at AnalysisRunner.scala:326, took 0.124459 s\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:30,547 INFO storage.BlockManagerInfo: Removed broadcast_13_piece0 on 10.0.160.23:42449 in memory (size: 16.0 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:30,547 INFO codegen.CodeGenerator: Code generated in 32.035978 ms\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:30,556 INFO storage.BlockManagerInfo: Removed broadcast_13_piece0 on algo-1:41915 in memory (size: 16.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:30,615 INFO spark.SparkContext: Starting job: countByKey at ColumnProfiler.scala:592\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:30,616 INFO scheduler.DAGScheduler: Registering RDD 115 (countByKey at ColumnProfiler.scala:592) as input to shuffle 8\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:30,616 INFO scheduler.DAGScheduler: Got job 19 (countByKey at ColumnProfiler.scala:592) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:30,616 INFO scheduler.DAGScheduler: Final stage: ResultStage 28 (countByKey at ColumnProfiler.scala:592)\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:30,616 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 27)\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:30,616 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 27)\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:30,621 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 27 (MapPartitionsRDD[115] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:30,625 INFO storage.BlockManagerInfo: Removed broadcast_14_piece0 on algo-1:41915 in memory (size: 23.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:30,629 INFO memory.MemoryStore: Block broadcast_23 stored as values in memory (estimated size 29.5 KiB, free 1457.2 MiB)\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:30,631 INFO memory.MemoryStore: Block broadcast_23_piece0 stored as bytes in memory (estimated size 13.6 KiB, free 1457.1 MiB)\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:30,632 INFO storage.BlockManagerInfo: Added broadcast_23_piece0 in memory on 10.0.160.23:42449 (size: 13.6 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:30,633 INFO spark.SparkContext: Created broadcast 23 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:30,634 INFO storage.BlockManagerInfo: Removed broadcast_14_piece0 on 10.0.160.23:42449 in memory (size: 23.2 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:30,636 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 27 (MapPartitionsRDD[115] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:30,636 INFO cluster.YarnScheduler: Adding task set 27.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:30,638 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 27.0 (TID 21) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4935 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:30,669 INFO storage.BlockManagerInfo: Added broadcast_23_piece0 in memory on algo-1:41915 (size: 13.6 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:30,685 INFO storage.BlockManagerInfo: Removed broadcast_8_piece0 on 10.0.160.23:42449 in memory (size: 19.1 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:30,707 INFO storage.BlockManagerInfo: Removed broadcast_8_piece0 on algo-1:41915 in memory (size: 19.1 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:30,725 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 27.0 (TID 21) in 87 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:30,726 INFO cluster.YarnScheduler: Removed TaskSet 27.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:30,726 INFO scheduler.DAGScheduler: ShuffleMapStage 27 (countByKey at ColumnProfiler.scala:592) finished in 0.104 s\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:30,727 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:30,727 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:30,727 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 28)\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:30,727 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:30,727 INFO scheduler.DAGScheduler: Submitting ResultStage 28 (ShuffledRDD[116] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:30,728 INFO memory.MemoryStore: Block broadcast_24 stored as values in memory (estimated size 5.1 KiB, free 1457.2 MiB)\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:30,730 INFO memory.MemoryStore: Block broadcast_24_piece0 stored as bytes in memory (estimated size 3.0 KiB, free 1457.2 MiB)\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:30,733 INFO storage.BlockManagerInfo: Added broadcast_24_piece0 in memory on 10.0.160.23:42449 (size: 3.0 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:30,734 INFO spark.SparkContext: Created broadcast 24 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:30,734 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 28 (ShuffledRDD[116] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:30,734 INFO cluster.YarnScheduler: Adding task set 28.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:30,736 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 28.0 (TID 22) (algo-1, executor 1, partition 0, NODE_LOCAL, 4282 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:30,743 INFO storage.BlockManagerInfo: Removed broadcast_16_piece0 on 10.0.160.23:42449 in memory (size: 13.8 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:30,756 INFO storage.BlockManagerInfo: Removed broadcast_16_piece0 on algo-1:41915 in memory (size: 13.8 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:30,756 INFO storage.BlockManagerInfo: Added broadcast_24_piece0 in memory on algo-1:41915 (size: 3.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:30,760 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 8 to 10.0.160.23:50750\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:30,774 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 28.0 (TID 22) in 39 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:30,774 INFO cluster.YarnScheduler: Removed TaskSet 28.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:30,775 INFO scheduler.DAGScheduler: ResultStage 28 (countByKey at ColumnProfiler.scala:592) finished in 0.046 s\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:30,775 INFO scheduler.DAGScheduler: Job 19 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:30,776 INFO cluster.YarnScheduler: Killing all running tasks in stage 28: Stage finished\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:30,776 INFO scheduler.DAGScheduler: Job 19 finished: countByKey at ColumnProfiler.scala:592, took 0.161706 s\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:30,800 INFO storage.BlockManagerInfo: Removed broadcast_9_piece0 on algo-1:41915 in memory (size: 13.8 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:30,802 INFO storage.BlockManagerInfo: Removed broadcast_9_piece0 on 10.0.160.23:42449 in memory (size: 13.8 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:30,830 INFO storage.BlockManagerInfo: Removed broadcast_10_piece0 on algo-1:41915 in memory (size: 3.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:30,848 INFO storage.BlockManagerInfo: Removed broadcast_10_piece0 on 10.0.160.23:42449 in memory (size: 3.0 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:30,906 INFO storage.BlockManagerInfo: Removed broadcast_19_piece0 on 10.0.160.23:42449 in memory (size: 22.9 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:30,906 INFO storage.BlockManagerInfo: Removed broadcast_19_piece0 on algo-1:41915 in memory (size: 22.9 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:30,950 INFO storage.BlockManagerInfo: Removed broadcast_18_piece0 on algo-1:41915 in memory (size: 17.6 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:30,955 INFO storage.BlockManagerInfo: Removed broadcast_18_piece0 on 10.0.160.23:42449 in memory (size: 17.6 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:30,996 INFO storage.BlockManagerInfo: Removed broadcast_11_piece0 on 10.0.160.23:42449 in memory (size: 27.1 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:30,998 INFO storage.BlockManagerInfo: Removed broadcast_11_piece0 on algo-1:41915 in memory (size: 27.1 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:31,062 INFO FileUtil: Write to file constraints.json at path /opt/ml/processing/output.\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:31,104 INFO codegen.CodeGenerator: Code generated in 14.987442 ms\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:31,109 INFO scheduler.DAGScheduler: Registering RDD 121 (count at StatsGenerator.scala:66) as input to shuffle 9\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:31,109 INFO scheduler.DAGScheduler: Got map stage job 20 (count at StatsGenerator.scala:66) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:31,109 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 29 (count at StatsGenerator.scala:66)\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:31,109 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:31,110 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:31,110 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 29 (MapPartitionsRDD[121] at count at StatsGenerator.scala:66), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:31,113 INFO memory.MemoryStore: Block broadcast_25 stored as values in memory (estimated size 21.9 KiB, free 1457.6 MiB)\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:31,115 INFO memory.MemoryStore: Block broadcast_25_piece0 stored as bytes in memory (estimated size 10.2 KiB, free 1457.6 MiB)\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:31,116 INFO storage.BlockManagerInfo: Added broadcast_25_piece0 in memory on 10.0.160.23:42449 (size: 10.2 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:31,117 INFO spark.SparkContext: Created broadcast 25 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:31,117 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 29 (MapPartitionsRDD[121] at count at StatsGenerator.scala:66) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:31,117 INFO cluster.YarnScheduler: Adding task set 29.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:31,119 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 29.0 (TID 23) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4935 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:31,132 INFO storage.BlockManagerInfo: Added broadcast_25_piece0 in memory on algo-1:41915 (size: 10.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:31,181 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 29.0 (TID 23) in 62 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:31,181 INFO cluster.YarnScheduler: Removed TaskSet 29.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:31,182 INFO scheduler.DAGScheduler: ShuffleMapStage 29 (count at StatsGenerator.scala:66) finished in 0.071 s\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:31,182 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:31,182 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:31,182 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:31,182 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:31,216 INFO codegen.CodeGenerator: Code generated in 18.948932 ms\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:31,238 INFO spark.SparkContext: Starting job: count at StatsGenerator.scala:66\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:31,242 INFO scheduler.DAGScheduler: Got job 21 (count at StatsGenerator.scala:66) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:31,242 INFO scheduler.DAGScheduler: Final stage: ResultStage 31 (count at StatsGenerator.scala:66)\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:31,242 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 30)\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:31,242 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:31,242 INFO scheduler.DAGScheduler: Submitting ResultStage 31 (MapPartitionsRDD[124] at count at StatsGenerator.scala:66), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:31,245 INFO memory.MemoryStore: Block broadcast_26 stored as values in memory (estimated size 11.1 KiB, free 1457.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:31,246 INFO memory.MemoryStore: Block broadcast_26_piece0 stored as bytes in memory (estimated size 5.5 KiB, free 1457.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:31,247 INFO storage.BlockManagerInfo: Added broadcast_26_piece0 in memory on 10.0.160.23:42449 (size: 5.5 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:31,248 INFO spark.SparkContext: Created broadcast 26 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:31,248 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 31 (MapPartitionsRDD[124] at count at StatsGenerator.scala:66) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:31,248 INFO cluster.YarnScheduler: Adding task set 31.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:31,250 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 31.0 (TID 24) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:31,262 INFO storage.BlockManagerInfo: Added broadcast_26_piece0 in memory on algo-1:41915 (size: 5.5 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:31,266 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 9 to 10.0.160.23:50750\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:31,281 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 31.0 (TID 24) in 31 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:31,281 INFO cluster.YarnScheduler: Removed TaskSet 31.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:31,282 INFO scheduler.DAGScheduler: ResultStage 31 (count at StatsGenerator.scala:66) finished in 0.038 s\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:31,282 INFO scheduler.DAGScheduler: Job 21 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:31,282 INFO cluster.YarnScheduler: Killing all running tasks in stage 31: Stage finished\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:31,283 INFO scheduler.DAGScheduler: Job 21 finished: count at StatsGenerator.scala:66, took 0.044241 s\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:31,654 INFO FileUtil: Write to file statistics.json at path /opt/ml/processing/output.\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:31,674 INFO cluster.YarnClientSchedulerBackend: Interrupting monitor thread\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:31,753 INFO cluster.YarnClientSchedulerBackend: Shutting down all executors\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:31,754 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Asking each executor to shut down\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:31,758 INFO cluster.YarnClientSchedulerBackend: YARN client scheduler backend Stopped\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:31,796 INFO spark.MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:31,833 INFO memory.MemoryStore: MemoryStore cleared\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:31,836 INFO storage.BlockManager: BlockManager stopped\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:31,841 INFO storage.BlockManagerMaster: BlockManagerMaster stopped\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:31,849 INFO scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:31,928 INFO spark.SparkContext: Successfully stopped SparkContext\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:31,928 INFO Main: Completed: Job completed successfully with no violations.\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:31,928 INFO Main: Write to file /opt/ml/output/message.\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:31,931 INFO util.ShutdownHookManager: Shutdown hook called\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:31,942 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-3e630076-17d0-469c-97fa-19b205273d84\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:31,958 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-36e0c8e4-1b32-422a-b989-479ad9565ec4\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:32,046 - DefaultDataAnalyzer - INFO - Completed spark-submit with return code : 0\u001b[0m\n",
      "\u001b[34m2023-06-13 15:05:32,047 - DefaultDataAnalyzer - INFO - Spark job completed.\u001b[0m\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<sagemaker.processing.ProcessingJob at 0x7f2b8ea8a2d0>"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sagemaker.model_monitor import DefaultModelMonitor\n",
    "from sagemaker.model_monitor import DatasetFormat\n",
    "\n",
    "baseline_data_uri = \"s3://sagemaker-us-east-1-709891711940/MyAbalonePipeline/7p6mevksvmgc/MyAbaloneProcess/output/train/train.csv\"\n",
    "\n",
    "my_default_monitor = DefaultModelMonitor(\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    volume_size_in_gb=20,\n",
    "    max_runtime_in_seconds=3600\n",
    ")\n",
    "\n",
    "my_default_monitor.suggest_baseline(\n",
    "    baseline_dataset=baseline_data_uri,\n",
    "    dataset_format=DatasetFormat.csv(header=False),\n",
    "    output_s3_uri=f\"s3://{default_bucket}/my-abalone-baseline-results\",\n",
    "    wait=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ba44f0-f27f-42d1-ae61-64c4014957c8",
   "metadata": {},
   "source": [
    "# Monitoring Data Qality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "cafdab3f-7bf9-4d2a-ac76-7a61a17fd725",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.model_monitor.model_monitoring:Creating Monitoring Schedule with name: MyAbaloneModelMonitorSchedule\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.model_monitor import CronExpressionGenerator\n",
    "from sagemaker.model_monitor import EndpointInput\n",
    "\n",
    "my_default_monitor.create_monitoring_schedule(\n",
    "    monitor_schedule_name=\"MyAbaloneModelMonitorSchedule\",\n",
    "    output_s3_uri=f\"s3://{default_bucket}/my-abalone-model-monitoring-report\",\n",
    "    schedule_cron_expression=CronExpressionGenerator.hourly(),\n",
    "    statistics=my_default_monitor.baseline_statistics(),\n",
    "    constraints=my_default_monitor.suggested_constraints(),\n",
    "    enable_cloudwatch_metrics=True,\n",
    "    endpoint_input=EndpointInput(\n",
    "        endpoint_name=endpoint_name,\n",
    "        destination=\"/opt/ml/processing/input/endpoint\",\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "276de9ed-273d-4d8e-9aa6-e74547266411",
   "metadata": {},
   "outputs": [],
   "source": [
    "status_default_monitor_schedule = my_default_monitor.stop_monitoring_schedule()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "1f7126ad-b00c-4f7a-aa01-9b302eae23b2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'MonitoringScheduleArn': 'arn:aws:sagemaker:us-east-1:709891711940:monitoring-schedule/MyAbaloneModelMonitorSchedule',\n",
       " 'MonitoringScheduleName': 'MyAbaloneModelMonitorSchedule',\n",
       " 'MonitoringScheduleStatus': 'Scheduled',\n",
       " 'MonitoringType': 'DataQuality',\n",
       " 'CreationTime': datetime.datetime(2023, 6, 14, 6, 45, 9, 514000, tzinfo=tzlocal()),\n",
       " 'LastModifiedTime': datetime.datetime(2023, 6, 14, 10, 8, 12, 93000, tzinfo=tzlocal()),\n",
       " 'MonitoringScheduleConfig': {'ScheduleConfig': {'ScheduleExpression': 'cron(0 * ? * * *)'},\n",
       "  'MonitoringJobDefinitionName': 'data-quality-job-definition-2023-06-14-06-45-08-925',\n",
       "  'MonitoringType': 'DataQuality'},\n",
       " 'EndpointName': '1-2023-05-26-10-26-28-921',\n",
       " 'LastMonitoringExecutionSummary': {'MonitoringScheduleName': 'MyAbaloneModelMonitorSchedule',\n",
       "  'ScheduledTime': datetime.datetime(2023, 6, 14, 10, 0, tzinfo=tzlocal()),\n",
       "  'CreationTime': datetime.datetime(2023, 6, 14, 10, 1, 31, 259000, tzinfo=tzlocal()),\n",
       "  'LastModifiedTime': datetime.datetime(2023, 6, 14, 10, 8, 12, 80000, tzinfo=tzlocal()),\n",
       "  'MonitoringExecutionStatus': 'Completed',\n",
       "  'ProcessingJobArn': 'arn:aws:sagemaker:us-east-1:709891711940:processing-job/model-monitoring-202306141000-0ff03899a2373a034458dc66',\n",
       "  'EndpointName': '1-2023-05-26-10-26-28-921'},\n",
       " 'ResponseMetadata': {'RequestId': 'bd275387-7d6a-438b-be2b-a0c82ac0aef8',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': 'bd275387-7d6a-438b-be2b-a0c82ac0aef8',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '962',\n",
       "   'date': 'Wed, 14 Jun 2023 10:10:33 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import boto3\n",
    "boto3_sm_client = boto3.client('sagemaker')\n",
    "boto3_sm_client.describe_monitoring_schedule(MonitoringScheduleName='MyAbaloneModelMonitorSchedule')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b8868b-40dd-423e-ab61-035eb17011f7",
   "metadata": {},
   "source": [
    "# Create a Model Quality Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "ae8a66ee-8d50-462b-b968-2321a436fb13",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('arn:aws:iam::709891711940:role/service-role/SageMaker-MLOpsEngineer-poc',\n",
       " <sagemaker.session.Session at 0x7f2b96033d50>)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "role, sagemaker_session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "6fa30a0d-5c51-49a3-9fa2-897efca7786f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:Defaulting to the only supported framework/algorithm version: .\n",
      "INFO:sagemaker.image_uris:Ignoring unnecessary instance type: None.\n"
     ]
    }
   ],
   "source": [
    "from sagemaker import get_execution_role, session, Session\n",
    "from sagemaker.model_monitor import ModelQualityMonitor\n",
    "\n",
    "# role = get_execution_role()\n",
    "# session = Session()\n",
    "\n",
    "model_quality_monitor = ModelQualityMonitor(\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    volume_size_in_gb=20,\n",
    "    max_runtime_in_seconds=1800,\n",
    "    sagemaker_session=sagemaker_session\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "213e49c9-9923-4708-b2bc-ffb0a4030939",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating processing-job with name MyAbaloneModelQualityMonitorBaselineJob-06-15-2023-09-57-58\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..........................\u001b[34m2023-06-15 10:02:16,519 - matplotlib.font_manager - INFO - Generating new fontManager, this may take some time...\u001b[0m\n",
      "\u001b[34m2023-06-15 10:02:17.068724: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\u001b[0m\n",
      "\u001b[34m2023-06-15 10:02:17.068753: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\u001b[0m\n",
      "\u001b[34m2023-06-15 10:02:18.656970: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\u001b[0m\n",
      "\u001b[34m2023-06-15 10:02:18.657000: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\u001b[0m\n",
      "\u001b[34m2023-06-15 10:02:18.657020: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (ip-10-0-96-99.ec2.internal): /proc/driver/nvidia/version does not exist\u001b[0m\n",
      "\u001b[34m2023-06-15 10:02:18.657311: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\u001b[0m\n",
      "\u001b[34mTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\u001b[0m\n",
      "\u001b[34m2023-06-15 10:02:20,263 - __main__ - INFO - All params:{'ProcessingJobArn': 'arn:aws:sagemaker:us-east-1:709891711940:processing-job/myabalonemodelqualitymonitorbaselinejob-06-15-2023-09-57-58', 'ProcessingJobName': 'MyAbaloneModelQualityMonitorBaselineJob-06-15-2023-09-57-58', 'Environment': {'analysis_type': 'MODEL_QUALITY', 'dataset_format': '{\"csv\": {\"header\": true, \"output_columns_position\": \"START\"}}', 'dataset_source': '/opt/ml/processing/input/baseline_dataset_input', 'ground_truth_attribute': 'label', 'inference_attribute': 'prediction', 'output_path': '/opt/ml/processing/output', 'problem_type': 'Regression', 'publish_cloudwatch_metrics': 'Disabled'}, 'AppSpecification': {'ImageUri': '156813124566.dkr.ecr.us-east-1.amazonaws.com/sagemaker-model-monitor-analyzer', 'ContainerEntrypoint': None, 'ContainerArguments': None}, 'ProcessingInputs': [{'InputName': 'baseline_dataset_input', 'AppManaged': False, 'S3Input': {'LocalPath': '/opt/ml/processing/input/baseline_dataset_input', 'S3Uri': 's3://sagemaker-us-east-1-709891711940/MyAbalonePipeline/7p6mevksvmgc/MyAbaloneProcess/output/validation/validation_with_predictions.csv', 'S3DataDistributionType': 'FullyReplicated', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3CompressionType': 'None', 'S3DownloadMode': 'StartOfJob'}, 'DatasetDefinition': None}], 'ProcessingOutputConfig': {'Outputs': [{'OutputName': 'monitoring_output', 'AppManaged': False, 'S3Output': {'LocalPath': '/opt/ml/processing/output', 'S3Uri': 's3://sagemaker-us-east-1-709891711940/my-abalone-model-quality-monitoring-report', 'S3UploadMode': 'EndOfJob'}, 'FeatureStoreOutput': None}], 'KmsKeyId': None}, 'ProcessingResources': {'ClusterConfig': {'InstanceCount': 1, 'InstanceType': 'ml.m5.xlarge', 'VolumeSizeInGB': 20, 'VolumeKmsKeyId': None}}, 'RoleArn': 'arn:aws:iam::709891711940:role/service-role/SageMaker-MLOpsEngineer-poc', 'StoppingCondition': {'MaxRuntimeInSeconds': 1800}}\u001b[0m\n",
      "\u001b[34m2023-06-15 10:02:20,263 - __main__ - INFO - Current Environment:{'analysis_type': 'MODEL_QUALITY', 'dataset_format': '{\"csv\": {\"header\": true, \"output_columns_position\": \"START\"}}', 'dataset_source': '/opt/ml/processing/input/baseline_dataset_input', 'ground_truth_attribute': 'label', 'inference_attribute': 'prediction', 'output_path': '/opt/ml/processing/output', 'problem_type': 'Regression', 'publish_cloudwatch_metrics': 'Disabled'}\u001b[0m\n",
      "\u001b[34m2023-06-15 10:02:20,263 - __main__ - INFO - categorical_drift_method:None\u001b[0m\n",
      "\u001b[34m2023-06-15 10:02:20,264 - DefaultDataAnalyzer - INFO - Performing analysis with input: {\"dataset_source\": \"/opt/ml/processing/input/baseline_dataset_input\", \"dataset_format\": {\"csv\": {\"header\": true, \"output_columns_position\": \"START\"}}, \"output_path\": \"/opt/ml/processing/output\", \"monitoring_input_type\": null, \"analysis_type\": \"MODEL_QUALITY\", \"problem_type\": \"Regression\", \"inference_attribute\": \"prediction\", \"probability_attribute\": null, \"ground_truth_attribute\": \"label\", \"probability_threshold_attribute\": null, \"positive_label\": null, \"record_preprocessor_script\": null, \"post_analytics_processor_script\": null, \"baseline_constraints\": null, \"baseline_statistics\": null, \"data_quality_monitoring_config\": {\"evaluate_constraints\": \"Enabled\", \"emit_metrics\": \"Enabled\", \"datatype_check_threshold\": 1.0, \"domain_content_threshold\": 1.0, \"distribution_constraints\": {\"perform_comparison\": \"Enabled\", \"comparison_threshold\": 0.1, \"comparison_method\": \"Robust\", \"categorical_comparison_threshold\": 0.1, \"categorical_drift_method\": \"LInfinity\"}}, \"start_time\": null, \"end_time\": null, \"metric_time\": null, \"cloudwatch_metrics_directory\": \"/opt/ml/output/metrics/cloudwatch\", \"publish_cloudwatch_metrics\": \"Disabled\", \"sagemaker_endpoint_name\": null, \"sagemaker_monitoring_schedule_name\": null, \"output_message_file\": \"/opt/ml/output/message\", \"detect_outliers\": null, \"detect_drift\": null, \"image_data\": null, \"report_enabled\": false, \"auto_ml_job_detail\": null}\u001b[0m\n",
      "\u001b[34m2023-06-15 10:02:20,264 - DefaultDataAnalyzer - INFO - Bootstrapping yarn\u001b[0m\n",
      "\u001b[34m2023-06-15 10:02:20,264 - bootstrap - INFO - Copy aws jars\u001b[0m\n",
      "\u001b[34m2023-06-15 10:02:20,321 - bootstrap - INFO - Copy cluster config\u001b[0m\n",
      "\u001b[34m2023-06-15 10:02:20,322 - bootstrap - INFO - Write runtime cluster config\u001b[0m\n",
      "\u001b[34m2023-06-15 10:02:20,322 - bootstrap - INFO - Resource Config is: {'current_host': 'algo-1', 'hosts': ['algo-1']}\u001b[0m\n",
      "\u001b[34m2023-06-15 10:02:20,334 - bootstrap - INFO - Finished Yarn configuration files setup.\u001b[0m\n",
      "\u001b[34m2023-06-15 10:02:20,334 - bootstrap - INFO - Starting spark process for master node algo-1\u001b[0m\n",
      "\u001b[34m2023-06-15 10:02:20,334 - bootstrap - INFO - Running command: /usr/hadoop-3.0.0/bin/hdfs namenode -format -force\u001b[0m\n",
      "\u001b[34mWARNING: /usr/hadoop-3.0.0/logs does not exist. Creating.\u001b[0m\n",
      "\u001b[34m2023-06-15 10:02:20,796 INFO namenode.NameNode: STARTUP_MSG: \u001b[0m\n",
      "\u001b[34m/************************************************************\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG: Starting NameNode\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   host = algo-1/10.0.96.99\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   args = [-format, -force]\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   version = 3.0.0\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   classpath = /usr/hadoop-3.0.0/etc/hadoop:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-core-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-crypto-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/avro-1.7.7.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/nimbus-jose-jwt-4.41.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/re2j-1.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/junit-4.11.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-security-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-lang3-3.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/asm-5.0.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-server-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/htrace-core4-4.1.0-incubating.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerby-xdr-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/curator-recipes-2.12.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-common-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerby-config-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-server-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/javax.servlet-api-3.1.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/xz-1.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jersey-core-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-util-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/netty-3.10.5.Final.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jsr305-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-admin-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-beanutils-1.9.3.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jersey-json-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/zookeeper-3.4.9.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-configuration2-2.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerby-asn1-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-simplekdc-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-http-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-webapp-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/woodstox-core-5.0.3.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/accessors-smart-1.2.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jsr311-api-1.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-xml-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/json-smart-2.3.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/hadoop-auth-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/paranamer-2.3.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-databind-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jul-to-slf4j-1.7.25.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/slf4j-api-1.7.25.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-core-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jettison-1.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/snappy-java-1.0.5.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/token-provider-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-servlet-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-client-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/curator-client-2.12.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-identity-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/hadoop-annotations-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jcip-annotations-1.0-1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerby-util-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/gson-2.2.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/guava-11.0.2.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/metrics-core-3.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/curator-framework-2.12.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-io-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/httpclient-4.5.2.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-util-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-io-2.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jaxb-api-2.2.11.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-net-3.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jsch-0.1.54.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jersey-server-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jersey-servlet-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-annotations-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-collections-3.2.2.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/stax2-api-3.1.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/httpcore-4.4.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerby-pkix-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/aws-java-sdk-bundle-1.11.199.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/hadoop-aws-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/hadoop-common-3.0.0-tests.jar:/usr/hadoop-3.0.0/share/hadoop/common/hadoop-kms-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/hadoop-nfs-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/hadoop-common-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-core-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-crypto-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/avro-1.7.7.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/nimbus-jose-jwt-4.41.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/re2j-1.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-security-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-lang3-3.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/asm-5.0.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-math3-3.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-server-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/htrace-core4-4.1.0-incubating.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerby-xdr-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/curator-recipes-2.12.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-common-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerby-config-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-server-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/javax.servlet-api-3.1.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/xz-1.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/json-simple-1.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jersey-core-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-util-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/netty-3.10.5.Final.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-admin-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-beanutils-1.9.3.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jersey-json-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-util-ajax-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/zookeeper-3.4.9.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jaxb-impl-2.2.3-1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-configuration2-2.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerby-asn1-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-simplekdc-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-jaxrs-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-http-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-webapp-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/woodstox-core-5.0.3.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/accessors-smart-1.2.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jsr311-api-1.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-xml-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/json-smart-2.3.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/hadoop-auth-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/paranamer-2.3.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-databind-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-core-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jettison-1.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/snappy-java-1.0.5.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/token-provider-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-compress-1.4.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-servlet-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-client-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/curator-client-2.12.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-identity-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/hadoop-annotations-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jcip-annotations-1.0-1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerby-util-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/gson-2.2.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/okio-1.4.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/curator-framework-2.12.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-io-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/httpclient-4.5.2.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-util-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jaxb-api-2.2.11.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-net-3.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jsch-0.1.54.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jersey-server-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jersey-servlet-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-annotations-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-collections-3.2.2.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/stax2-api-3.1.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/httpcore-4.4.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-xc-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/okhttp-2.4.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerby-pkix-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-client-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-client-3.0.0-tests.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-nfs-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-httpfs-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-native-client-3.0.0-tests.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-3.0.0-tests.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-native-client-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.0.0-tests.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-common-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-nativetask-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-app-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-core-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/java-util-1.9.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jamon-runtime-2.4.1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-client-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jasper-runtime-5.5.23.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/ehcache-3.3.1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jackson-jaxrs-base-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jcodings-1.0.8.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/json-io-2.5.1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/fst-2.50.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-hadoop2-compat-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jasper-compiler-5.5.23.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-procedure-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jsp-2.1-6.1.14.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/commons-el-1.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/commons-httpclient-3.1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jsp-api-2.1-6.1.14.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-hadoop-compat-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-annotations-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/findbugs-annotations-1.3.9-1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/disruptor-3.3.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-common-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jackson-module-jaxb-annotations-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/joni-2.1.2.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-prefix-tree-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jackson-jaxrs-json-provider-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jersey-client-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/HikariCP-java7-2.4.12.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/commons-csv-1.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/htrace-core-3.1.0-incubating.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-server-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/commons-math-2.2.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-protocol-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/metrics-core-3.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/guice-servlet-4.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/metrics-core-2.2.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jersey-guice-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/guice-4.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/servlet-api-2.5-6.1.14.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-timelineservice-hbase-tests-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-timelineservice-hbase-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-timelineservice-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-common-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-common-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-3.0.0.jar:/usr/ha\u001b[0m\n",
      "\u001b[34mdoop-3.0.0/share/hadoop/yarn/hadoop-yarn-client-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-web-proxy-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-nodemanager-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-registry-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-api-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-tests-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-router-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-3.0.0.jar\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r c25427ceca461ee979d30edd7a4b0f50718e6533; compiled by 'andrew' on 2017-12-08T19:16Z\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   java = 1.8.0_362\u001b[0m\n",
      "\u001b[34m************************************************************/\u001b[0m\n",
      "\u001b[34m2023-06-15 10:02:20,806 INFO namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]\u001b[0m\n",
      "\u001b[34m2023-06-15 10:02:20,809 INFO namenode.NameNode: createNameNode [-format, -force]\u001b[0m\n",
      "\u001b[34mFormatting using clusterid: CID-bd21bb50-d96b-48ae-a7aa-901a1a087196\u001b[0m\n",
      "\u001b[34m2023-06-15 10:02:21,322 INFO namenode.FSEditLog: Edit logging is async:true\u001b[0m\n",
      "\u001b[34m2023-06-15 10:02:21,334 INFO namenode.FSNamesystem: KeyProvider: null\u001b[0m\n",
      "\u001b[34m2023-06-15 10:02:21,335 INFO namenode.FSNamesystem: fsLock is fair: true\u001b[0m\n",
      "\u001b[34m2023-06-15 10:02:21,337 INFO namenode.FSNamesystem: Detailed lock hold time metrics enabled: false\u001b[0m\n",
      "\u001b[34m2023-06-15 10:02:21,342 INFO namenode.FSNamesystem: fsOwner             = root (auth:SIMPLE)\u001b[0m\n",
      "\u001b[34m2023-06-15 10:02:21,342 INFO namenode.FSNamesystem: supergroup          = supergroup\u001b[0m\n",
      "\u001b[34m2023-06-15 10:02:21,342 INFO namenode.FSNamesystem: isPermissionEnabled = true\u001b[0m\n",
      "\u001b[34m2023-06-15 10:02:21,342 INFO namenode.FSNamesystem: HA Enabled: false\u001b[0m\n",
      "\u001b[34m2023-06-15 10:02:21,373 INFO common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling\u001b[0m\n",
      "\u001b[34m2023-06-15 10:02:21,385 INFO blockmanagement.DatanodeManager: dfs.block.invalidate.limit: configured=1000, counted=60, effected=1000\u001b[0m\n",
      "\u001b[34m2023-06-15 10:02:21,385 INFO blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true\u001b[0m\n",
      "\u001b[34m2023-06-15 10:02:21,389 INFO blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000\u001b[0m\n",
      "\u001b[34m2023-06-15 10:02:21,392 INFO blockmanagement.BlockManager: The block deletion will start around 2023 Jun 15 10:02:21\u001b[0m\n",
      "\u001b[34m2023-06-15 10:02:21,393 INFO util.GSet: Computing capacity for map BlocksMap\u001b[0m\n",
      "\u001b[34m2023-06-15 10:02:21,393 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m2023-06-15 10:02:21,395 INFO util.GSet: 2.0% max memory 3.1 GB = 63.8 MB\u001b[0m\n",
      "\u001b[34m2023-06-15 10:02:21,395 INFO util.GSet: capacity      = 2^23 = 8388608 entries\u001b[0m\n",
      "\u001b[34m2023-06-15 10:02:21,430 INFO blockmanagement.BlockManager: dfs.block.access.token.enable = false\u001b[0m\n",
      "\u001b[34m2023-06-15 10:02:21,433 INFO Configuration.deprecation: No unit for dfs.namenode.safemode.extension(30000) assuming MILLISECONDS\u001b[0m\n",
      "\u001b[34m2023-06-15 10:02:21,433 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.threshold-pct = 0.9990000128746033\u001b[0m\n",
      "\u001b[34m2023-06-15 10:02:21,434 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.min.datanodes = 0\u001b[0m\n",
      "\u001b[34m2023-06-15 10:02:21,434 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.extension = 30000\u001b[0m\n",
      "\u001b[34m2023-06-15 10:02:21,434 INFO blockmanagement.BlockManager: defaultReplication         = 3\u001b[0m\n",
      "\u001b[34m2023-06-15 10:02:21,434 INFO blockmanagement.BlockManager: maxReplication             = 512\u001b[0m\n",
      "\u001b[34m2023-06-15 10:02:21,434 INFO blockmanagement.BlockManager: minReplication             = 1\u001b[0m\n",
      "\u001b[34m2023-06-15 10:02:21,434 INFO blockmanagement.BlockManager: maxReplicationStreams      = 2\u001b[0m\n",
      "\u001b[34m2023-06-15 10:02:21,434 INFO blockmanagement.BlockManager: redundancyRecheckInterval  = 3000ms\u001b[0m\n",
      "\u001b[34m2023-06-15 10:02:21,434 INFO blockmanagement.BlockManager: encryptDataTransfer        = false\u001b[0m\n",
      "\u001b[34m2023-06-15 10:02:21,434 INFO blockmanagement.BlockManager: maxNumBlocksToLog          = 1000\u001b[0m\n",
      "\u001b[34m2023-06-15 10:02:21,460 INFO util.GSet: Computing capacity for map INodeMap\u001b[0m\n",
      "\u001b[34m2023-06-15 10:02:21,461 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m2023-06-15 10:02:21,461 INFO util.GSet: 1.0% max memory 3.1 GB = 31.9 MB\u001b[0m\n",
      "\u001b[34m2023-06-15 10:02:21,461 INFO util.GSet: capacity      = 2^22 = 4194304 entries\u001b[0m\n",
      "\u001b[34m2023-06-15 10:02:21,463 INFO namenode.FSDirectory: ACLs enabled? false\u001b[0m\n",
      "\u001b[34m2023-06-15 10:02:21,463 INFO namenode.FSDirectory: POSIX ACL inheritance enabled? true\u001b[0m\n",
      "\u001b[34m2023-06-15 10:02:21,463 INFO namenode.FSDirectory: XAttrs enabled? true\u001b[0m\n",
      "\u001b[34m2023-06-15 10:02:21,463 INFO namenode.NameNode: Caching file names occurring more than 10 times\u001b[0m\n",
      "\u001b[34m2023-06-15 10:02:21,467 INFO snapshot.SnapshotManager: Loaded config captureOpenFiles: false, skipCaptureAccessTimeOnlyChange: false, snapshotDiffAllowSnapRootDescendant: true\u001b[0m\n",
      "\u001b[34m2023-06-15 10:02:21,471 INFO util.GSet: Computing capacity for map cachedBlocks\u001b[0m\n",
      "\u001b[34m2023-06-15 10:02:21,471 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m2023-06-15 10:02:21,471 INFO util.GSet: 0.25% max memory 3.1 GB = 8.0 MB\u001b[0m\n",
      "\u001b[34m2023-06-15 10:02:21,471 INFO util.GSet: capacity      = 2^20 = 1048576 entries\u001b[0m\n",
      "\u001b[34m2023-06-15 10:02:21,510 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10\u001b[0m\n",
      "\u001b[34m2023-06-15 10:02:21,510 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10\u001b[0m\n",
      "\u001b[34m2023-06-15 10:02:21,510 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25\u001b[0m\n",
      "\u001b[34m2023-06-15 10:02:21,513 INFO namenode.FSNamesystem: Retry cache on namenode is enabled\u001b[0m\n",
      "\u001b[34m2023-06-15 10:02:21,513 INFO namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis\u001b[0m\n",
      "\u001b[34m2023-06-15 10:02:21,514 INFO util.GSet: Computing capacity for map NameNodeRetryCache\u001b[0m\n",
      "\u001b[34m2023-06-15 10:02:21,514 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m2023-06-15 10:02:21,514 INFO util.GSet: 0.029999999329447746% max memory 3.1 GB = 979.8 KB\u001b[0m\n",
      "\u001b[34m2023-06-15 10:02:21,515 INFO util.GSet: capacity      = 2^17 = 131072 entries\u001b[0m\n",
      "\u001b[34m2023-06-15 10:02:21,537 INFO namenode.FSImage: Allocated new BlockPoolId: BP-1230793509-10.0.96.99-1686823341528\u001b[0m\n",
      "\u001b[34m2023-06-15 10:02:21,549 INFO common.Storage: Storage directory /opt/amazon/hadoop/hdfs/namenode has been successfully formatted.\u001b[0m\n",
      "\u001b[34m2023-06-15 10:02:21,556 INFO namenode.FSImageFormatProtobuf: Saving image file /opt/amazon/hadoop/hdfs/namenode/current/fsimage.ckpt_0000000000000000000 using no compression\u001b[0m\n",
      "\u001b[34m2023-06-15 10:02:21,632 INFO namenode.FSImageFormatProtobuf: Image file /opt/amazon/hadoop/hdfs/namenode/current/fsimage.ckpt_0000000000000000000 of size 389 bytes saved in 0 seconds.\u001b[0m\n",
      "\u001b[34m2023-06-15 10:02:21,645 INFO namenode.NNStorageRetentionManager: Going to retain 1 images with txid >= 0\u001b[0m\n",
      "\u001b[34m2023-06-15 10:02:21,648 INFO namenode.NameNode: SHUTDOWN_MSG: \u001b[0m\n",
      "\u001b[34m/************************************************************\u001b[0m\n",
      "\u001b[34mSHUTDOWN_MSG: Shutting down NameNode at algo-1/10.0.96.99\u001b[0m\n",
      "\u001b[34m************************************************************/\u001b[0m\n",
      "\u001b[34m2023-06-15 10:02:21,659 - bootstrap - INFO - Running command: /usr/hadoop-3.0.0/bin/hdfs --daemon start namenode\u001b[0m\n",
      "\u001b[34m2023-06-15 10:02:23,720 - bootstrap - INFO - Failed to run /usr/hadoop-3.0.0/bin/hdfs --daemon start namenode, return code 1\u001b[0m\n",
      "\u001b[34m2023-06-15 10:02:23,720 - bootstrap - INFO - Running command: /usr/hadoop-3.0.0/bin/hdfs --daemon start datanode\u001b[0m\n",
      "\u001b[34m2023-06-15 10:02:25,783 - bootstrap - INFO - Failed to run /usr/hadoop-3.0.0/bin/hdfs --daemon start datanode, return code 1\u001b[0m\n",
      "\u001b[34m2023-06-15 10:02:25,783 - bootstrap - INFO - Running command: /usr/hadoop-3.0.0/bin/yarn --daemon start resourcemanager\u001b[0m\n",
      "\u001b[34mWARNING: YARN_LOG_DIR has been replaced by HADOOP_LOG_DIR. Using value of YARN_LOG_DIR.\u001b[0m\n",
      "\u001b[34mWARNING: /var/log/yarn/ does not exist. Creating.\u001b[0m\n",
      "\u001b[34m2023-06-15 10:02:27,868 - bootstrap - INFO - Failed to run /usr/hadoop-3.0.0/bin/yarn --daemon start resourcemanager, return code 1\u001b[0m\n",
      "\u001b[34m2023-06-15 10:02:27,868 - bootstrap - INFO - Running command: /usr/hadoop-3.0.0/bin/yarn --daemon start nodemanager\u001b[0m\n",
      "\u001b[34mWARNING: YARN_LOG_DIR has been replaced by HADOOP_LOG_DIR. Using value of YARN_LOG_DIR.\u001b[0m\n",
      "\u001b[34m2023-06-15 10:02:29,970 - bootstrap - INFO - Failed to run /usr/hadoop-3.0.0/bin/yarn --daemon start nodemanager, return code 1\u001b[0m\n",
      "\u001b[34m2023-06-15 10:02:29,971 - bootstrap - INFO - Running command: /usr/hadoop-3.0.0/bin/yarn --daemon start proxyserver\u001b[0m\n",
      "\u001b[34mWARNING: YARN_LOG_DIR has been replaced by HADOOP_LOG_DIR. Using value of YARN_LOG_DIR.\u001b[0m\n",
      "\u001b[34m2023-06-15 10:02:32,416 - bootstrap - INFO - Failed to run /usr/hadoop-3.0.0/bin/yarn --daemon start proxyserver, return code 1\u001b[0m\n",
      "\u001b[34m2023-06-15 10:02:32,416 - DefaultDataAnalyzer - INFO - Total number of hosts in the cluster: 1\u001b[0m\n",
      "\u001b[34m2023-06-15 10:02:42,427 - DefaultDataAnalyzer - INFO - Running command: bin/spark-submit --master yarn --deploy-mode client --conf spark.hadoop.fs.s3a.aws.credentials.provider=org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider --conf spark.serializer=org.apache.spark.serializer.KryoSerializer /opt/amazon/sagemaker-data-analyzer-1.0-jar-with-dependencies.jar --analytics_input /tmp/spark_job_config.json\u001b[0m\n",
      "\u001b[34m2023-06-15 10:02:44,138 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\u001b[0m\n",
      "\u001b[34m2023-06-15 10:02:44,602 INFO Main: Start analyzing with args: --analytics_input /tmp/spark_job_config.json\u001b[0m\n",
      "\u001b[34m2023-06-15 10:02:44,643 INFO Main: Analytics input path: DataAnalyzerParams(/tmp/spark_job_config.json,yarn)\u001b[0m\n",
      "\u001b[34m2023-06-15 10:02:44,653 INFO FileUtil: Read file from path /tmp/spark_job_config.json.\u001b[0m\n",
      "\u001b[34m2023-06-15 10:02:45,191 INFO spark.SparkContext: Running Spark version 3.3.0\u001b[0m\n",
      "\u001b[34m2023-06-15 10:02:45,217 INFO resource.ResourceUtils: ==============================================================\u001b[0m\n",
      "\u001b[34m2023-06-15 10:02:45,217 INFO resource.ResourceUtils: No custom resources configured for spark.driver.\u001b[0m\n",
      "\u001b[34m2023-06-15 10:02:45,218 INFO resource.ResourceUtils: ==============================================================\u001b[0m\n",
      "\u001b[34m2023-06-15 10:02:45,218 INFO spark.SparkContext: Submitted application: SageMakerDataAnalyzer\u001b[0m\n",
      "\u001b[34m2023-06-15 10:02:45,247 INFO resource.ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 3, script: , vendor: , memory -> name: memory, amount: 11536, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\u001b[0m\n",
      "\u001b[34m2023-06-15 10:02:45,262 INFO resource.ResourceProfile: Limiting resource is cpus at 3 tasks per executor\u001b[0m\n",
      "\u001b[34m2023-06-15 10:02:45,264 INFO resource.ResourceProfileManager: Added ResourceProfile id: 0\u001b[0m\n",
      "\u001b[34m2023-06-15 10:02:45,311 INFO spark.SecurityManager: Changing view acls to: root\u001b[0m\n",
      "\u001b[34m2023-06-15 10:02:45,312 INFO spark.SecurityManager: Changing modify acls to: root\u001b[0m\n",
      "\u001b[34m2023-06-15 10:02:45,312 INFO spark.SecurityManager: Changing view acls groups to: \u001b[0m\n",
      "\u001b[34m2023-06-15 10:02:45,312 INFO spark.SecurityManager: Changing modify acls groups to: \u001b[0m\n",
      "\u001b[34m2023-06-15 10:02:45,313 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\u001b[0m\n",
      "\u001b[34m2023-06-15 10:02:45,652 INFO util.Utils: Successfully started service 'sparkDriver' on port 36877.\u001b[0m\n",
      "\u001b[34m2023-06-15 10:02:45,685 INFO spark.SparkEnv: Registering MapOutputTracker\u001b[0m\n",
      "\u001b[34m2023-06-15 10:02:45,721 INFO spark.SparkEnv: Registering BlockManagerMaster\u001b[0m\n",
      "\u001b[34m2023-06-15 10:02:45,742 INFO storage.BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\u001b[0m\n",
      "\u001b[34m2023-06-15 10:02:45,743 INFO storage.BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\u001b[0m\n",
      "\u001b[34m2023-06-15 10:02:45,776 INFO spark.SparkEnv: Registering BlockManagerMasterHeartbeat\u001b[0m\n",
      "\u001b[34m2023-06-15 10:02:45,800 INFO storage.DiskBlockManager: Created local directory at /tmp/blockmgr-cfa2f2fa-4860-4473-aec6-4c2a5492c57b\u001b[0m\n",
      "\u001b[34m2023-06-15 10:02:45,817 INFO memory.MemoryStore: MemoryStore started with capacity 1458.6 MiB\u001b[0m\n",
      "\u001b[34m2023-06-15 10:02:45,856 INFO spark.SparkEnv: Registering OutputCommitCoordinator\u001b[0m\n",
      "\u001b[34m2023-06-15 10:02:45,890 INFO spark.SparkContext: Added JAR file:/opt/amazon/sagemaker-data-analyzer-1.0-jar-with-dependencies.jar at spark://10.0.96.99:36877/jars/sagemaker-data-analyzer-1.0-jar-with-dependencies.jar with timestamp 1686823365187\u001b[0m\n",
      "\u001b[34m2023-06-15 10:02:46,437 INFO client.RMProxy: Connecting to ResourceManager at /10.0.96.99:8032\u001b[0m\n",
      "\u001b[34m2023-06-15 10:02:47,147 INFO conf.Configuration: resource-types.xml not found\u001b[0m\n",
      "\u001b[34m2023-06-15 10:02:47,147 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\u001b[0m\n",
      "\u001b[34m2023-06-15 10:02:47,154 INFO yarn.Client: Verifying our application has not requested more than the maximum memory capability of the cluster (15731 MB per container)\u001b[0m\n",
      "\u001b[34m2023-06-15 10:02:47,155 INFO yarn.Client: Will allocate AM container, with 896 MB memory including 384 MB overhead\u001b[0m\n",
      "\u001b[34m2023-06-15 10:02:47,155 INFO yarn.Client: Setting up container launch context for our AM\u001b[0m\n",
      "\u001b[34m2023-06-15 10:02:47,155 INFO yarn.Client: Setting up the launch environment for our AM container\u001b[0m\n",
      "\u001b[34m2023-06-15 10:02:47,162 INFO yarn.Client: Preparing resources for our AM container\u001b[0m\n",
      "\u001b[34m2023-06-15 10:02:47,253 WARN yarn.Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\u001b[0m\n",
      "\u001b[34m2023-06-15 10:02:49,112 INFO yarn.Client: Uploading resource file:/tmp/spark-51af56ea-8521-4344-a944-c0f03684b6a3/__spark_libs__5078001686772058798.zip -> hdfs://10.0.96.99/user/root/.sparkStaging/application_1686823347572_0001/__spark_libs__5078001686772058798.zip\u001b[0m\n",
      "\u001b[34m2023-06-15 10:02:51,647 INFO yarn.Client: Uploading resource file:/tmp/spark-51af56ea-8521-4344-a944-c0f03684b6a3/__spark_conf__156492575588868110.zip -> hdfs://10.0.96.99/user/root/.sparkStaging/application_1686823347572_0001/__spark_conf__.zip\u001b[0m\n",
      "\u001b[34m2023-06-15 10:02:52,101 INFO spark.SecurityManager: Changing view acls to: root\u001b[0m\n",
      "\u001b[34m2023-06-15 10:02:52,101 INFO spark.SecurityManager: Changing modify acls to: root\u001b[0m\n",
      "\u001b[34m2023-06-15 10:02:52,101 INFO spark.SecurityManager: Changing view acls groups to: \u001b[0m\n",
      "\u001b[34m2023-06-15 10:02:52,102 INFO spark.SecurityManager: Changing modify acls groups to: \u001b[0m\n",
      "\u001b[34m2023-06-15 10:02:52,102 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\u001b[0m\n",
      "\u001b[34m2023-06-15 10:02:52,130 INFO yarn.Client: Submitting application application_1686823347572_0001 to ResourceManager\u001b[0m\n",
      "\u001b[34m2023-06-15 10:02:52,313 INFO impl.YarnClientImpl: Submitted application application_1686823347572_0001\u001b[0m\n",
      "\u001b[34m2023-06-15 10:02:53,317 INFO yarn.Client: Application report for application_1686823347572_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2023-06-15 10:02:53,320 INFO yarn.Client: \u001b[0m\n",
      "\u001b[34m#011 client token: N/A\u001b[0m\n",
      "\u001b[34m#011 diagnostics: AM container is launched, waiting for AM container to Register with RM\u001b[0m\n",
      "\u001b[34m#011 ApplicationMaster host: N/A\u001b[0m\n",
      "\u001b[34m#011 ApplicationMaster RPC port: -1\u001b[0m\n",
      "\u001b[34m#011 queue: default\u001b[0m\n",
      "\u001b[34m#011 start time: 1686823372221\u001b[0m\n",
      "\u001b[34m#011 final status: UNDEFINED\u001b[0m\n",
      "\u001b[34m#011 tracking URL: http://algo-1:8088/proxy/application_1686823347572_0001/\u001b[0m\n",
      "\u001b[34m#011 user: root\u001b[0m\n",
      "\u001b[34m2023-06-15 10:02:54,325 INFO yarn.Client: Application report for application_1686823347572_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2023-06-15 10:02:55,327 INFO yarn.Client: Application report for application_1686823347572_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2023-06-15 10:02:56,330 INFO yarn.Client: Application report for application_1686823347572_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2023-06-15 10:02:57,337 INFO yarn.Client: Application report for application_1686823347572_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2023-06-15 10:02:57,568 INFO cluster.YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> algo-1, PROXY_URI_BASES -> http://algo-1:8088/proxy/application_1686823347572_0001), /proxy/application_1686823347572_0001\u001b[0m\n",
      "\u001b[34m2023-06-15 10:02:58,341 INFO yarn.Client: Application report for application_1686823347572_0001 (state: RUNNING)\u001b[0m\n",
      "\u001b[34m2023-06-15 10:02:58,342 INFO yarn.Client: \u001b[0m\n",
      "\u001b[34m#011 client token: N/A\u001b[0m\n",
      "\u001b[34m#011 diagnostics: N/A\u001b[0m\n",
      "\u001b[34m#011 ApplicationMaster host: 10.0.96.99\u001b[0m\n",
      "\u001b[34m#011 ApplicationMaster RPC port: -1\u001b[0m\n",
      "\u001b[34m#011 queue: default\u001b[0m\n",
      "\u001b[34m#011 start time: 1686823372221\u001b[0m\n",
      "\u001b[34m#011 final status: UNDEFINED\u001b[0m\n",
      "\u001b[34m#011 tracking URL: http://algo-1:8088/proxy/application_1686823347572_0001/\u001b[0m\n",
      "\u001b[34m#011 user: root\u001b[0m\n",
      "\u001b[34m2023-06-15 10:02:58,344 INFO cluster.YarnClientSchedulerBackend: Application application_1686823347572_0001 has started running.\u001b[0m\n",
      "\u001b[34m2023-06-15 10:02:58,358 INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 45033.\u001b[0m\n",
      "\u001b[34m2023-06-15 10:02:58,358 INFO netty.NettyBlockTransferService: Server created on 10.0.96.99:45033\u001b[0m\n",
      "\u001b[34m2023-06-15 10:02:58,360 INFO storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\u001b[0m\n",
      "\u001b[34m2023-06-15 10:02:58,370 INFO storage.BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 10.0.96.99, 45033, None)\u001b[0m\n",
      "\u001b[34m2023-06-15 10:02:58,374 INFO storage.BlockManagerMasterEndpoint: Registering block manager 10.0.96.99:45033 with 1458.6 MiB RAM, BlockManagerId(driver, 10.0.96.99, 45033, None)\u001b[0m\n",
      "\u001b[34m2023-06-15 10:02:58,377 INFO storage.BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 10.0.96.99, 45033, None)\u001b[0m\n",
      "\u001b[34m2023-06-15 10:02:58,378 INFO storage.BlockManager: Initialized BlockManager: BlockManagerId(driver, 10.0.96.99, 45033, None)\u001b[0m\n",
      "\u001b[34m2023-06-15 10:02:58,525 INFO util.log: Logging initialized @15900ms to org.sparkproject.jetty.util.log.Slf4jLog\u001b[0m\n",
      "\u001b[34m2023-06-15 10:02:59,074 INFO cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM)\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:03,414 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.0.96.99:51742) with ID 1,  ResourceProfileId 0\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:03,598 INFO storage.BlockManagerMasterEndpoint: Registering block manager algo-1:39213 with 5.8 GiB RAM, BlockManagerId(1, algo-1, 39213, None)\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:16,342 INFO cluster.YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after waiting maxRegisteredResourcesWaitingTime: 30000000000(ns)\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:16,444 WARN spark.SparkContext: Spark is not running in local mode, therefore the checkpoint directory must not be on the local filesystem. Directory '/tmp' appears to be on the local filesystem.\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:16,484 INFO internal.SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:16,486 INFO internal.SharedState: Warehouse path is 'file:/usr/spark-3.3.0/spark-warehouse'.\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:17,425 INFO datasources.InMemoryFileIndex: It took 38 ms to list leaf files for 1 paths.\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:17,590 INFO memory.MemoryStore: Block broadcast_0 stored as values in memory (estimated size 416.9 KiB, free 1458.2 MiB)\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:17,861 INFO memory.MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 39.3 KiB, free 1458.2 MiB)\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:17,863 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.0.96.99:45033 (size: 39.3 KiB, free: 1458.6 MiB)\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:17,867 INFO spark.SparkContext: Created broadcast 0 from csv at DatasetReader.scala:99\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:18,188 INFO input.FileInputFormat: Total input files to process : 1\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:18,190 INFO input.FileInputFormat: Total input files to process : 1\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:18,193 INFO input.CombineFileInputFormat: DEBUG: Terminated node allocation with : CompletedNodes: 1, size left: 4596\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:18,241 INFO spark.SparkContext: Starting job: csv at DatasetReader.scala:99\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:18,257 INFO scheduler.DAGScheduler: Got job 0 (csv at DatasetReader.scala:99) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:18,257 INFO scheduler.DAGScheduler: Final stage: ResultStage 0 (csv at DatasetReader.scala:99)\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:18,257 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:18,259 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:18,263 INFO scheduler.DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at csv at DatasetReader.scala:99), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:18,308 INFO memory.MemoryStore: Block broadcast_1 stored as values in memory (estimated size 7.3 KiB, free 1458.1 MiB)\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:18,311 INFO memory.MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 4.2 KiB, free 1458.1 MiB)\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:18,312 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on 10.0.96.99:45033 (size: 4.2 KiB, free: 1458.6 MiB)\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:18,313 INFO spark.SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:18,326 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at csv at DatasetReader.scala:99) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:18,326 INFO cluster.YarnScheduler: Adding task set 0.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:18,366 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4640 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:18,603 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on algo-1:39213 (size: 4.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:19,453 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on algo-1:39213 (size: 39.3 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:19,786 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1435 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:19,788 INFO cluster.YarnScheduler: Removed TaskSet 0.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:19,794 INFO scheduler.DAGScheduler: ResultStage 0 (csv at DatasetReader.scala:99) finished in 1.510 s\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:19,823 INFO scheduler.DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:19,823 INFO cluster.YarnScheduler: Killing all running tasks in stage 0: Stage finished\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:19,826 INFO scheduler.DAGScheduler: Job 0 finished: csv at DatasetReader.scala:99, took 1.585070 s\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:20,006 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on 10.0.96.99:45033 in memory (size: 4.2 KiB, free: 1458.6 MiB)\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:20,010 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on algo-1:39213 in memory (size: 4.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:20,041 INFO storage.BlockManagerInfo: Removed broadcast_0_piece0 on 10.0.96.99:45033 in memory (size: 39.3 KiB, free: 1458.6 MiB)\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:20,042 INFO storage.BlockManagerInfo: Removed broadcast_0_piece0 on algo-1:39213 in memory (size: 39.3 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:22,301 INFO datasources.FileSourceStrategy: Pushed Filters: \u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:22,302 INFO datasources.FileSourceStrategy: Post-Scan Filters: \u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:22,304 INFO datasources.FileSourceStrategy: Output Data Schema: struct<>\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:22,663 INFO codegen.CodeGenerator: Code generated in 151.40467 ms\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:22,670 INFO memory.MemoryStore: Block broadcast_2 stored as values in memory (estimated size 416.5 KiB, free 1458.2 MiB)\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:22,682 INFO memory.MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 39.2 KiB, free 1458.2 MiB)\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:22,683 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on 10.0.96.99:45033 (size: 39.2 KiB, free: 1458.6 MiB)\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:22,685 INFO spark.SparkContext: Created broadcast 2 from count at RegressionAnalyzer.scala:32\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:22,696 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:22,739 INFO scheduler.DAGScheduler: Registering RDD 7 (count at RegressionAnalyzer.scala:32) as input to shuffle 0\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:22,742 INFO scheduler.DAGScheduler: Got map stage job 1 (count at RegressionAnalyzer.scala:32) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:22,743 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 1 (count at RegressionAnalyzer.scala:32)\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:22,743 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:22,743 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:22,747 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 1 (MapPartitionsRDD[7] at count at RegressionAnalyzer.scala:32), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:22,790 INFO memory.MemoryStore: Block broadcast_3 stored as values in memory (estimated size 16.0 KiB, free 1458.1 MiB)\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:22,792 INFO memory.MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 8.1 KiB, free 1458.1 MiB)\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:22,793 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on 10.0.96.99:45033 (size: 8.1 KiB, free: 1458.6 MiB)\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:22,794 INFO spark.SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:22,796 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[7] at count at RegressionAnalyzer.scala:32) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:22,796 INFO cluster.YarnScheduler: Adding task set 1.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:22,801 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4957 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:22,847 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on algo-1:39213 (size: 8.1 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:23,758 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on algo-1:39213 (size: 39.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:23,863 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 1065 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:23,863 INFO cluster.YarnScheduler: Removed TaskSet 1.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:23,868 INFO scheduler.DAGScheduler: ShuffleMapStage 1 (count at RegressionAnalyzer.scala:32) finished in 1.113 s\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:23,869 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:23,870 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:23,871 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:23,871 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:23,926 INFO codegen.CodeGenerator: Code generated in 17.7452 ms\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:23,966 INFO spark.SparkContext: Starting job: count at RegressionAnalyzer.scala:32\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:23,969 INFO scheduler.DAGScheduler: Got job 2 (count at RegressionAnalyzer.scala:32) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:23,969 INFO scheduler.DAGScheduler: Final stage: ResultStage 3 (count at RegressionAnalyzer.scala:32)\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:23,969 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 2)\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:23,970 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:23,972 INFO scheduler.DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[10] at count at RegressionAnalyzer.scala:32), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:23,986 INFO memory.MemoryStore: Block broadcast_4 stored as values in memory (estimated size 11.1 KiB, free 1458.1 MiB)\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:23,988 INFO memory.MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 5.5 KiB, free 1458.1 MiB)\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:23,990 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on 10.0.96.99:45033 (size: 5.5 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:23,991 INFO spark.SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:23,992 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[10] at count at RegressionAnalyzer.scala:32) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:23,992 INFO cluster.YarnScheduler: Adding task set 3.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:24,002 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 3.0 (TID 2) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:24,042 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on algo-1:39213 (size: 5.5 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:24,106 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 10.0.96.99:51742\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:24,258 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 3.0 (TID 2) in 258 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:24,259 INFO cluster.YarnScheduler: Removed TaskSet 3.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:24,259 INFO scheduler.DAGScheduler: ResultStage 3 (count at RegressionAnalyzer.scala:32) finished in 0.275 s\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:24,260 INFO scheduler.DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:24,260 INFO cluster.YarnScheduler: Killing all running tasks in stage 3: Stage finished\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:24,260 INFO scheduler.DAGScheduler: Job 2 finished: count at RegressionAnalyzer.scala:32, took 0.294014 s\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:24,368 INFO datasources.FileSourceStrategy: Pushed Filters: \u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:24,368 INFO datasources.FileSourceStrategy: Post-Scan Filters: \u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:24,368 INFO datasources.FileSourceStrategy: Output Data Schema: struct<prediction: string, label: string>\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:24,400 INFO codegen.CodeGenerator: Code generated in 15.968371 ms\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:24,407 INFO memory.MemoryStore: Block broadcast_5 stored as values in memory (estimated size 416.5 KiB, free 1457.7 MiB)\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:24,424 INFO memory.MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 39.2 KiB, free 1457.7 MiB)\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:24,425 INFO storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on 10.0.96.99:45033 (size: 39.2 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:24,426 INFO spark.SparkContext: Created broadcast 5 from rdd at RegressionAnalyzer.scala:87\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:24,427 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:24,513 INFO spark.SparkContext: Starting job: treeAggregate at Statistics.scala:58\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:24,514 INFO scheduler.DAGScheduler: Got job 3 (treeAggregate at Statistics.scala:58) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:24,514 INFO scheduler.DAGScheduler: Final stage: ResultStage 4 (treeAggregate at Statistics.scala:58)\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:24,515 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:24,515 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:24,516 INFO scheduler.DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[19] at treeAggregate at Statistics.scala:58), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:24,550 INFO memory.MemoryStore: Block broadcast_6 stored as values in memory (estimated size 29.3 KiB, free 1457.6 MiB)\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:24,551 INFO memory.MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 13.7 KiB, free 1457.6 MiB)\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:24,552 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on 10.0.96.99:45033 (size: 13.7 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:24,552 INFO spark.SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:24,553 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[19] at treeAggregate at Statistics.scala:58) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:24,553 INFO cluster.YarnScheduler: Adding task set 4.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:24,554 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 4.0 (TID 3) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4968 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:24,574 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on algo-1:39213 (size: 13.7 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:25,912 INFO storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on algo-1:39213 (size: 39.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\n",
      "\u001b[34m2023-06-15 10:03:26,497 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 4.0 (TID 3) in 1943 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:26,498 INFO cluster.YarnScheduler: Removed TaskSet 4.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:26,498 INFO scheduler.DAGScheduler: ResultStage 4 (treeAggregate at Statistics.scala:58) finished in 1.980 s\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:26,498 INFO scheduler.DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:26,498 INFO cluster.YarnScheduler: Killing all running tasks in stage 4: Stage finished\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:26,499 INFO scheduler.DAGScheduler: Job 3 finished: treeAggregate at Statistics.scala:58, took 1.986181 s\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:26,655 INFO datasources.FileSourceStrategy: Pushed Filters: \u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:26,655 INFO datasources.FileSourceStrategy: Post-Scan Filters: \u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:26,655 INFO datasources.FileSourceStrategy: Output Data Schema: struct<prediction: string, label: string>\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:26,707 INFO codegen.CodeGenerator: Code generated in 29.509255 ms\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:26,713 INFO memory.MemoryStore: Block broadcast_7 stored as values in memory (estimated size 416.5 KiB, free 1457.2 MiB)\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:26,730 INFO memory.MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 39.2 KiB, free 1457.2 MiB)\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:26,730 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on 10.0.96.99:45033 (size: 39.2 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:26,731 INFO spark.SparkContext: Created broadcast 7 from rdd at RegressionAnalyzer.scala:87\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:26,733 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:26,757 INFO spark.SparkContext: Starting job: treeAggregate at Statistics.scala:58\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:26,758 INFO scheduler.DAGScheduler: Got job 4 (treeAggregate at Statistics.scala:58) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:26,758 INFO scheduler.DAGScheduler: Final stage: ResultStage 5 (treeAggregate at Statistics.scala:58)\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:26,758 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:26,759 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:26,761 INFO scheduler.DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[28] at treeAggregate at Statistics.scala:58), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:26,786 INFO memory.MemoryStore: Block broadcast_8 stored as values in memory (estimated size 33.6 KiB, free 1457.2 MiB)\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:26,789 INFO memory.MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 15.8 KiB, free 1457.1 MiB)\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:26,790 INFO storage.BlockManagerInfo: Added broadcast_8_piece0 in memory on 10.0.96.99:45033 (size: 15.8 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:26,791 INFO spark.SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:26,791 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[28] at treeAggregate at Statistics.scala:58) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:26,792 INFO cluster.YarnScheduler: Adding task set 5.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:26,793 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 5.0 (TID 4) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4968 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:26,816 INFO storage.BlockManagerInfo: Added broadcast_8_piece0 in memory on algo-1:39213 (size: 15.8 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:26,946 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on algo-1:39213 (size: 39.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:27,018 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 5.0 (TID 4) in 225 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:27,019 INFO cluster.YarnScheduler: Removed TaskSet 5.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:27,020 INFO scheduler.DAGScheduler: ResultStage 5 (treeAggregate at Statistics.scala:58) finished in 0.258 s\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:27,020 INFO scheduler.DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:27,020 INFO cluster.YarnScheduler: Killing all running tasks in stage 5: Stage finished\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:27,021 INFO scheduler.DAGScheduler: Job 4 finished: treeAggregate at Statistics.scala:58, took 0.263604 s\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:27,073 INFO datasources.FileSourceStrategy: Pushed Filters: \u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:27,073 INFO datasources.FileSourceStrategy: Post-Scan Filters: \u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:27,074 INFO datasources.FileSourceStrategy: Output Data Schema: struct<prediction: string, label: string>\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:27,110 INFO codegen.CodeGenerator: Code generated in 21.966606 ms\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:27,116 INFO memory.MemoryStore: Block broadcast_9 stored as values in memory (estimated size 416.5 KiB, free 1456.7 MiB)\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:27,132 INFO memory.MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 39.2 KiB, free 1456.7 MiB)\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:27,134 INFO storage.BlockManagerInfo: Added broadcast_9_piece0 in memory on 10.0.96.99:45033 (size: 39.2 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:27,135 INFO spark.SparkContext: Created broadcast 9 from rdd at RegressionAnalyzer.scala:87\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:27,136 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:27,161 INFO spark.SparkContext: Starting job: treeAggregate at Statistics.scala:58\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:27,163 INFO scheduler.DAGScheduler: Got job 5 (treeAggregate at Statistics.scala:58) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:27,163 INFO scheduler.DAGScheduler: Final stage: ResultStage 6 (treeAggregate at Statistics.scala:58)\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:27,163 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:27,164 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:27,165 INFO scheduler.DAGScheduler: Submitting ResultStage 6 (MapPartitionsRDD[37] at treeAggregate at Statistics.scala:58), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:27,186 INFO memory.MemoryStore: Block broadcast_10 stored as values in memory (estimated size 33.6 KiB, free 1456.7 MiB)\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:27,188 INFO memory.MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 15.8 KiB, free 1456.6 MiB)\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:27,189 INFO storage.BlockManagerInfo: Added broadcast_10_piece0 in memory on 10.0.96.99:45033 (size: 15.8 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:27,189 INFO spark.SparkContext: Created broadcast 10 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:27,190 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[37] at treeAggregate at Statistics.scala:58) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:27,190 INFO cluster.YarnScheduler: Adding task set 6.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:27,192 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 6.0 (TID 5) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4968 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:27,211 INFO storage.BlockManagerInfo: Added broadcast_10_piece0 in memory on algo-1:39213 (size: 15.8 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:27,273 INFO storage.BlockManagerInfo: Added broadcast_9_piece0 in memory on algo-1:39213 (size: 39.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:27,314 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 6.0 (TID 5) in 122 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:27,314 INFO cluster.YarnScheduler: Removed TaskSet 6.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:27,315 INFO scheduler.DAGScheduler: ResultStage 6 (treeAggregate at Statistics.scala:58) finished in 0.146 s\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:27,315 INFO scheduler.DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:27,315 INFO cluster.YarnScheduler: Killing all running tasks in stage 6: Stage finished\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:27,316 INFO scheduler.DAGScheduler: Job 5 finished: treeAggregate at Statistics.scala:58, took 0.153853 s\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:27,363 INFO datasources.FileSourceStrategy: Pushed Filters: \u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:27,363 INFO datasources.FileSourceStrategy: Post-Scan Filters: \u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:27,363 INFO datasources.FileSourceStrategy: Output Data Schema: struct<prediction: string, label: string>\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:27,404 INFO codegen.CodeGenerator: Code generated in 27.033052 ms\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:27,414 INFO memory.MemoryStore: Block broadcast_11 stored as values in memory (estimated size 416.5 KiB, free 1456.2 MiB)\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:27,434 INFO memory.MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 39.2 KiB, free 1456.2 MiB)\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:27,434 INFO storage.BlockManagerInfo: Added broadcast_11_piece0 in memory on 10.0.96.99:45033 (size: 39.2 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:27,435 INFO spark.SparkContext: Created broadcast 11 from rdd at RegressionAnalyzer.scala:87\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:27,436 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:27,463 INFO spark.SparkContext: Starting job: treeAggregate at Statistics.scala:58\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:27,465 INFO scheduler.DAGScheduler: Got job 6 (treeAggregate at Statistics.scala:58) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:27,465 INFO scheduler.DAGScheduler: Final stage: ResultStage 7 (treeAggregate at Statistics.scala:58)\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:27,466 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:27,466 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:27,469 INFO scheduler.DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[46] at treeAggregate at Statistics.scala:58), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:27,482 INFO memory.MemoryStore: Block broadcast_12 stored as values in memory (estimated size 33.6 KiB, free 1456.2 MiB)\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:27,484 INFO memory.MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 15.8 KiB, free 1456.1 MiB)\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:27,486 INFO storage.BlockManagerInfo: Added broadcast_12_piece0 in memory on 10.0.96.99:45033 (size: 15.8 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:27,486 INFO spark.SparkContext: Created broadcast 12 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:27,487 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[46] at treeAggregate at Statistics.scala:58) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:27,487 INFO cluster.YarnScheduler: Adding task set 7.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:27,492 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 7.0 (TID 6) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4968 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:27,508 INFO storage.BlockManagerInfo: Added broadcast_12_piece0 in memory on algo-1:39213 (size: 15.8 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:27,567 INFO storage.BlockManagerInfo: Added broadcast_11_piece0 in memory on algo-1:39213 (size: 39.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:27,598 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 7.0 (TID 6) in 106 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:27,598 INFO cluster.YarnScheduler: Removed TaskSet 7.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:27,599 INFO scheduler.DAGScheduler: ResultStage 7 (treeAggregate at Statistics.scala:58) finished in 0.129 s\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:27,601 INFO scheduler.DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:27,601 INFO cluster.YarnScheduler: Killing all running tasks in stage 7: Stage finished\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:27,602 INFO scheduler.DAGScheduler: Job 6 finished: treeAggregate at Statistics.scala:58, took 0.138499 s\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:27,647 INFO datasources.FileSourceStrategy: Pushed Filters: \u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:27,648 INFO datasources.FileSourceStrategy: Post-Scan Filters: \u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:27,648 INFO datasources.FileSourceStrategy: Output Data Schema: struct<prediction: string, label: string>\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:27,690 INFO codegen.CodeGenerator: Code generated in 16.951373 ms\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:27,695 INFO memory.MemoryStore: Block broadcast_13 stored as values in memory (estimated size 416.5 KiB, free 1455.7 MiB)\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:27,714 INFO memory.MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 39.2 KiB, free 1455.7 MiB)\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:27,715 INFO storage.BlockManagerInfo: Added broadcast_13_piece0 in memory on 10.0.96.99:45033 (size: 39.2 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:27,717 INFO spark.SparkContext: Created broadcast 13 from rdd at RegressionAnalyzer.scala:87\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:27,719 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:27,751 INFO spark.SparkContext: Starting job: treeAggregate at Statistics.scala:58\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:27,752 INFO scheduler.DAGScheduler: Got job 7 (treeAggregate at Statistics.scala:58) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:27,752 INFO scheduler.DAGScheduler: Final stage: ResultStage 8 (treeAggregate at Statistics.scala:58)\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:27,753 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:27,753 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:27,754 INFO scheduler.DAGScheduler: Submitting ResultStage 8 (MapPartitionsRDD[55] at treeAggregate at Statistics.scala:58), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:27,764 INFO memory.MemoryStore: Block broadcast_14 stored as values in memory (estimated size 33.6 KiB, free 1455.7 MiB)\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:27,768 INFO memory.MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 15.8 KiB, free 1455.7 MiB)\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:27,775 INFO storage.BlockManagerInfo: Added broadcast_14_piece0 in memory on 10.0.96.99:45033 (size: 15.8 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:27,777 INFO spark.SparkContext: Created broadcast 14 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:27,777 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 8 (MapPartitionsRDD[55] at treeAggregate at Statistics.scala:58) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:27,777 INFO cluster.YarnScheduler: Adding task set 8.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:27,779 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 8.0 (TID 7) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4968 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:27,793 INFO storage.BlockManagerInfo: Added broadcast_14_piece0 in memory on algo-1:39213 (size: 15.8 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:27,913 INFO storage.BlockManagerInfo: Added broadcast_13_piece0 in memory on algo-1:39213 (size: 39.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:27,950 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 8.0 (TID 7) in 172 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:27,950 INFO cluster.YarnScheduler: Removed TaskSet 8.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:27,953 INFO scheduler.DAGScheduler: ResultStage 8 (treeAggregate at Statistics.scala:58) finished in 0.197 s\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:27,954 INFO scheduler.DAGScheduler: Job 7 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:27,955 INFO cluster.YarnScheduler: Killing all running tasks in stage 8: Stage finished\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:27,955 INFO scheduler.DAGScheduler: Job 7 finished: treeAggregate at Statistics.scala:58, took 0.203754 s\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:28,004 INFO datasources.FileSourceStrategy: Pushed Filters: \u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:28,004 INFO datasources.FileSourceStrategy: Post-Scan Filters: \u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:28,005 INFO datasources.FileSourceStrategy: Output Data Schema: struct<prediction: string, label: string>\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:28,036 INFO codegen.CodeGenerator: Code generated in 19.819131 ms\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:28,040 INFO memory.MemoryStore: Block broadcast_15 stored as values in memory (estimated size 416.5 KiB, free 1455.2 MiB)\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:28,056 INFO memory.MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 39.2 KiB, free 1455.2 MiB)\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:28,057 INFO storage.BlockManagerInfo: Added broadcast_15_piece0 in memory on 10.0.96.99:45033 (size: 39.2 KiB, free: 1458.2 MiB)\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:28,057 INFO spark.SparkContext: Created broadcast 15 from rdd at RegressionAnalyzer.scala:87\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:28,058 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:28,081 INFO spark.SparkContext: Starting job: treeAggregate at Statistics.scala:58\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:28,082 INFO scheduler.DAGScheduler: Got job 8 (treeAggregate at Statistics.scala:58) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:28,082 INFO scheduler.DAGScheduler: Final stage: ResultStage 9 (treeAggregate at Statistics.scala:58)\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:28,082 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:28,082 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:28,083 INFO scheduler.DAGScheduler: Submitting ResultStage 9 (MapPartitionsRDD[64] at treeAggregate at Statistics.scala:58), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:28,091 INFO memory.MemoryStore: Block broadcast_16 stored as values in memory (estimated size 33.6 KiB, free 1455.2 MiB)\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:28,100 INFO memory.MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 15.8 KiB, free 1455.2 MiB)\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:28,100 INFO storage.BlockManagerInfo: Added broadcast_16_piece0 in memory on 10.0.96.99:45033 (size: 15.8 KiB, free: 1458.2 MiB)\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:28,101 INFO spark.SparkContext: Created broadcast 16 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:28,102 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 9 (MapPartitionsRDD[64] at treeAggregate at Statistics.scala:58) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:28,102 INFO cluster.YarnScheduler: Adding task set 9.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:28,103 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 9.0 (TID 8) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4968 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:28,114 INFO storage.BlockManagerInfo: Added broadcast_16_piece0 in memory on algo-1:39213 (size: 15.8 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:28,152 INFO storage.BlockManagerInfo: Added broadcast_15_piece0 in memory on algo-1:39213 (size: 39.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:28,177 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 9.0 (TID 8) in 74 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:28,177 INFO cluster.YarnScheduler: Removed TaskSet 9.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:28,178 INFO scheduler.DAGScheduler: ResultStage 9 (treeAggregate at Statistics.scala:58) finished in 0.094 s\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:28,179 INFO scheduler.DAGScheduler: Job 8 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:28,179 INFO cluster.YarnScheduler: Killing all running tasks in stage 9: Stage finished\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:28,180 INFO scheduler.DAGScheduler: Job 8 finished: treeAggregate at Statistics.scala:58, took 0.098842 s\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:28,364 INFO codegen.CodeGenerator: Code generated in 15.021534 ms\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:28,493 INFO storage.BlockManagerInfo: Removed broadcast_4_piece0 on algo-1:39213 in memory (size: 5.5 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:28,495 INFO storage.BlockManagerInfo: Removed broadcast_4_piece0 on 10.0.96.99:45033 in memory (size: 5.5 KiB, free: 1458.2 MiB)\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:28,515 INFO storage.BlockManagerInfo: Removed broadcast_2_piece0 on 10.0.96.99:45033 in memory (size: 39.2 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:28,519 INFO storage.BlockManagerInfo: Removed broadcast_2_piece0 on algo-1:39213 in memory (size: 39.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:28,537 INFO storage.BlockManagerInfo: Removed broadcast_8_piece0 on algo-1:39213 in memory (size: 15.8 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:28,546 INFO storage.BlockManagerInfo: Removed broadcast_8_piece0 on 10.0.96.99:45033 in memory (size: 15.8 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:28,551 INFO storage.BlockManagerInfo: Removed broadcast_15_piece0 on algo-1:39213 in memory (size: 39.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:28,553 INFO codegen.CodeGenerator: Code generated in 36.406269 ms\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:28,562 INFO storage.BlockManagerInfo: Removed broadcast_15_piece0 on 10.0.96.99:45033 in memory (size: 39.2 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:28,570 INFO codegen.CodeGenerator: Code generated in 11.100414 ms\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:28,581 INFO scheduler.DAGScheduler: Registering RDD 67 (first at RegressionAnalyzer.scala:57) as input to shuffle 1\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:28,581 INFO scheduler.DAGScheduler: Got map stage job 9 (first at RegressionAnalyzer.scala:57) with 3 output partitions\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:28,581 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 10 (first at RegressionAnalyzer.scala:57)\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:28,581 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:28,581 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:28,582 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 10 (MapPartitionsRDD[67] at first at RegressionAnalyzer.scala:57), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:28,595 INFO memory.MemoryStore: Block broadcast_17 stored as values in memory (estimated size 25.7 KiB, free 1456.1 MiB)\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:28,597 INFO memory.MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 9.6 KiB, free 1456.1 MiB)\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:28,597 INFO storage.BlockManagerInfo: Added broadcast_17_piece0 in memory on 10.0.96.99:45033 (size: 9.6 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:28,598 INFO spark.SparkContext: Created broadcast 17 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:28,599 INFO scheduler.DAGScheduler: Submitting 3 missing tasks from ShuffleMapStage 10 (MapPartitionsRDD[67] at first at RegressionAnalyzer.scala:57) (first 15 tasks are for partitions Vector(0, 1, 2))\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:28,599 INFO cluster.YarnScheduler: Adding task set 10.0 with 3 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:28,604 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 10.0 (TID 9) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4468 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:28,604 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 10.0 (TID 10) (algo-1, executor 1, partition 1, PROCESS_LOCAL, 4519 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:28,604 INFO scheduler.TaskSetManager: Starting task 2.0 in stage 10.0 (TID 11) (algo-1, executor 1, partition 2, PROCESS_LOCAL, 4519 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:28,618 INFO storage.BlockManagerInfo: Removed broadcast_16_piece0 on 10.0.96.99:45033 in memory (size: 15.8 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:28,636 INFO storage.BlockManagerInfo: Removed broadcast_16_piece0 on algo-1:39213 in memory (size: 15.8 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:28,647 INFO storage.BlockManagerInfo: Added broadcast_17_piece0 in memory on algo-1:39213 (size: 9.6 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:28,659 INFO storage.BlockManagerInfo: Removed broadcast_9_piece0 on 10.0.96.99:45033 in memory (size: 39.2 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:28,688 INFO storage.BlockManagerInfo: Removed broadcast_9_piece0 on algo-1:39213 in memory (size: 39.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:28,699 INFO storage.BlockManagerInfo: Removed broadcast_10_piece0 on 10.0.96.99:45033 in memory (size: 15.8 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:28,701 INFO storage.BlockManagerInfo: Removed broadcast_10_piece0 on algo-1:39213 in memory (size: 15.8 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:28,716 INFO storage.BlockManagerInfo: Removed broadcast_12_piece0 on 10.0.96.99:45033 in memory (size: 15.8 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:28,722 INFO storage.BlockManagerInfo: Removed broadcast_12_piece0 on algo-1:39213 in memory (size: 15.8 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:28,730 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 10.0 (TID 9) in 130 ms on algo-1 (executor 1) (1/3)\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:28,731 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 10.0 (TID 10) in 127 ms on algo-1 (executor 1) (2/3)\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:28,737 INFO scheduler.TaskSetManager: Finished task 2.0 in stage 10.0 (TID 11) in 133 ms on algo-1 (executor 1) (3/3)\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:28,737 INFO cluster.YarnScheduler: Removed TaskSet 10.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:28,738 INFO scheduler.DAGScheduler: ShuffleMapStage 10 (first at RegressionAnalyzer.scala:57) finished in 0.155 s\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:28,738 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:28,738 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:28,738 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:28,738 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:28,757 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on 10.0.96.99:45033 in memory (size: 8.1 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:28,758 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on algo-1:39213 in memory (size: 8.1 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:28,770 INFO storage.BlockManagerInfo: Removed broadcast_11_piece0 on 10.0.96.99:45033 in memory (size: 39.2 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:28,771 INFO storage.BlockManagerInfo: Removed broadcast_11_piece0 on algo-1:39213 in memory (size: 39.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:28,783 INFO storage.BlockManagerInfo: Removed broadcast_5_piece0 on 10.0.96.99:45033 in memory (size: 39.2 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:28,784 INFO storage.BlockManagerInfo: Removed broadcast_5_piece0 on algo-1:39213 in memory (size: 39.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:28,801 INFO storage.BlockManagerInfo: Removed broadcast_6_piece0 on 10.0.96.99:45033 in memory (size: 13.7 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:28,802 INFO storage.BlockManagerInfo: Removed broadcast_6_piece0 on algo-1:39213 in memory (size: 13.7 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:28,834 INFO storage.BlockManagerInfo: Removed broadcast_13_piece0 on 10.0.96.99:45033 in memory (size: 39.2 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:28,834 INFO storage.BlockManagerInfo: Removed broadcast_13_piece0 on algo-1:39213 in memory (size: 39.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:28,848 INFO storage.BlockManagerInfo: Removed broadcast_7_piece0 on algo-1:39213 in memory (size: 39.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:28,849 INFO storage.BlockManagerInfo: Removed broadcast_7_piece0 on 10.0.96.99:45033 in memory (size: 39.2 KiB, free: 1458.6 MiB)\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:28,850 INFO codegen.CodeGenerator: Code generated in 46.828 ms\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:28,863 INFO spark.SparkContext: Starting job: first at RegressionAnalyzer.scala:57\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:28,864 INFO scheduler.DAGScheduler: Got job 10 (first at RegressionAnalyzer.scala:57) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:28,864 INFO scheduler.DAGScheduler: Final stage: ResultStage 12 (first at RegressionAnalyzer.scala:57)\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:28,865 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 11)\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:28,865 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:28,866 INFO scheduler.DAGScheduler: Submitting ResultStage 12 (MapPartitionsRDD[70] at first at RegressionAnalyzer.scala:57), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:28,868 INFO memory.MemoryStore: Block broadcast_18 stored as values in memory (estimated size 30.2 KiB, free 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:28,869 INFO memory.MemoryStore: Block broadcast_18_piece0 stored as bytes in memory (estimated size 10.4 KiB, free 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:28,869 INFO storage.BlockManagerInfo: Added broadcast_18_piece0 in memory on 10.0.96.99:45033 (size: 10.4 KiB, free: 1458.6 MiB)\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:28,870 INFO spark.SparkContext: Created broadcast 18 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:28,870 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 12 (MapPartitionsRDD[70] at first at RegressionAnalyzer.scala:57) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:28,870 INFO cluster.YarnScheduler: Adding task set 12.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:28,872 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 12.0 (TID 12) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:28,880 INFO storage.BlockManagerInfo: Removed broadcast_14_piece0 on 10.0.96.99:45033 in memory (size: 15.8 KiB, free: 1458.6 MiB)\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:28,886 INFO storage.BlockManagerInfo: Removed broadcast_14_piece0 on algo-1:39213 in memory (size: 15.8 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:28,893 INFO storage.BlockManagerInfo: Added broadcast_18_piece0 in memory on algo-1:39213 (size: 10.4 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:28,899 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to 10.0.96.99:51742\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:28,955 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 12.0 (TID 12) in 84 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:28,955 INFO cluster.YarnScheduler: Removed TaskSet 12.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:28,957 INFO scheduler.DAGScheduler: ResultStage 12 (first at RegressionAnalyzer.scala:57) finished in 0.089 s\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:28,957 INFO scheduler.DAGScheduler: Job 10 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:28,957 INFO cluster.YarnScheduler: Killing all running tasks in stage 12: Stage finished\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:28,958 INFO scheduler.DAGScheduler: Job 10 finished: first at RegressionAnalyzer.scala:57, took 0.094743 s\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:28,979 INFO codegen.CodeGenerator: Code generated in 9.922121 ms\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:29,006 INFO datasources.FileSourceStrategy: Pushed Filters: \u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:29,006 INFO datasources.FileSourceStrategy: Post-Scan Filters: \u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:29,007 INFO datasources.FileSourceStrategy: Output Data Schema: struct<>\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:29,020 INFO memory.MemoryStore: Block broadcast_19 stored as values in memory (estimated size 416.5 KiB, free 1458.1 MiB)\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:29,035 INFO memory.MemoryStore: Block broadcast_19_piece0 stored as bytes in memory (estimated size 39.2 KiB, free 1458.1 MiB)\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:29,035 INFO storage.BlockManagerInfo: Added broadcast_19_piece0 in memory on 10.0.96.99:45033 (size: 39.2 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:29,036 INFO spark.SparkContext: Created broadcast 19 from count at ModelQualityAnalyzer.scala:144\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:29,037 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:29,042 INFO scheduler.DAGScheduler: Registering RDD 74 (count at ModelQualityAnalyzer.scala:144) as input to shuffle 2\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:29,042 INFO scheduler.DAGScheduler: Got map stage job 11 (count at ModelQualityAnalyzer.scala:144) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:29,043 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 13 (count at ModelQualityAnalyzer.scala:144)\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:29,043 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:29,043 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:29,043 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 13 (MapPartitionsRDD[74] at count at ModelQualityAnalyzer.scala:144), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:29,046 INFO memory.MemoryStore: Block broadcast_20 stored as values in memory (estimated size 16.0 KiB, free 1458.1 MiB)\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:29,047 INFO memory.MemoryStore: Block broadcast_20_piece0 stored as bytes in memory (estimated size 8.1 KiB, free 1458.1 MiB)\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:29,048 INFO storage.BlockManagerInfo: Added broadcast_20_piece0 in memory on 10.0.96.99:45033 (size: 8.1 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:29,048 INFO spark.SparkContext: Created broadcast 20 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:29,048 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 13 (MapPartitionsRDD[74] at count at ModelQualityAnalyzer.scala:144) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:29,048 INFO cluster.YarnScheduler: Adding task set 13.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:29,050 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 13.0 (TID 13) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4957 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:29,061 INFO storage.BlockManagerInfo: Added broadcast_20_piece0 in memory on algo-1:39213 (size: 8.1 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:29,079 INFO storage.BlockManagerInfo: Added broadcast_19_piece0 in memory on algo-1:39213 (size: 39.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:29,112 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 13.0 (TID 13) in 63 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:29,112 INFO cluster.YarnScheduler: Removed TaskSet 13.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:29,113 INFO scheduler.DAGScheduler: ShuffleMapStage 13 (count at ModelQualityAnalyzer.scala:144) finished in 0.069 s\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:29,113 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:29,113 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:29,113 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:29,113 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:29,144 INFO spark.SparkContext: Starting job: count at ModelQualityAnalyzer.scala:144\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:29,147 INFO scheduler.DAGScheduler: Got job 12 (count at ModelQualityAnalyzer.scala:144) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:29,147 INFO scheduler.DAGScheduler: Final stage: ResultStage 15 (count at ModelQualityAnalyzer.scala:144)\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:29,147 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 14)\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:29,147 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:29,147 INFO scheduler.DAGScheduler: Submitting ResultStage 15 (MapPartitionsRDD[77] at count at ModelQualityAnalyzer.scala:144), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:29,149 INFO memory.MemoryStore: Block broadcast_21 stored as values in memory (estimated size 11.1 KiB, free 1458.0 MiB)\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:29,151 INFO memory.MemoryStore: Block broadcast_21_piece0 stored as bytes in memory (estimated size 5.5 KiB, free 1458.0 MiB)\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:29,151 INFO storage.BlockManagerInfo: Added broadcast_21_piece0 in memory on 10.0.96.99:45033 (size: 5.5 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:29,152 INFO spark.SparkContext: Created broadcast 21 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:29,152 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 15 (MapPartitionsRDD[77] at count at ModelQualityAnalyzer.scala:144) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:29,153 INFO cluster.YarnScheduler: Adding task set 15.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:29,154 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 15.0 (TID 14) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:29,167 INFO storage.BlockManagerInfo: Added broadcast_21_piece0 in memory on algo-1:39213 (size: 5.5 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:29,175 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 2 to 10.0.96.99:51742\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:29,184 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 15.0 (TID 14) in 29 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:29,184 INFO cluster.YarnScheduler: Removed TaskSet 15.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:29,186 INFO scheduler.DAGScheduler: ResultStage 15 (count at ModelQualityAnalyzer.scala:144) finished in 0.038 s\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:29,186 INFO scheduler.DAGScheduler: Job 12 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:29,187 INFO cluster.YarnScheduler: Killing all running tasks in stage 15: Stage finished\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:29,187 INFO scheduler.DAGScheduler: Job 12 finished: count at ModelQualityAnalyzer.scala:144, took 0.042796 s\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:29,314 INFO FileUtil: Write to file statistics.json at path /opt/ml/processing/output.\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:29,332 INFO FileUtil: Write to file constraints.json at path /opt/ml/processing/output.\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:29,340 INFO cluster.YarnClientSchedulerBackend: Interrupting monitor thread\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:29,405 INFO cluster.YarnClientSchedulerBackend: Shutting down all executors\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:29,405 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Asking each executor to shut down\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:29,410 INFO cluster.YarnClientSchedulerBackend: YARN client scheduler backend Stopped\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:29,426 INFO spark.MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:29,478 INFO memory.MemoryStore: MemoryStore cleared\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:29,483 INFO storage.BlockManager: BlockManager stopped\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:29,490 INFO storage.BlockManagerMaster: BlockManagerMaster stopped\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:29,496 INFO scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:29,566 INFO spark.SparkContext: Successfully stopped SparkContext\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:29,566 INFO Main: Completed: Job completed successfully with no violations.\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:29,566 INFO Main: Write to file /opt/ml/output/message.\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:29,581 INFO util.ShutdownHookManager: Shutdown hook called\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:29,582 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-51af56ea-8521-4344-a944-c0f03684b6a3\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:29,585 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-1cdede9c-55c2-45d9-909f-8bb9ee292ade\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:29,643 - DefaultDataAnalyzer - INFO - Completed spark-submit with return code : 0\u001b[0m\n",
      "\u001b[34m2023-06-15 10:03:29,643 - DefaultDataAnalyzer - INFO - Spark job completed.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "baseline_model_quality_job_name = \"MyAbaloneModelQualityMonitorBaselineJob-\" + datetime.now().strftime(\"%m-%d-%Y-%H-%M-%S\")\n",
    "job = model_quality_monitor.suggest_baseline(\n",
    "    job_name=baseline_model_quality_job_name,\n",
    "    baseline_dataset=f\"s3://{default_bucket}/MyAbalonePipeline/7p6mevksvmgc/MyAbaloneProcess/output/validation/validation_with_predictions.csv\",\n",
    "    dataset_format=DatasetFormat.csv(header=True),\n",
    "    output_s3_uri=f\"s3://{default_bucket}/my-abalone-model-quality-monitoring-report\",\n",
    "    problem_type=\"Regression\",\n",
    "    inference_attribute=\"prediction\",\n",
    "    # probability_attribute=\"probability\",\n",
    "    ground_truth_attribute=\"label\",\n",
    ")\n",
    "job.wait(logs=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "2ec6ee42-c22a-4189-96de-a1cf0cf68283",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "baseline_job_model_quality = model_quality_monitor.latest_baselining_job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "fd90b200-aa10-47a4-8860-b1b89ea69e03",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ProcessingInputs': [{'InputName': 'baseline_dataset_input',\n",
       "   'AppManaged': False,\n",
       "   'S3Input': {'S3Uri': 's3://sagemaker-us-east-1-709891711940/MyAbalonePipeline/7p6mevksvmgc/MyAbaloneProcess/output/validation/validation_with_predictions.csv',\n",
       "    'LocalPath': '/opt/ml/processing/input/baseline_dataset_input',\n",
       "    'S3DataType': 'S3Prefix',\n",
       "    'S3InputMode': 'File',\n",
       "    'S3DataDistributionType': 'FullyReplicated',\n",
       "    'S3CompressionType': 'None'}}],\n",
       " 'ProcessingOutputConfig': {'Outputs': [{'OutputName': 'monitoring_output',\n",
       "    'S3Output': {'S3Uri': 's3://sagemaker-us-east-1-709891711940/my-abalone-model-quality-monitoring-report',\n",
       "     'LocalPath': '/opt/ml/processing/output',\n",
       "     'S3UploadMode': 'EndOfJob'},\n",
       "    'AppManaged': False}]},\n",
       " 'ProcessingJobName': 'MyAbaloneModelQualityMonitorBaselineJob-06-15-2023-09-57-58',\n",
       " 'ProcessingResources': {'ClusterConfig': {'InstanceCount': 1,\n",
       "   'InstanceType': 'ml.m5.xlarge',\n",
       "   'VolumeSizeInGB': 20}},\n",
       " 'StoppingCondition': {'MaxRuntimeInSeconds': 1800},\n",
       " 'AppSpecification': {'ImageUri': '156813124566.dkr.ecr.us-east-1.amazonaws.com/sagemaker-model-monitor-analyzer'},\n",
       " 'Environment': {'analysis_type': 'MODEL_QUALITY',\n",
       "  'dataset_format': '{\"csv\": {\"header\": true, \"output_columns_position\": \"START\"}}',\n",
       "  'dataset_source': '/opt/ml/processing/input/baseline_dataset_input',\n",
       "  'ground_truth_attribute': 'label',\n",
       "  'inference_attribute': 'prediction',\n",
       "  'output_path': '/opt/ml/processing/output',\n",
       "  'problem_type': 'Regression',\n",
       "  'publish_cloudwatch_metrics': 'Disabled'},\n",
       " 'RoleArn': 'arn:aws:iam::709891711940:role/service-role/SageMaker-MLOpsEngineer-poc',\n",
       " 'ProcessingJobArn': 'arn:aws:sagemaker:us-east-1:709891711940:processing-job/MyAbaloneModelQualityMonitorBaselineJob-06-15-2023-09-57-58',\n",
       " 'ProcessingJobStatus': 'Completed',\n",
       " 'ExitMessage': 'Completed: Job completed successfully with no violations.',\n",
       " 'ProcessingEndTime': datetime.datetime(2023, 6, 15, 10, 3, 37, 118000, tzinfo=tzlocal()),\n",
       " 'ProcessingStartTime': datetime.datetime(2023, 6, 15, 10, 2, 8, 990000, tzinfo=tzlocal()),\n",
       " 'LastModifiedTime': datetime.datetime(2023, 6, 15, 10, 3, 37, 475000, tzinfo=tzlocal()),\n",
       " 'CreationTime': datetime.datetime(2023, 6, 15, 9, 57, 58, 534000, tzinfo=tzlocal()),\n",
       " 'ResponseMetadata': {'RequestId': '71cd93ee-b423-4778-b32a-884838591130',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '71cd93ee-b423-4778-b32a-884838591130',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '2079',\n",
       "   'date': 'Thu, 15 Jun 2023 10:21:59 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline_job_model_quality.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "5e9bff38-d963-47f2-b8fc-11010ee0666c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>threshold</th>\n",
       "      <th>comparison_operator</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mae</th>\n",
       "      <td>1.186433</td>\n",
       "      <td>GreaterThanThreshold</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mse</th>\n",
       "      <td>2.700755</td>\n",
       "      <td>GreaterThanThreshold</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rmse</th>\n",
       "      <td>1.643397</td>\n",
       "      <td>GreaterThanThreshold</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>r2</th>\n",
       "      <td>0.756449</td>\n",
       "      <td>LessThanThreshold</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     threshold   comparison_operator\n",
       "mae   1.186433  GreaterThanThreshold\n",
       "mse   2.700755  GreaterThanThreshold\n",
       "rmse  1.643397  GreaterThanThreshold\n",
       "r2    0.756449     LessThanThreshold"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.DataFrame(baseline_job_model_quality.suggested_constraints().body_dict[\"regression_constraints\"]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "8996ff83-2149-4cd8-a772-f35fd4fe0319",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "baseline_job_model_quality1 = model_quality_monitor.latest_baselining_job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "387d94cf-69cf-4457-b2d7-917245959ab1",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ProcessingInputs': [{'InputName': 'baseline_dataset_input',\n",
       "   'AppManaged': False,\n",
       "   'S3Input': {'S3Uri': 's3://sagemaker-us-east-1-709891711940/MyAbalonePipeline/7p6mevksvmgc/MyAbaloneProcess/output/validation/validation_with_predictions.csv',\n",
       "    'LocalPath': '/opt/ml/processing/input/baseline_dataset_input',\n",
       "    'S3DataType': 'S3Prefix',\n",
       "    'S3InputMode': 'File',\n",
       "    'S3DataDistributionType': 'FullyReplicated',\n",
       "    'S3CompressionType': 'None'}}],\n",
       " 'ProcessingOutputConfig': {'Outputs': [{'OutputName': 'monitoring_output',\n",
       "    'S3Output': {'S3Uri': 's3://sagemaker-us-east-1-709891711940/my-abalone-model-quality-monitoring-report',\n",
       "     'LocalPath': '/opt/ml/processing/output',\n",
       "     'S3UploadMode': 'EndOfJob'},\n",
       "    'AppManaged': False}]},\n",
       " 'ProcessingJobName': 'MyAbaloneModelQualityMonitorBaselineJob-06-15-2023-09-57-58',\n",
       " 'ProcessingResources': {'ClusterConfig': {'InstanceCount': 1,\n",
       "   'InstanceType': 'ml.m5.xlarge',\n",
       "   'VolumeSizeInGB': 20}},\n",
       " 'StoppingCondition': {'MaxRuntimeInSeconds': 1800},\n",
       " 'AppSpecification': {'ImageUri': '156813124566.dkr.ecr.us-east-1.amazonaws.com/sagemaker-model-monitor-analyzer'},\n",
       " 'Environment': {'analysis_type': 'MODEL_QUALITY',\n",
       "  'dataset_format': '{\"csv\": {\"header\": true, \"output_columns_position\": \"START\"}}',\n",
       "  'dataset_source': '/opt/ml/processing/input/baseline_dataset_input',\n",
       "  'ground_truth_attribute': 'label',\n",
       "  'inference_attribute': 'prediction',\n",
       "  'output_path': '/opt/ml/processing/output',\n",
       "  'problem_type': 'Regression',\n",
       "  'publish_cloudwatch_metrics': 'Disabled'},\n",
       " 'RoleArn': 'arn:aws:iam::709891711940:role/service-role/SageMaker-MLOpsEngineer-poc',\n",
       " 'ProcessingJobArn': 'arn:aws:sagemaker:us-east-1:709891711940:processing-job/MyAbaloneModelQualityMonitorBaselineJob-06-15-2023-09-57-58',\n",
       " 'ProcessingJobStatus': 'Completed',\n",
       " 'ExitMessage': 'Completed: Job completed successfully with no violations.',\n",
       " 'ProcessingEndTime': datetime.datetime(2023, 6, 15, 10, 3, 37, 118000, tzinfo=tzlocal()),\n",
       " 'ProcessingStartTime': datetime.datetime(2023, 6, 15, 10, 2, 8, 990000, tzinfo=tzlocal()),\n",
       " 'LastModifiedTime': datetime.datetime(2023, 6, 15, 10, 3, 37, 475000, tzinfo=tzlocal()),\n",
       " 'CreationTime': datetime.datetime(2023, 6, 15, 9, 57, 58, 534000, tzinfo=tzlocal()),\n",
       " 'ResponseMetadata': {'RequestId': 'bfe41699-fb6c-4c9e-a303-206893fdd6e0',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': 'bfe41699-fb6c-4c9e-a303-206893fdd6e0',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '2079',\n",
       "   'date': 'Fri, 16 Jun 2023 11:27:13 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline_job_model_quality1.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "2d7d86c6-9394-4aed-b7b1-0480da5b7bfb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>threshold</th>\n",
       "      <th>comparison_operator</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mae</th>\n",
       "      <td>1.186433</td>\n",
       "      <td>GreaterThanThreshold</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mse</th>\n",
       "      <td>2.700755</td>\n",
       "      <td>GreaterThanThreshold</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rmse</th>\n",
       "      <td>1.643397</td>\n",
       "      <td>GreaterThanThreshold</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>r2</th>\n",
       "      <td>0.756449</td>\n",
       "      <td>LessThanThreshold</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     threshold   comparison_operator\n",
       "mae   1.186433  GreaterThanThreshold\n",
       "mse   2.700755  GreaterThanThreshold\n",
       "rmse  1.643397  GreaterThanThreshold\n",
       "r2    0.756449     LessThanThreshold"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.DataFrame(baseline_job_model_quality1.suggested_constraints().body_dict[\"regression_constraints\"]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "fb44fa3a-07fb-4d2d-a7e4-fe81b5ce78d7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"test_data/validation.csv\", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "cd8f1323-604e-462d-933c-8211278c9bf8",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.0</td>\n",
       "      <td>-2.115216</td>\n",
       "      <td>-2.145375</td>\n",
       "      <td>-1.901303</td>\n",
       "      <td>-1.541292</td>\n",
       "      <td>-1.490821</td>\n",
       "      <td>-1.433321</td>\n",
       "      <td>-1.500373</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13.0</td>\n",
       "      <td>0.258230</td>\n",
       "      <td>0.172519</td>\n",
       "      <td>-0.227545</td>\n",
       "      <td>-0.126939</td>\n",
       "      <td>-0.429706</td>\n",
       "      <td>-0.114904</td>\n",
       "      <td>0.259862</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8.0</td>\n",
       "      <td>-0.449640</td>\n",
       "      <td>-0.482538</td>\n",
       "      <td>-0.944870</td>\n",
       "      <td>-0.730614</td>\n",
       "      <td>-0.882539</td>\n",
       "      <td>-0.835699</td>\n",
       "      <td>-0.720840</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.0</td>\n",
       "      <td>-2.031938</td>\n",
       "      <td>-2.044597</td>\n",
       "      <td>-2.020857</td>\n",
       "      <td>-1.458695</td>\n",
       "      <td>-1.416476</td>\n",
       "      <td>-1.396825</td>\n",
       "      <td>-1.475226</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11.0</td>\n",
       "      <td>0.966100</td>\n",
       "      <td>0.877965</td>\n",
       "      <td>0.609334</td>\n",
       "      <td>0.975379</td>\n",
       "      <td>1.435693</td>\n",
       "      <td>1.016471</td>\n",
       "      <td>0.195200</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>622</th>\n",
       "      <td>10.0</td>\n",
       "      <td>-1.490625</td>\n",
       "      <td>-1.540707</td>\n",
       "      <td>-1.423087</td>\n",
       "      <td>-1.345506</td>\n",
       "      <td>-1.348889</td>\n",
       "      <td>-1.259965</td>\n",
       "      <td>-1.327941</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>623</th>\n",
       "      <td>8.0</td>\n",
       "      <td>0.508067</td>\n",
       "      <td>0.273297</td>\n",
       "      <td>0.011563</td>\n",
       "      <td>-0.271740</td>\n",
       "      <td>-0.229199</td>\n",
       "      <td>-0.470740</td>\n",
       "      <td>-0.102962</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>624</th>\n",
       "      <td>8.0</td>\n",
       "      <td>-0.449640</td>\n",
       "      <td>-0.432149</td>\n",
       "      <td>-0.466653</td>\n",
       "      <td>-0.442033</td>\n",
       "      <td>-0.145841</td>\n",
       "      <td>-0.279136</td>\n",
       "      <td>-0.710063</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>625</th>\n",
       "      <td>8.0</td>\n",
       "      <td>0.216591</td>\n",
       "      <td>0.172519</td>\n",
       "      <td>0.250672</td>\n",
       "      <td>-0.126939</td>\n",
       "      <td>-0.091772</td>\n",
       "      <td>-0.041912</td>\n",
       "      <td>-0.207139</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>626</th>\n",
       "      <td>8.0</td>\n",
       "      <td>1.132658</td>\n",
       "      <td>0.777187</td>\n",
       "      <td>0.370226</td>\n",
       "      <td>0.813243</td>\n",
       "      <td>1.129299</td>\n",
       "      <td>0.852239</td>\n",
       "      <td>0.439478</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>627 rows  11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0         1         2         3         4         5         6   \\\n",
       "0     5.0 -2.115216 -2.145375 -1.901303 -1.541292 -1.490821 -1.433321   \n",
       "1    13.0  0.258230  0.172519 -0.227545 -0.126939 -0.429706 -0.114904   \n",
       "2     8.0 -0.449640 -0.482538 -0.944870 -0.730614 -0.882539 -0.835699   \n",
       "3     7.0 -2.031938 -2.044597 -2.020857 -1.458695 -1.416476 -1.396825   \n",
       "4    11.0  0.966100  0.877965  0.609334  0.975379  1.435693  1.016471   \n",
       "..    ...       ...       ...       ...       ...       ...       ...   \n",
       "622  10.0 -1.490625 -1.540707 -1.423087 -1.345506 -1.348889 -1.259965   \n",
       "623   8.0  0.508067  0.273297  0.011563 -0.271740 -0.229199 -0.470740   \n",
       "624   8.0 -0.449640 -0.432149 -0.466653 -0.442033 -0.145841 -0.279136   \n",
       "625   8.0  0.216591  0.172519  0.250672 -0.126939 -0.091772 -0.041912   \n",
       "626   8.0  1.132658  0.777187  0.370226  0.813243  1.129299  0.852239   \n",
       "\n",
       "           7    8    9    10  \n",
       "0   -1.500373  0.0  1.0  0.0  \n",
       "1    0.259862  0.0  0.0  1.0  \n",
       "2   -0.720840  1.0  0.0  0.0  \n",
       "3   -1.475226  0.0  1.0  0.0  \n",
       "4    0.195200  0.0  0.0  1.0  \n",
       "..        ...  ...  ...  ...  \n",
       "622 -1.327941  0.0  0.0  1.0  \n",
       "623 -0.102962  1.0  0.0  0.0  \n",
       "624 -0.710063  0.0  0.0  1.0  \n",
       "625 -0.207139  0.0  1.0  0.0  \n",
       "626  0.439478  0.0  0.0  1.0  \n",
       "\n",
       "[627 rows x 11 columns]"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "805007e0-1471-4d73-862b-b2fc3db67169",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df.columns)\n",
    "for i in range(1, len(df.columns)):\n",
    "    if i < 8:\n",
    "        df[i] = np.random.uniform(df[i].min(), df[i].max(), len(df))\n",
    "    else:\n",
    "        df[i] = np.random.randint(df[i].min(), 2, len(df), )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "637975a5-94ad-4819-9423-7aea4961edea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.to_csv(\"test_data/validation_with_data_drift.csv\", header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "691b1827-4517-4ded-8891-12a606f80eb4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "........................................................................................................................................................................................................\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "from time import sleep\n",
    "\n",
    "validate_dataset = \"validation_with_predictions_data_drift.csv\"\n",
    "sm_client = boto3.client(\"sagemaker-runtime\")\n",
    "# Cut off threshold of 80%\n",
    "cutoff = 0.8\n",
    "\n",
    "limit = 200 # Need at least 200 samples to compute standard deviations\n",
    "i = 0\n",
    "with open(f\"test_data/{validate_dataset}\", \"w\") as validation_file:\n",
    "    validation_file.write(\"prediction,label\\n\") # CSV header\n",
    "    with open(\"test_data/validation_with_data_drift.csv\", \"r\") as f:\n",
    "        for row in f:\n",
    "            (label, input_cols) = row.split(\",\", 1)\n",
    "            res = sm_client.invoke_endpoint(EndpointName=endpoint_name,\n",
    "                          ContentType='text/csv',\n",
    "                          Body=input_cols)\n",
    "            prediction = res[\"Body\"].read().decode()\n",
    "            # prediction = \"1\" if probability > cutoff else \"0\"\n",
    "            validation_file.write(f\"{prediction},{label}\\n\")\n",
    "            i += 1\n",
    "            if i > limit:\n",
    "                break\n",
    "            print(\".\", end=\"\", flush=True)\n",
    "            sleep(0.5)\n",
    "print()\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "105a85e4-ca86-42b0-ac63-d9002d6023dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "a = res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "b99c72fa-35aa-4b7a-ad72-355d8df5e6af",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b''"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "872ffcb6-a67f-4e30-98f1-f9ff560d67e2",
   "metadata": {},
   "source": [
    "# Schedule Model Quality Monitoring Jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "2aa7e9ed-d468-4839-99c7-3ee6bbedb3c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "109a6f68-51f3-4971-acba-154b0a9f7e2d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.model_monitor.model_monitoring:Creating Monitoring Schedule with name: MyAbaloneModelQualityMonitorSchedule\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.model_monitor import CronExpressionGenerator\n",
    "from sagemaker.model_monitor import EndpointInput\n",
    "\n",
    "model_quality_schedule_name = \"MyAbaloneModelQualityMonitorSchedule\"\n",
    "gt_s3_uri = f\"s3://{default_bucket}/my-abalone-capture_data/1-2023-05-26-10-26-28-921/AllTraffic/2023/06/15/12/\"\n",
    "\n",
    "model_quality_monitor_schedule = model_quality_monitor.create_monitoring_schedule(\n",
    "    monitor_schedule_name=model_quality_schedule_name,\n",
    "    output_s3_uri=f\"s3://{default_bucket}/my-abalone-model-quality-monitoring-report\",\n",
    "    schedule_cron_expression=CronExpressionGenerator.hourly(),\n",
    "    # statistics=model_quality_monitor.baseline_statistics(),\n",
    "    ground_truth_input=gt_s3_uri,\n",
    "    problem_type=\"Regression\",\n",
    "    constraints=model_quality_monitor.suggested_constraints(),\n",
    "    enable_cloudwatch_metrics=True,\n",
    "    endpoint_input=EndpointInput(\n",
    "        endpoint_name=endpoint_name,\n",
    "        destination=\"/opt/ml/processing/input/endpoint\",\n",
    "        start_time_offset=\"-PT2H\",\n",
    "        end_time_offset=\"-PT1H\",\n",
    "        inference_attribute=\"prediction\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "632c714b-3b1e-42b5-9507-14f5a6a89d08",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stopping Monitoring Schedule with name: MyAbaloneModelQualityMonitorSchedule\n"
     ]
    }
   ],
   "source": [
    "model_quality_monitor.stop_monitoring_schedule()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c341118-e142-45ae-ac51-e817f7662e7f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
